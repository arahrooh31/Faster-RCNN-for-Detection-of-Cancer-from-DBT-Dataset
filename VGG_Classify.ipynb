{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_Classify.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOgY/DTciQORZ/trND+GCsV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arahrooh31/UCLA_BE223C/blob/Keane_temp/VGG_Classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_KlcbCgFI4z"
      },
      "source": [
        "#VGG_CLASSIFY\n",
        "\n",
        "\n",
        "*   This function will take in an input file and ROI and process a 3 class classification \n",
        "*   The final output will be a Cancer, No Cancer decision for the ROI given\n",
        "\n",
        "USAGE:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-Zb4FwANg0e",
        "outputId": "1eb179f4-ac76-4495-fbd7-35256e118097"
      },
      "source": [
        "import os\n",
        "import sys  #to set local import folder\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import gc  #debug memory leaks in matplotlib\n",
        "import csv #read in description files\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "#show model design parameters with torchsummary\n",
        "import torchsummary\n",
        "from torchsummary import summary\n",
        "from torch import FloatTensor\n",
        "from torch import tensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#\n",
        "# Read Data from google drive\n",
        "#\n",
        "from google.colab import drive #for loading gdrive data\n",
        "from google.colab import files\n",
        "################################################################################\n",
        "#\n",
        "# Load data from google drive\n",
        "#\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#\n",
        "# SET LOCAL IMPORT DIRECTORY FOR .PY FILES\n",
        "#\n",
        "sys.path.append('/content/gdrive/My Drive/DBT_WORKSPACE/DBT_PY_FILES')\n",
        "!ls '/content/gdrive/My Drive/DBT_WORKSPACE/DBT_PY_FILES'\n",
        "from CustomImageDataset import CustomImageDataset\n",
        "from VGG16 import VGG16\n",
        "\n",
        "# install dependencies not included by Colab\n",
        "# use pip3 to ensure compatibility w/ Google Deep Learning Images \n",
        "!pip3 install -q pydicom \n",
        "!pip3 install -q tqdm \n",
        "!pip3 install -q imgaug\n",
        "!pip3 install -q pickle5\n",
        "\n",
        "import pydicom #to read dicom files\n",
        "from pydicom import dcmread\n",
        "import pickle5 as pickle; #generic storage of image array\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Enable GPU, if present\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if (train_on_gpu):\n",
        "    !nvidia-smi -L\n",
        "    !nvidia-smi \n",
        "    dev=torch.device(\"cuda\")\n",
        "else:\n",
        "    print('GPU NOT FOUND!!! USING CPU INSTEAD!!!!!')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data_dir = '/content/gdrive/My Drive/DBT_DATA/IMG_ARRAYS'\n",
        "patch_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES' \n",
        "model_dir = '/content/gdrive/My Drive/BE223C_SPRING_2021/MODEL_SAVE'\n",
        "tensorboard_dir = '/content/gdrive/My Drive/BE223C_SPRING_2021/TENSORBOARD_SUMMARIES'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#from get_dirs import get_dirs\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "CustomImageDataset.py  __pycache__  VGG16.py\n",
            "GPU NOT FOUND!!! USING CPU INSTEAD!!!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0dCHWoxmWei"
      },
      "source": [
        "#\n",
        "# Read inputs\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzVhXmL_h-uK"
      },
      "source": [
        "################################################################################\n",
        "# GET FULL LIST OF FILES IN IMAGE ARRAY DIRECTORY\n",
        "################################################################################\n",
        "use_patch_files = 1\n",
        "if (use_patch_files == 0):\n",
        "    raw_files = os.listdir(data_dir)\n",
        "    print('found #files: ',len(raw_files))\n",
        "else: #patches broken up into directories\n",
        "    category_folders = os.listdir(patch_dir)\n",
        "    #raw_files = os.listdir(data_dir)\n",
        "    #print('found #files: ',len(raw_files))    \n",
        "\n",
        "if (0):\n",
        "    #create fake patches for now\n",
        "    patch_dict = {}\n",
        "    for counter,filename in enumerate(raw_files):\n",
        "        #load full array\n",
        "        full_filename = os.path.join(data_dir,filename)\n",
        "        img_data = pickle.load( open( full_filename, \"rb\" ) )\n",
        "        patch_data = img_data[0:3,:,:]\n",
        "        patch_dict[counter]= patch_data\n",
        "        print(full_filename,np.shape(patch_data))\n",
        "\n",
        "        if (counter > 0):\n",
        "            break\n",
        "        \n",
        "### SKIP ACTIONABLE FOLDER\n",
        "### Remove actionable folder item from list, since we're no longer using that data\n",
        "category_folders.remove('ACTIONABLE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "-cHnebpfg1Kt",
        "outputId": "b21d188c-4ad2-4698-af47-890414050e06"
      },
      "source": [
        "for ii in category_folders:\n",
        "    print(ii)\n",
        "    flist = os.listdir(os.path.join(patch_dir,ii))\n",
        "\n",
        "temp = flist[0].split(sep='_')\n",
        "print(temp[3])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-19bc3f42764b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategory_folders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mflist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'category_folders' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09AcXoOggwmF"
      },
      "source": [
        "#generate full file list for use in indexing the dataloader\n",
        "#this replaces the older loader, which was only inputting Normal\n",
        "full_file_list = [] #store the full filename of every file\n",
        "full_category_name = []\n",
        "for category_folder in category_folders:\n",
        "            print('------------- ',category_folder)\n",
        "            file_list = os.listdir(os.path.join(patch_dir,category_folder))\n",
        "            cat_count = 0\n",
        "            for file_name in file_list:\n",
        "                cat_count = cat_count + 1\n",
        "                full_category_name.append(category_folder)\n",
        "                full_file_list.append(file_name)\n",
        "                print(os.path.join(category_folder,file_name))\n",
        "full_file_count = len(full_file_list)\n",
        "print(len(full_file_list))\n",
        "full_file_list[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMDx0HNSQY6I"
      },
      "source": [
        "#Show summary of model setup and move model to the GPU\n",
        " #train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "bsize = 50\n",
        "\n",
        "model_vgg16 = VGG16() #.to(device)\n",
        "model_vgg16 = model_vgg16.float()\n",
        "print(model_vgg16)\n",
        "\n",
        "\n",
        "if (train_on_gpu == 1):\n",
        "    #dev=torch.device(\"cuda\") \n",
        "    model_vgg16.to(dev)\n",
        "    summary(model_vgg16,(3,244,244), batch_size = bsize, device='cuda')\n",
        "else:\n",
        "    summary(model_vgg16,(3,244,244), batch_size = bsize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6wot0haE0ox"
      },
      "source": [
        "model_name = 'vgg16_best_accuracy_93_EPOCH_96_0.04582521319389343'\n",
        "index_storage_file = os.path.join(tensorboard_dir,'data_index_052521_gpu.pickle')\n",
        "index_file = os.path.join(tensorboard_dir,'data_index_052521_gpu.pickle') #052021_gpu.pickle') #data_index_last.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT9hq8MnG3S6"
      },
      "source": [
        "##########################################################\n",
        "# Test against the test dataset to get accuracy results\n",
        "##########################################################\n",
        "run_test= 1\n",
        "if (run_test == 1):\n",
        "    #Mixing GPU and CPU model saves doesn't seem to map well yet\n",
        "    final_file = os.path.join(model_dir,model_name) #'vgg16_best_accuracy_81_EPOCH_79') #95_gpu_051821')#'vgg16_best_accuracy_97_gpu_final')\n",
        "    checkpoint = torch.load(final_file, map_location=torch.device('cpu'))\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    use_index = 1\n",
        "    if (use_index == 1):\n",
        "        #[val_index, test_index, file_list]\n",
        "        validation_saved_index,test_saved_index,saved_flist=pickle.load( open( index_file, \"rb\" )) \n",
        "        test_files=[]\n",
        "        for ii in test_saved_index:\n",
        "            test_files.append(full_file_list[ii])\n",
        "        print('!!!! Using Test Indexed Files ONLY !!!!')\n",
        "    else: #use full files      \n",
        "        bc_files=[]\n",
        "        for ii in full_file_list:\n",
        "            if (('Benign' in ii) or ('Cancer' in ii)):\n",
        "                bc_files.append(ii)\n",
        "                #print(ii)\n",
        "    \n",
        "    #new_file_list=[]\n",
        "    #for ii in b:\n",
        "    #    new_file_list.append(full_file_list[ii])\n",
        "\n",
        "\n",
        "    model_vgg16.load_state_dict(checkpoint)\n",
        "    model_vgg16.eval()\n",
        "\n",
        "\n",
        "    #load up with the pre-sized patch images\n",
        "    all_data = CustomImageDataset(img_dir=patch_dir,\n",
        "                                    category = full_category_name, \n",
        "                                    file_count=len(test_files), #full_file_count,\n",
        "                                    file_list = test_files, #flist, #new_file_list, #full_file_list, \n",
        "                                    transform=None, \n",
        "                                    target_transform=None)\n",
        "\n",
        "    dataloader_all = DataLoader(all_data, batch_size=bsize,shuffle=True, num_workers=2)#, \n",
        "\n",
        "    total_accuracy = []\n",
        "    total_precision = []\n",
        "    total_average_precision=[] \n",
        "    total_final_accuracy = []\n",
        "    total_final_precision = []\n",
        "    total_final_recall_sens = []\n",
        "    total_f1_score =[]\n",
        "    for epoch in range(0,1):\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(dataloader_all, 0):\n",
        "                print(i)\n",
        "                # get the inputs; data is a list of [inputs, labels]\n",
        "                #inputs, labels = data\n",
        "                inputs = data['image'].type(FloatTensor)\n",
        "                labels = data['label'] #.type(FloatTensor)\n",
        "\n",
        "                if (train_on_gpu):\n",
        "                    inputs, labels = inputs.to(dev), labels.to(dev)\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs = model_vgg16(inputs) #.permute(0, 1, 2, 3))\n",
        "\n",
        "                outputs=torch.flatten(outputs, start_dim=1)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "\n",
        "\n",
        "                #print(outputs)\n",
        "                y_pred_softmax = torch.log_softmax(outputs, dim = 1)\n",
        "                _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
        "                #print(y_pred_tags)\n",
        "                #print(labels)\n",
        "                correct_pred = (y_pred_tags == labels).float()\n",
        "                accuracy = correct_pred.sum() / len(correct_pred)\n",
        "                #accuracy = torch.round(accuracy)\n",
        "\n",
        "\n",
        "                #Get advanced metrics\n",
        "                average_precision, final_accuracy, final_precision, final_recall_sens, f1_score = calculate_metrics(labels,y_pred_tags)\n",
        "\n",
        "                total_average_precision.append(average_precision) \n",
        "                total_final_accuracy.append(final_accuracy)\n",
        "                total_final_precision.append(final_precision)\n",
        "                total_final_recall_sens.append(final_recall_sens)\n",
        "                total_f1_score.append(f1_score)\n",
        "\n",
        "                total_accuracy.append(accuracy)\n",
        "                \n",
        "                if (i%100 == 0):\n",
        "                    print('@ interim accuracy = ',i,  sum(total_accuracy)/len(total_accuracy))                \n",
        "                #print('-----#correct, training accuracy = ',correct_pred,accuracy)\n",
        "    print('Finished testing all data')\n",
        "    print('total accuracy = ', sum(total_accuracy)/len(total_accuracy))\n",
        "    test_p_file = os.path.join(tensorboard_dir,'test_metrics_debug.pickle')\n",
        "    pickle.dump([total_average_precision,total_final_accuracy, \n",
        "                 total_final_precision, total_final_recall_sens, \n",
        "                 total_f1_score, total_accuracy],\n",
        "                 open( test_p_file, \"wb\" ),protocol=5 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2z-ZQGXMLnT"
      },
      "source": [
        "def vgg_classify():\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}