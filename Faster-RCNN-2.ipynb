{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Faster-RCNN-2.ipynb","provenance":[{"file_id":"1wqtUYyAnG1kRXsjs0utCQmeHoWWctA4t","timestamp":1620268307984}],"collapsed_sections":[],"authorship_tag":"ABX9TyMI1SM03Bxa5HQ1pWZzChoo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b71a50ea03d9495195c08752ccd44ee4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1db386ef8d8f44d8ae299edfea92edc3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cee2c39f13d24ce59feb603dc5bd4cb7","IPY_MODEL_cbfa98a41aa24bfd96d5dd618ffe960d"]}},"1db386ef8d8f44d8ae299edfea92edc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cee2c39f13d24ce59feb603dc5bd4cb7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d7ab6be06d6f4a9bab9bfc09d2e8908a","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":167502836,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":167502836,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2058a9617f2043a7a75716c02f8ea687"}},"cbfa98a41aa24bfd96d5dd618ffe960d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4b2b3c6bc3b64a50a17aec1e30a7f7fd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 160M/160M [00:02&lt;00:00, 77.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_03e22a2b44654243812d8c1dc0660001"}},"d7ab6be06d6f4a9bab9bfc09d2e8908a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2058a9617f2043a7a75716c02f8ea687":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b2b3c6bc3b64a50a17aec1e30a7f7fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"03e22a2b44654243812d8c1dc0660001":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d35ce75c494745e4aa9a520c12896425":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5da3c209624c410bb441c8facb40e375","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_94a911b9d47649efb7f706fb0065c702","IPY_MODEL_89ffbc32b6354eed96465969c318b4e7"]}},"5da3c209624c410bb441c8facb40e375":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"94a911b9d47649efb7f706fb0065c702":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_73a0e4e67a464cf7882b8dbfe2110762","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":14212972,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":14212972,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fb680773f0904c909b6062a79623f133"}},"89ffbc32b6354eed96465969c318b4e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_03164fa4e2d14fc4977bc652f20a6449","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 13.6M/13.6M [05:28&lt;00:00, 43.3kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b7e6cf59957a4ba38be994498f4b6d08"}},"73a0e4e67a464cf7882b8dbfe2110762":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fb680773f0904c909b6062a79623f133":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"03164fa4e2d14fc4977bc652f20a6449":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b7e6cf59957a4ba38be994498f4b6d08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ede5910cfaa940c7a7a5f9ceeeb3054c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e1503e9c8aa345f0851ab7283b583640","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4ce977973cd9484182541142c9df9e86","IPY_MODEL_13ef327071034e8b9209c02e56652159"]}},"e1503e9c8aa345f0851ab7283b583640":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4ce977973cd9484182541142c9df9e86":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c567534c1c9d456eb5404e12262a2861","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":102502400,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102502400,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8f77aeeb1b014bfab9a382ba68c837d1"}},"13ef327071034e8b9209c02e56652159":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ca44d045c52740bfb6ab54afd48ab7c0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [00:00&lt;00:00, 126MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0a72447bed1749d981f97c7e35966ec4"}},"c567534c1c9d456eb5404e12262a2861":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8f77aeeb1b014bfab9a382ba68c837d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ca44d045c52740bfb6ab54afd48ab7c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0a72447bed1749d981f97c7e35966ec4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nju050s_07eo","executionInfo":{"status":"ok","timestamp":1620737394517,"user_tz":240,"elapsed":23008,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"0953ee32-ff1a-4cc2-9a41-b62153061465"},"source":["!pip install pydicom\n","!pip install pickle5\n","!pip install dicom2nifti\n","!pip install hiddenlayer\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pydicom\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/15/df16546bc59bfca390cf072d473fb2c8acd4231636f64356593a63137e55/pydicom-2.1.2-py3-none-any.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 4.3MB/s \n","\u001b[?25hInstalling collected packages: pydicom\n","Successfully installed pydicom-2.1.2\n","Collecting pickle5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n","\u001b[K     |████████████████████████████████| 133kB 4.3MB/s \n","\u001b[?25hBuilding wheels for collected packages: pickle5\n","  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219258 sha256=ca0567336ff7edb7f639ed23ddbd42d44e69442a1957e1b21da82e734babca8f\n","  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n","Successfully built pickle5\n","Installing collected packages: pickle5\n","Successfully installed pickle5-0.0.11\n","Collecting dicom2nifti\n","  Downloading https://files.pythonhosted.org/packages/9b/be/7611901c0ef7f0b60024bcbe3af0f1e0baefe0d4b48fe5f53bd6b621ae22/dicom2nifti-2.2.12.tar.gz\n","Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (from dicom2nifti) (3.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from dicom2nifti) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from dicom2nifti) (1.4.1)\n","Requirement already satisfied: pydicom>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from dicom2nifti) (2.1.2)\n","Building wheels for collected packages: dicom2nifti\n","  Building wheel for dicom2nifti (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dicom2nifti: filename=dicom2nifti-2.2.12-cp37-none-any.whl size=42620 sha256=83f6233d8ff2833b690beedc2de6082b4bb948c821007cbf083c677ee85b5a6d\n","  Stored in directory: /root/.cache/pip/wheels/50/d5/44/984e7449cae4fcacfa4ee0481274a052d52fbcd66e03e377f4\n","Successfully built dicom2nifti\n","Installing collected packages: dicom2nifti\n","Successfully installed dicom2nifti-2.2.12\n","Collecting hiddenlayer\n","  Downloading https://files.pythonhosted.org/packages/64/f8/ea51d02695a4dc397f3b2487fae462cd3f2ce707c54250e0fdfaec2ff92e/hiddenlayer-0.3-py3-none-any.whl\n","Installing collected packages: hiddenlayer\n","Successfully installed hiddenlayer-0.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TucEWtfDI8U7","executionInfo":{"status":"ok","timestamp":1620737406535,"user_tz":240,"elapsed":555,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["#Dependencies\n","from torchvision.models.detection import FasterRCNN\n","\n","import os\n","import sys\n","import pandas as pd\n","from glob import glob\n","import pickle5 as pickle\n","import pydicom\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from matplotlib.patches import Rectangle\n","import numpy as np\n","from pathlib import Path\n","import six\n","import csv\n","import logging\n","import math\n","from mpl_toolkits.mplot3d import Axes3D\n","import pandas as pd\n","import dicom2nifti\n","import nibabel as nib\n","from PIL import Image \n","import scipy.misc\n","from scipy import stats\n","from sklearn.model_selection import train_test_split\n","from datetime import datetime\n","from scipy.spatial import distance\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import torchvision\n","from torchvision.models.detection import RetinaNet\n","from torchvision.models.detection.anchor_utils import AnchorGenerator\n","import torch.optim as optim\n","import hiddenlayer as hl\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import torchvision\n","from torchvision.models.detection import RetinaNet\n","from torchvision.models.detection.anchor_utils import AnchorGenerator\n","import torch.optim as optim\n","\n","from torch import nn\n","import torch.nn.functional as F\n","\n","from torchvision.ops import MultiScaleRoIAlign\n","\n","import numpy as np\n","import cv2\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torch.utils.data import DataLoader, Dataset\n","\n","import torch\n","import os\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","from torchvision import datasets, transforms\n","from PIL import Image\n","from xml.dom.minidom import parse\n","%matplotlib inline\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m2TRJSP8RM1q"},"source":["DATALOADER"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CImN33EmDMEM","executionInfo":{"status":"ok","timestamp":1620737431549,"user_tz":240,"elapsed":21699,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"9815273c-f659-40dc-e912-90705a59f243"},"source":["# Mount the google drive so that we can access the data stored in the drive\n","from google.colab import drive\n","from google.colab import files\n","drive.mount('/content/gdrive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9mD7iES3wpMz","executionInfo":{"status":"ok","timestamp":1620737434825,"user_tz":240,"elapsed":583,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["BATCH_SIZE = 6"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JSnJ9kl66W3","executionInfo":{"status":"ok","timestamp":1620737440487,"user_tz":240,"elapsed":2472,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"ca0c1942-1d73-4afc-87c4-e00ab58bfa95"},"source":["img_arrays_path = '/content/gdrive/MyDrive/BE223C/DBT_DATA/IMG_ARRAYS/'\n","boxes_path = '/content/gdrive/MyDrive/BE223C/DBT_DATA/TRAINING_DATA/BCS-DBT boxes-train-v2.csv'\n","df_boxes = pd.read_csv(boxes_path)\n","sum(df_boxes['Class'] == 'cancer') "],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["87"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"wPJqZcbEwt53","executionInfo":{"status":"ok","timestamp":1620737441508,"user_tz":240,"elapsed":359,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["labels_unsplit = []\n","for i in range(len(df_boxes)):\n","  if df_boxes['Class'][i] == 'cancer':\n","    labels_unsplit.append(1)\n","  else:\n","    labels_unsplit.append(0)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ip7B4TUQ8se","executionInfo":{"status":"ok","timestamp":1620737472910,"user_tz":240,"elapsed":30272,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["def select_cases_with_boxes(img_arrays_path, boxes_path):\n","  df_boxes = pd.read_csv(boxes_path)\n","  img_names = glob(os.path.join(img_arrays_path, '*.pickle'))\n","  img_names[0].split('_')[5]\n","  img_names_valid = []\n","  for i in range(len(df_boxes)):\n","    for j in range(len(img_names)):\n","      img_studyID = img_names[j].split('_')[3]\n","      view = img_names[j].split('_')[4]\n","      if df_boxes['StudyUID'][i] == img_studyID and df_boxes['View'][i] == view:\n","        img_names_valid.append(img_names[j])\n","  return img_names_valid \n","\n","img_names = glob(os.path.join(img_arrays_path, '*.pickle'))\n","\n","class BreastDataset(Dataset):\n","  def __init__(self, df_boxes, img_names, batch_size, transform=None):\n","    self.df_boxes = df_boxes\n","    self.img_names = img_names\n","    self.batch_size = batch_size\n","\n","  def __len__(self):\n","    return len(self.df_boxes)\n","\n","  def __getitem__(self, index):\n","    def padding(array, xx, yy):\n","    \n","      h = array.shape[0]\n","      w = array.shape[1]\n","\n","      a = (xx - h) // 2\n","      aa = xx - a - h\n","\n","      b = (yy - w) // 2\n","      bb = yy - b - w\n","\n","      return np.pad(array, pad_width=((a, aa), (b, bb)), mode='constant')\n","    # Get the image slice where the bounding box resides\n","    img_array = pickle.load(open(self.img_names[index], \"rb\" ))\n","    box_slice_index = self.df_boxes['Slice'][index]\n","    img_slice = img_array[box_slice_index, :, :]\n","    # Normalize the image (min max norm)\n","    img_slice = (img_slice - img_slice.min()) / (img_slice.max() - img_slice.min())\n","    img_slice = padding(img_slice, 2457, 1996)\n","    # Convert into a 3 channel image\n","    img_slice = np.stack((img_slice,)*3, axis = 0)\n","    img_slice = torch.from_numpy(img_slice.astype(np.float32))\n","    img_slice = img_slice.to(device = torch.device('cuda:0'))\n","\n","    # Create the label based on the file name\n","    if 'Benign' in os.path.basename(self.img_names[index]):\n","      label = torch.tensor([0])\n","      label = label.to(device = torch.device('cuda:0'))\n","    elif 'Cancer' in os.path.basename(self.img_names[index]):\n","      label = torch.tensor([1])\n","      label = label.to(device = torch.device('cuda:0'))\n","    \n","    # Get the bounding box \n","    box = [df_boxes['X'][index], df_boxes['Y'][index], df_boxes['Width'][index], df_boxes['Height'][index]]\n","\n","    # transform from [x, y, w, h] to [x1, y1, x2, y2]\n","    box[2] = box[0] + box[2]\n","    box[3] = box[1] + box[3]\n","    \n","    box = torch.as_tensor(box)\n","    box = box.to(device = torch.device('cuda:0'))\n","    target = {}\n","    target[\"boxes\"] = box\n","    target[\"labels\"] = label\n","\n","    \n","    return img_slice, [target] * self.batch_size"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"5FMJP9xVw2w3","executionInfo":{"status":"ok","timestamp":1620737484237,"user_tz":240,"elapsed":5102,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["img_names_valid = select_cases_with_boxes(img_arrays_path, boxes_path)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"90rr9tglw4mo"},"source":["img_names_valid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FYEPGIscRfPS","executionInfo":{"status":"ok","timestamp":1620737487821,"user_tz":240,"elapsed":522,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["breast_dataset = BreastDataset(df_boxes, img_names_valid, BATCH_SIZE)\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"8U33FWK9xDMx","executionInfo":{"status":"ok","timestamp":1620737489756,"user_tz":240,"elapsed":666,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["# Stratified train, val, test split. First do train-test split, and then take out val from the train\n","train_indices, test_indices = train_test_split(\n","    np.arange(len(labels_unsplit)), test_size = 0.1, shuffle = True, stratify = labels_unsplit, random_state = 123)\n","train_indices, val_indices = train_test_split(\n","    train_indices, test_size = 0.1, shuffle = True, stratify = [labels_unsplit[i] for i in train_indices], random_state = 123)\n","\n","train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n","val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n","test_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n","\n","train_dataloader = DataLoader(breast_dataset, batch_size = BATCH_SIZE, shuffle = False, sampler = train_sampler, num_workers = 0)\n","val_dataloader = DataLoader(breast_dataset, batch_size = BATCH_SIZE, shuffle = False, sampler = val_sampler, num_workers = 0)\n","test_dataloader = DataLoader(breast_dataset, batch_size = BATCH_SIZE, shuffle = False, sampler = test_sampler, num_workers = 0)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nrIrGwsvRFwc"},"source":["TRAINING THE MODEL"]},{"cell_type":"code","metadata":{"id":"1Hep2pQvhMFK","executionInfo":{"status":"ok","timestamp":1620737494328,"user_tz":240,"elapsed":1299,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\n","import torch\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import os\n","import re\n","\n","from PIL import Image\n","\n","import albumentations as A\n","\n","import torch\n","import torchvision\n","\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data.sampler import SequentialSampler\n","\n","from matplotlib import pyplot as plt\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"id":"JLr6xKg1jCQn","executionInfo":{"status":"error","timestamp":1620268980823,"user_tz":240,"elapsed":275,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"1763541d-df16-4213-9f08-3aeebdfdd425"},"source":["fasterRCNN = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","# Define RPN \n","anchor_generator = AnchorGenerator(sizes=tuple([(16, 32, 64, 128, 256) for _ in range(5)]), # let num of tuple equal to num of feature maps\n","                                  aspect_ratios=tuple([(0.75, 0.5, 1.25) for _ in range(5)])) # ref: https://github.com/pytorch/vision/issues/978\n","rpn_head = RPNHead(256, anchor_generator.num_anchors_per_location()[0])\n","fasterRCNN.rpn = RegionProposalNetwork(\n","    anchor_generator= anchor_generator, head= rpn_head,\n","    fg_iou_thresh= 0.7, bg_iou_thresh=0.3,\n","    batch_size_per_image=48, # use fewer proposals\n","    positive_fraction = 0.5,\n","    pre_nms_top_n=dict(training=200, testing=100),\n","    post_nms_top_n=dict(training=160, testing=80),\n","    nms_thresh = 0.7\n","\n","\n","in_features = fasterRCNN.roi_heads.box_predictor.cls_score.in_features \n","fasterRCNN.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes = 2)\n","fasterRCNN.roi_heads.fg_bg_sampler.batch_size_per_image = 24\n","fasterRCNN.roi_heads.fg_bg_sampler.positive_fraction = 0.5"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-67b43f2ab80d>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    in_features = fasterRCNN.roi_heads.box_predictor.cls_score.in_features\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"fAVcVcyghjrJ"},"source":["# move model to the right device\n","fasterRCNN.to(device)\n","\n","params = [p for p in fasterRCNN.parameters() if p.requires_grad]\n","optimizer = torch.optim.Adam(params, lr=0.0005, betas=(0.9, 0.999), weight_decay=0.0005)\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","metric_collector = []\n","num_epochs = 15\n","for epoch in range(num_epochs):\n","    # train for one epoch, printing every 5 iterations\n","    metric_logger = train_one_epoch(fasterRCNN, optimizer, train_data_loader, device, epoch, print_freq=5)\n","    metric_collector.append(metric_logger)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # Evaluate with validation dataset\n","    metric_logger_val = validate(fasterRCNN, val_data_loader, device, print_freq=5)\n","    #save checlpoint\n","    torch.save( fasterRCNN.state_dict(), os.path.join( weights_path,'fasterRCNN_ep'+str(epoch)+'.pth') )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["b71a50ea03d9495195c08752ccd44ee4","1db386ef8d8f44d8ae299edfea92edc3","cee2c39f13d24ce59feb603dc5bd4cb7","cbfa98a41aa24bfd96d5dd618ffe960d","d7ab6be06d6f4a9bab9bfc09d2e8908a","2058a9617f2043a7a75716c02f8ea687","4b2b3c6bc3b64a50a17aec1e30a7f7fd","03e22a2b44654243812d8c1dc0660001"]},"id":"6eRTVMHJakgc","executionInfo":{"status":"ok","timestamp":1620737715723,"user_tz":240,"elapsed":3692,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"b890909c-5d75-42fb-d3b1-08f86573dc93"},"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","# load a model pre-trained pre-trained on COCO\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","\n","# replace the classifier with a new one, that has\n","# num_classes which is user-defined\n","num_classes = 2  # 1 class (person) + background\n","# get number of input features for the classifier\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# replace the pre-trained head with a new one\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b71a50ea03d9495195c08752ccd44ee4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=167502836.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["d35ce75c494745e4aa9a520c12896425","5da3c209624c410bb441c8facb40e375","94a911b9d47649efb7f706fb0065c702","89ffbc32b6354eed96465969c318b4e7","73a0e4e67a464cf7882b8dbfe2110762","fb680773f0904c909b6062a79623f133","03164fa4e2d14fc4977bc652f20a6449","b7e6cf59957a4ba38be994498f4b6d08"]},"id":"dRTfTTvlbVRc","executionInfo":{"status":"ok","timestamp":1620737724322,"user_tz":240,"elapsed":2146,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"2bb0c31b-5cd5-4a49-923d-9c65c9d9f84a"},"source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","# load a pre-trained model for classification and return\n","# only the features\n","backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n","# FasterRCNN needs to know the number of\n","# output channels in a backbone. For mobilenet_v2, it's 1280\n","# so we need to add it here\n","backbone.out_channels = 1280\n","\n","# let's make the RPN generate 5 x 3 anchors per spatial\n","# location, with 5 different sizes and 3 different aspect\n","# ratios. We have a Tuple[Tuple[int]] because each feature\n","# map could potentially have different sizes and\n","# aspect ratios\n","anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n","                                   aspect_ratios=((0.5, 1.0, 2.0),))\n","\n","# let's define what are the feature maps that we will\n","# use to perform the region of interest cropping, as well as\n","# the size of the crop after rescaling.\n","# if your backbone returns a Tensor, featmap_names is expected to\n","# be [0]. More generally, the backbone should return an\n","# OrderedDict[Tensor], and in featmap_names you can choose which\n","# feature maps to use.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","\n","# put the pieces together inside a FasterRCNN model\n","model = FasterRCNN(backbone,\n","                   num_classes=2,\n","                   rpn_anchor_generator=anchor_generator,\n","                   box_roi_pool=roi_pooler)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d35ce75c494745e4aa9a520c12896425","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=14212972.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"04MEEgATbkqp","executionInfo":{"status":"ok","timestamp":1620737804546,"user_tz":240,"elapsed":611,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","\n","def get_model_instance_segmentation(num_classes):\n","    # load an instance segmentation model pre-trained pre-trained on COCO\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n","\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # now get the number of input features for the mask classifier\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # and replace the mask predictor with a new one\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n","                                                       hidden_layer,\n","                                                       num_classes)\n","\n","    return model"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"xEW8YWHkgFF5","executionInfo":{"status":"ok","timestamp":1620737826051,"user_tz":240,"elapsed":526,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["import torchvision.transforms as T\n","\n","def get_transform(train):\n","    transforms = []\n","    transforms.append(T.ToTensor())\n","    if train:\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ahqdU-o8hC4A","executionInfo":{"status":"ok","timestamp":1620738070490,"user_tz":240,"elapsed":4050,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"5730f004-c409-4a2c-816e-635984ff51e2"},"source":["!pip install utils"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Collecting utils\n","  Downloading https://files.pythonhosted.org/packages/55/e6/c2d2b2703e7debc8b501caae0e6f7ead148fd0faa3c8131292a599930029/utils-1.0.1-py2.py3-none-any.whl\n","Installing collected packages: utils\n","Successfully installed utils-1.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VjACOnhwh7V_","executionInfo":{"status":"ok","timestamp":1620738861159,"user_tz":240,"elapsed":9901,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"2c7cac1c-353f-4c95-f694-3087d496c89c"},"source":["!pip install pycocotools\n","!pip install cython\n","!pip install matplotlib"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (2.0.2)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools) (0.29.22)\n","Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools) (56.1.0)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools) (3.2.2)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools) (1.15.0)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.22)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujccQE8SijkB","executionInfo":{"status":"ok","timestamp":1620738488737,"user_tz":240,"elapsed":8282,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"0fcfb01d-0bf0-410e-8c4d-3a8d0540673e"},"source":["!git clone https://github.com/pytorch/vision.git output/vision"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Cloning into 'output/vision'...\n","remote: Enumerating objects: 23953, done.\u001b[K\n","remote: Counting objects: 100% (1423/1423), done.\u001b[K\n","remote: Compressing objects: 100% (418/418), done.\u001b[K\n","remote: Total 23953 (delta 1009), reused 1320 (delta 973), pack-reused 22530\u001b[K\n","Receiving objects: 100% (23953/23953), 31.40 MiB | 18.59 MiB/s, done.\n","Resolving deltas: 100% (17702/17702), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"varwRNTdjM9S","executionInfo":{"status":"ok","timestamp":1620738691080,"user_tz":240,"elapsed":524,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"01a16c5a-a858-4a15-b0ac-3af9c07aac5b"},"source":["!ls\n"],"execution_count":39,"outputs":[{"output_type":"stream","text":["gdrive\toutput\tsample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"78tKDGaukuow","executionInfo":{"status":"ok","timestamp":1620739032646,"user_tz":240,"elapsed":1965,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["import json\n","import tempfile\n","\n","import numpy as np\n","import copy\n","import time\n","import torch\n","import torch._six\n","\n","from pycocotools.cocoeval import COCOeval\n","from pycocotools.coco import COCO\n","import pycocotools.mask as mask_util\n","\n","from collections import defaultdict\n","\n","import utils\n","\n","\n","class CocoEvaluator(object):\n","    def __init__(self, coco_gt, iou_types):\n","        assert isinstance(iou_types, (list, tuple))\n","        coco_gt = copy.deepcopy(coco_gt)\n","        self.coco_gt = coco_gt\n","\n","        self.iou_types = iou_types\n","        self.coco_eval = {}\n","        for iou_type in iou_types:\n","            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n","\n","        self.img_ids = []\n","        self.eval_imgs = {k: [] for k in iou_types}\n","\n","    def update(self, predictions):\n","        img_ids = list(np.unique(list(predictions.keys())))\n","        self.img_ids.extend(img_ids)\n","\n","        for iou_type in self.iou_types:\n","            results = self.prepare(predictions, iou_type)\n","            coco_dt = loadRes(self.coco_gt, results) if results else COCO()\n","            coco_eval = self.coco_eval[iou_type]\n","\n","            coco_eval.cocoDt = coco_dt\n","            coco_eval.params.imgIds = list(img_ids)\n","            img_ids, eval_imgs = evaluate(coco_eval)\n","\n","            self.eval_imgs[iou_type].append(eval_imgs)\n","\n","    def synchronize_between_processes(self):\n","        for iou_type in self.iou_types:\n","            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n","            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n","\n","    def accumulate(self):\n","        for coco_eval in self.coco_eval.values():\n","            coco_eval.accumulate()\n","\n","    def summarize(self):\n","        for iou_type, coco_eval in self.coco_eval.items():\n","            print(\"IoU metric: {}\".format(iou_type))\n","            coco_eval.summarize()\n","\n","    def prepare(self, predictions, iou_type):\n","        if iou_type == \"bbox\":\n","            return self.prepare_for_coco_detection(predictions)\n","        elif iou_type == \"segm\":\n","            return self.prepare_for_coco_segmentation(predictions)\n","        elif iou_type == \"keypoints\":\n","            return self.prepare_for_coco_keypoint(predictions)\n","        else:\n","            raise ValueError(\"Unknown iou type {}\".format(iou_type))\n","\n","    def prepare_for_coco_detection(self, predictions):\n","        coco_results = []\n","        for original_id, prediction in predictions.items():\n","            if len(prediction) == 0:\n","                continue\n","\n","            boxes = prediction[\"boxes\"]\n","            boxes = convert_to_xywh(boxes).tolist()\n","            scores = prediction[\"scores\"].tolist()\n","            labels = prediction[\"labels\"].tolist()\n","\n","            coco_results.extend(\n","                [\n","                    {\n","                        \"image_id\": original_id,\n","                        \"category_id\": labels[k],\n","                        \"bbox\": box,\n","                        \"score\": scores[k],\n","                    }\n","                    for k, box in enumerate(boxes)\n","                ]\n","            )\n","        return coco_results\n","\n","    def prepare_for_coco_segmentation(self, predictions):\n","        coco_results = []\n","        for original_id, prediction in predictions.items():\n","            if len(prediction) == 0:\n","                continue\n","\n","            scores = prediction[\"scores\"]\n","            labels = prediction[\"labels\"]\n","            masks = prediction[\"masks\"]\n","\n","            masks = masks > 0.5\n","\n","            scores = prediction[\"scores\"].tolist()\n","            labels = prediction[\"labels\"].tolist()\n","\n","            rles = [\n","                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0]\n","                for mask in masks\n","            ]\n","            for rle in rles:\n","                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n","\n","            coco_results.extend(\n","                [\n","                    {\n","                        \"image_id\": original_id,\n","                        \"category_id\": labels[k],\n","                        \"segmentation\": rle,\n","                        \"score\": scores[k],\n","                    }\n","                    for k, rle in enumerate(rles)\n","                ]\n","            )\n","        return coco_results\n","\n","    def prepare_for_coco_keypoint(self, predictions):\n","        coco_results = []\n","        for original_id, prediction in predictions.items():\n","            if len(prediction) == 0:\n","                continue\n","\n","            boxes = prediction[\"boxes\"]\n","            boxes = convert_to_xywh(boxes).tolist()\n","            scores = prediction[\"scores\"].tolist()\n","            labels = prediction[\"labels\"].tolist()\n","            keypoints = prediction[\"keypoints\"]\n","            keypoints = keypoints.flatten(start_dim=1).tolist()\n","\n","            coco_results.extend(\n","                [\n","                    {\n","                        \"image_id\": original_id,\n","                        \"category_id\": labels[k],\n","                        'keypoints': keypoint,\n","                        \"score\": scores[k],\n","                    }\n","                    for k, keypoint in enumerate(keypoints)\n","                ]\n","            )\n","        return coco_results\n","\n","\n","def convert_to_xywh(boxes):\n","    xmin, ymin, xmax, ymax = boxes.unbind(1)\n","    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n","\n","\n","def merge(img_ids, eval_imgs):\n","    all_img_ids = utils.all_gather(img_ids)\n","    all_eval_imgs = utils.all_gather(eval_imgs)\n","\n","    merged_img_ids = []\n","    for p in all_img_ids:\n","        merged_img_ids.extend(p)\n","\n","    merged_eval_imgs = []\n","    for p in all_eval_imgs:\n","        merged_eval_imgs.append(p)\n","\n","    merged_img_ids = np.array(merged_img_ids)\n","    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n","\n","    # keep only unique (and in sorted order) images\n","    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n","    merged_eval_imgs = merged_eval_imgs[..., idx]\n","\n","    return merged_img_ids, merged_eval_imgs\n","\n","\n","def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n","    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n","    img_ids = list(img_ids)\n","    eval_imgs = list(eval_imgs.flatten())\n","\n","    coco_eval.evalImgs = eval_imgs\n","    coco_eval.params.imgIds = img_ids\n","    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n","\n","\n","#################################################################\n","# From pycocotools, just removed the prints and fixed\n","# a Python3 bug about unicode not defined\n","#################################################################\n","\n","# Ideally, pycocotools wouldn't have hard-coded prints\n","# so that we could avoid copy-pasting those two functions\n","\n","def createIndex(self):\n","    # create index\n","    # print('creating index...')\n","    anns, cats, imgs = {}, {}, {}\n","    imgToAnns, catToImgs = defaultdict(list), defaultdict(list)\n","    if 'annotations' in self.dataset:\n","        for ann in self.dataset['annotations']:\n","            imgToAnns[ann['image_id']].append(ann)\n","            anns[ann['id']] = ann\n","\n","    if 'images' in self.dataset:\n","        for img in self.dataset['images']:\n","            imgs[img['id']] = img\n","\n","    if 'categories' in self.dataset:\n","        for cat in self.dataset['categories']:\n","            cats[cat['id']] = cat\n","\n","    if 'annotations' in self.dataset and 'categories' in self.dataset:\n","        for ann in self.dataset['annotations']:\n","            catToImgs[ann['category_id']].append(ann['image_id'])\n","\n","    # print('index created!')\n","\n","    # create class members\n","    self.anns = anns\n","    self.imgToAnns = imgToAnns\n","    self.catToImgs = catToImgs\n","    self.imgs = imgs\n","    self.cats = cats\n","\n","\n","maskUtils = mask_util\n","\n","\n","def loadRes(self, resFile):\n","    \"\"\"\n","    Load result file and return a result api object.\n","    Args:\n","        self (obj): coco object with ground truth annotations\n","        resFile (str): file name of result file\n","    Returns:\n","    res (obj): result api object\n","    \"\"\"\n","    res = COCO()\n","    res.dataset['images'] = [img for img in self.dataset['images']]\n","\n","    # print('Loading and preparing results...')\n","    # tic = time.time()\n","    if isinstance(resFile, torch._six.string_classes):\n","        anns = json.load(open(resFile))\n","    elif type(resFile) == np.ndarray:\n","        anns = self.loadNumpyAnnotations(resFile)\n","    else:\n","        anns = resFile\n","    assert type(anns) == list, 'results in not an array of objects'\n","    annsImgIds = [ann['image_id'] for ann in anns]\n","    assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n","        'Results do not correspond to current coco set'\n","    if 'caption' in anns[0]:\n","        imgIds = set([img['id'] for img in res.dataset['images']]) & set([ann['image_id'] for ann in anns])\n","        res.dataset['images'] = [img for img in res.dataset['images'] if img['id'] in imgIds]\n","        for id, ann in enumerate(anns):\n","            ann['id'] = id + 1\n","    elif 'bbox' in anns[0] and not anns[0]['bbox'] == []:\n","        res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n","        for id, ann in enumerate(anns):\n","            bb = ann['bbox']\n","            x1, x2, y1, y2 = [bb[0], bb[0] + bb[2], bb[1], bb[1] + bb[3]]\n","            if 'segmentation' not in ann:\n","                ann['segmentation'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n","            ann['area'] = bb[2] * bb[3]\n","            ann['id'] = id + 1\n","            ann['iscrowd'] = 0\n","    elif 'segmentation' in anns[0]:\n","        res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n","        for id, ann in enumerate(anns):\n","            # now only support compressed RLE format as segmentation results\n","            ann['area'] = maskUtils.area(ann['segmentation'])\n","            if 'bbox' not in ann:\n","                ann['bbox'] = maskUtils.toBbox(ann['segmentation'])\n","            ann['id'] = id + 1\n","            ann['iscrowd'] = 0\n","    elif 'keypoints' in anns[0]:\n","        res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n","        for id, ann in enumerate(anns):\n","            s = ann['keypoints']\n","            x = s[0::3]\n","            y = s[1::3]\n","            x1, x2, y1, y2 = np.min(x), np.max(x), np.min(y), np.max(y)\n","            ann['area'] = (x2 - x1) * (y2 - y1)\n","            ann['id'] = id + 1\n","            ann['bbox'] = [x1, y1, x2 - x1, y2 - y1]\n","    # print('DONE (t={:0.2f}s)'.format(time.time()- tic))\n","\n","    res.dataset['annotations'] = anns\n","    createIndex(res)\n","    return res\n","\n","\n","def evaluate(self):\n","    '''\n","    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n","    :return: None\n","    '''\n","    # tic = time.time()\n","    # print('Running per image evaluation...')\n","    p = self.params\n","    # add backward compatibility if useSegm is specified in params\n","    if p.useSegm is not None:\n","        p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n","        print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n","    # print('Evaluate annotation type *{}*'.format(p.iouType))\n","    p.imgIds = list(np.unique(p.imgIds))\n","    if p.useCats:\n","        p.catIds = list(np.unique(p.catIds))\n","    p.maxDets = sorted(p.maxDets)\n","    self.params = p\n","\n","    self._prepare()\n","    # loop through images, area range, max detection number\n","    catIds = p.catIds if p.useCats else [-1]\n","\n","    if p.iouType == 'segm' or p.iouType == 'bbox':\n","        computeIoU = self.computeIoU\n","    elif p.iouType == 'keypoints':\n","        computeIoU = self.computeOks\n","    self.ious = {\n","        (imgId, catId): computeIoU(imgId, catId)\n","        for imgId in p.imgIds\n","        for catId in catIds}\n","\n","    evaluateImg = self.evaluateImg\n","    maxDet = p.maxDets[-1]\n","    evalImgs = [\n","        evaluateImg(imgId, catId, areaRng, maxDet)\n","        for catId in catIds\n","        for areaRng in p.areaRng\n","        for imgId in p.imgIds\n","    ]\n","    # this is NOT in the pycocotools code, but could be done outside\n","    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))\n","    self._paramsEval = copy.deepcopy(self.params)\n","    # toc = time.time()\n","    # print('DONE (t={:0.2f}s).'.format(toc-tic))\n","    return p.imgIds, evalImgs\n","\n","#################################################################\n","# end of straight copy from pycocotools, just removing the prints\n","#################################################################"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"4uzso3a0k05Z","executionInfo":{"status":"ok","timestamp":1620739073389,"user_tz":240,"elapsed":1305,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["import copy\n","import os\n","from PIL import Image\n","\n","import torch\n","import torch.utils.data\n","import torchvision\n","\n","from pycocotools import mask as coco_mask\n","from pycocotools.coco import COCO\n","\n","import torchvision.transforms as T\n","\n","\n","class FilterAndRemapCocoCategories(object):\n","    def __init__(self, categories, remap=True):\n","        self.categories = categories\n","        self.remap = remap\n","\n","    def __call__(self, image, target):\n","        anno = target[\"annotations\"]\n","        anno = [obj for obj in anno if obj[\"category_id\"] in self.categories]\n","        if not self.remap:\n","            target[\"annotations\"] = anno\n","            return image, target\n","        anno = copy.deepcopy(anno)\n","        for obj in anno:\n","            obj[\"category_id\"] = self.categories.index(obj[\"category_id\"])\n","        target[\"annotations\"] = anno\n","        return image, target\n","\n","\n","def convert_coco_poly_to_mask(segmentations, height, width):\n","    masks = []\n","    for polygons in segmentations:\n","        rles = coco_mask.frPyObjects(polygons, height, width)\n","        mask = coco_mask.decode(rles)\n","        if len(mask.shape) < 3:\n","            mask = mask[..., None]\n","        mask = torch.as_tensor(mask, dtype=torch.uint8)\n","        mask = mask.any(dim=2)\n","        masks.append(mask)\n","    if masks:\n","        masks = torch.stack(masks, dim=0)\n","    else:\n","        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n","    return masks\n","\n","\n","class ConvertCocoPolysToMask(object):\n","    def __call__(self, image, target):\n","        w, h = image.size\n","\n","        image_id = target[\"image_id\"]\n","        image_id = torch.tensor([image_id])\n","\n","        anno = target[\"annotations\"]\n","\n","        anno = [obj for obj in anno if obj['iscrowd'] == 0]\n","\n","        boxes = [obj[\"bbox\"] for obj in anno]\n","        # guard against no boxes via resizing\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n","        boxes[:, 2:] += boxes[:, :2]\n","        boxes[:, 0::2].clamp_(min=0, max=w)\n","        boxes[:, 1::2].clamp_(min=0, max=h)\n","\n","        classes = [obj[\"category_id\"] for obj in anno]\n","        classes = torch.tensor(classes, dtype=torch.int64)\n","\n","        segmentations = [obj[\"segmentation\"] for obj in anno]\n","        masks = convert_coco_poly_to_mask(segmentations, h, w)\n","\n","        keypoints = None\n","        if anno and \"keypoints\" in anno[0]:\n","            keypoints = [obj[\"keypoints\"] for obj in anno]\n","            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n","            num_keypoints = keypoints.shape[0]\n","            if num_keypoints:\n","                keypoints = keypoints.view(num_keypoints, -1, 3)\n","\n","        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n","        boxes = boxes[keep]\n","        classes = classes[keep]\n","        masks = masks[keep]\n","        if keypoints is not None:\n","            keypoints = keypoints[keep]\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = classes\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        if keypoints is not None:\n","            target[\"keypoints\"] = keypoints\n","\n","        # for conversion to coco api\n","        area = torch.tensor([obj[\"area\"] for obj in anno])\n","        iscrowd = torch.tensor([obj[\"iscrowd\"] for obj in anno])\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        return image, target\n","\n","\n","def _coco_remove_images_without_annotations(dataset, cat_list=None):\n","    def _has_only_empty_bbox(anno):\n","        return all(any(o <= 1 for o in obj[\"bbox\"][2:]) for obj in anno)\n","\n","    def _count_visible_keypoints(anno):\n","        return sum(sum(1 for v in ann[\"keypoints\"][2::3] if v > 0) for ann in anno)\n","\n","    min_keypoints_per_image = 10\n","\n","    def _has_valid_annotation(anno):\n","        # if it's empty, there is no annotation\n","        if len(anno) == 0:\n","            return False\n","        # if all boxes have close to zero area, there is no annotation\n","        if _has_only_empty_bbox(anno):\n","            return False\n","        # keypoints task have a slight different critera for considering\n","        # if an annotation is valid\n","        if \"keypoints\" not in anno[0]:\n","            return True\n","        # for keypoint detection tasks, only consider valid images those\n","        # containing at least min_keypoints_per_image\n","        if _count_visible_keypoints(anno) >= min_keypoints_per_image:\n","            return True\n","        return False\n","\n","    assert isinstance(dataset, torchvision.datasets.CocoDetection)\n","    ids = []\n","    for ds_idx, img_id in enumerate(dataset.ids):\n","        ann_ids = dataset.coco.getAnnIds(imgIds=img_id, iscrowd=None)\n","        anno = dataset.coco.loadAnns(ann_ids)\n","        if cat_list:\n","            anno = [obj for obj in anno if obj[\"category_id\"] in cat_list]\n","        if _has_valid_annotation(anno):\n","            ids.append(ds_idx)\n","\n","    dataset = torch.utils.data.Subset(dataset, ids)\n","    return dataset\n","\n","\n","def convert_to_coco_api(ds):\n","    coco_ds = COCO()\n","    # annotation IDs need to start at 1, not 0, see torchvision issue #1530\n","    ann_id = 1\n","    dataset = {'images': [], 'categories': [], 'annotations': []}\n","    categories = set()\n","    for img_idx in range(len(ds)):\n","        # find better way to get target\n","        # targets = ds.get_annotations(img_idx)\n","        img, targets = ds[img_idx]\n","        image_id = targets[\"image_id\"].item()\n","        img_dict = {}\n","        img_dict['id'] = image_id\n","        img_dict['height'] = img.shape[-2]\n","        img_dict['width'] = img.shape[-1]\n","        dataset['images'].append(img_dict)\n","        bboxes = targets[\"boxes\"]\n","        bboxes[:, 2:] -= bboxes[:, :2]\n","        bboxes = bboxes.tolist()\n","        labels = targets['labels'].tolist()\n","        areas = targets['area'].tolist()\n","        iscrowd = targets['iscrowd'].tolist()\n","        if 'masks' in targets:\n","            masks = targets['masks']\n","            # make masks Fortran contiguous for coco_mask\n","            masks = masks.permute(0, 2, 1).contiguous().permute(0, 2, 1)\n","        if 'keypoints' in targets:\n","            keypoints = targets['keypoints']\n","            keypoints = keypoints.reshape(keypoints.shape[0], -1).tolist()\n","        num_objs = len(bboxes)\n","        for i in range(num_objs):\n","            ann = {}\n","            ann['image_id'] = image_id\n","            ann['bbox'] = bboxes[i]\n","            ann['category_id'] = labels[i]\n","            categories.add(labels[i])\n","            ann['area'] = areas[i]\n","            ann['iscrowd'] = iscrowd[i]\n","            ann['id'] = ann_id\n","            if 'masks' in targets:\n","                ann[\"segmentation\"] = coco_mask.encode(masks[i].numpy())\n","            if 'keypoints' in targets:\n","                ann['keypoints'] = keypoints[i]\n","                ann['num_keypoints'] = sum(k != 0 for k in keypoints[i][2::3])\n","            dataset['annotations'].append(ann)\n","            ann_id += 1\n","    dataset['categories'] = [{'id': i} for i in sorted(categories)]\n","    coco_ds.dataset = dataset\n","    coco_ds.createIndex()\n","    return coco_ds\n","\n","\n","def get_coco_api_from_dataset(dataset):\n","    for _ in range(10):\n","        if isinstance(dataset, torchvision.datasets.CocoDetection):\n","            break\n","        if isinstance(dataset, torch.utils.data.Subset):\n","            dataset = dataset.dataset\n","    if isinstance(dataset, torchvision.datasets.CocoDetection):\n","        return dataset.coco\n","    return convert_to_coco_api(dataset)\n","\n","\n","class CocoDetection(torchvision.datasets.CocoDetection):\n","    def __init__(self, img_folder, ann_file, transforms):\n","        super(CocoDetection, self).__init__(img_folder, ann_file)\n","        self._transforms = transforms\n","\n","    def __getitem__(self, idx):\n","        img, target = super(CocoDetection, self).__getitem__(idx)\n","        image_id = self.ids[idx]\n","        target = dict(image_id=image_id, annotations=target)\n","        if self._transforms is not None:\n","            img, target = self._transforms(img, target)\n","        return img, target\n","\n","\n","def get_coco(root, image_set, transforms, mode='instances'):\n","    anno_file_template = \"{}_{}2017.json\"\n","    PATHS = {\n","        \"train\": (\"train2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"train\"))),\n","        \"val\": (\"val2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"val\"))),\n","        # \"train\": (\"val2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"val\")))\n","    }\n","\n","    t = [ConvertCocoPolysToMask()]\n","\n","    if transforms is not None:\n","        t.append(transforms)\n","    transforms = T.Compose(t)\n","\n","    img_folder, ann_file = PATHS[image_set]\n","    img_folder = os.path.join(root, img_folder)\n","    ann_file = os.path.join(root, ann_file)\n","\n","    dataset = CocoDetection(img_folder, ann_file, transforms=transforms)\n","\n","    if image_set == \"train\":\n","        dataset = _coco_remove_images_without_annotations(dataset)\n","\n","    # dataset = torch.utils.data.Subset(dataset, [i for i in range(500)])\n","\n","    return dataset\n","\n","\n","def get_coco_kp(root, image_set, transforms):\n","    return get_coco(root, image_set, transforms, mode=\"person_keypoints\")"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"B5lXDUYfjxjM","executionInfo":{"status":"ok","timestamp":1620739085926,"user_tz":240,"elapsed":592,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["import math\n","import sys\n","import time\n","import torch\n","\n","import torchvision.models.detection.mask_rcnn\n","\n","import utils\n","\n","\n","def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n","    model.train()\n","    metric_logger = utils.MetricLogger(delimiter=\"  \")\n","    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n","    header = 'Epoch: [{}]'.format(epoch)\n","\n","    lr_scheduler = None\n","    if epoch == 0:\n","        warmup_factor = 1. / 1000\n","        warmup_iters = min(1000, len(data_loader) - 1)\n","\n","        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n","\n","    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        loss_dict = model(images, targets)\n","\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        # reduce losses over all GPUs for logging purposes\n","        loss_dict_reduced = utils.reduce_dict(loss_dict)\n","        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n","\n","        loss_value = losses_reduced.item()\n","\n","        if not math.isfinite(loss_value):\n","            print(\"Loss is {}, stopping training\".format(loss_value))\n","            print(loss_dict_reduced)\n","            sys.exit(1)\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        if lr_scheduler is not None:\n","            lr_scheduler.step()\n","\n","        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n","        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n","\n","    return metric_logger\n","\n","\n","def _get_iou_types(model):\n","    model_without_ddp = model\n","    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n","        model_without_ddp = model.module\n","    iou_types = [\"bbox\"]\n","    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n","        iou_types.append(\"segm\")\n","    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n","        iou_types.append(\"keypoints\")\n","    return iou_types\n","\n","\n","@torch.no_grad()\n","def evaluate(model, data_loader, device):\n","    n_threads = torch.get_num_threads()\n","    # FIXME remove this and make paste_masks_in_image run on the GPU\n","    torch.set_num_threads(1)\n","    cpu_device = torch.device(\"cpu\")\n","    model.eval()\n","    metric_logger = utils.MetricLogger(delimiter=\"  \")\n","    header = 'Test:'\n","\n","    coco = get_coco_api_from_dataset(data_loader.dataset)\n","    iou_types = _get_iou_types(model)\n","    coco_evaluator = CocoEvaluator(coco, iou_types)\n","\n","    for images, targets in metric_logger.log_every(data_loader, 100, header):\n","        images = list(img.to(device) for img in images)\n","\n","        if torch.cuda.is_available():\n","            torch.cuda.synchronize()\n","        model_time = time.time()\n","        outputs = model(images)\n","\n","        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n","        model_time = time.time() - model_time\n","\n","        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n","        evaluator_time = time.time()\n","        coco_evaluator.update(res)\n","        evaluator_time = time.time() - evaluator_time\n","        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n","\n","    # gather the stats from all processes\n","    metric_logger.synchronize_between_processes()\n","    print(\"Averaged stats:\", metric_logger)\n","    coco_evaluator.synchronize_between_processes()\n","\n","    # accumulate predictions from all images\n","    coco_evaluator.accumulate()\n","    coco_evaluator.summarize()\n","    torch.set_num_threads(n_threads)\n","    return coco_evaluator"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":351},"id":"8riDHJ_DcM6w","executionInfo":{"status":"error","timestamp":1620739091408,"user_tz":240,"elapsed":1637,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"97ac7f71-526c-4165-d163-0905f637f994"},"source":["import utils\n","\n","\n","# train on the GPU or on the CPU, if a GPU is not available\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# our dataset has two classes only - background and person\n","num_classes = 2\n","\n","# get the model using our helper function\n","model = get_model_instance_segmentation(num_classes)\n","\n","# move model to the right device\n","model.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","# and a learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                                step_size=3,\n","                                                gamma=0.1)\n","\n","# let's train it for 10 epochs\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=10)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","    evaluate(model, test_dataloader, device=device)\n","\n","print(\"That's it!\")\n"],"execution_count":50,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-09d2d91343f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-49-f82e70253da4>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmetric_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetricLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"  \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_meter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSmoothedValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'{value:.6f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Epoch: [{}]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'utils' has no attribute 'MetricLogger'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["ede5910cfaa940c7a7a5f9ceeeb3054c","e1503e9c8aa345f0851ab7283b583640","4ce977973cd9484182541142c9df9e86","13ef327071034e8b9209c02e56652159","c567534c1c9d456eb5404e12262a2861","8f77aeeb1b014bfab9a382ba68c837d1","ca44d045c52740bfb6ab54afd48ab7c0","0a72447bed1749d981f97c7e35966ec4"]},"id":"SgywEt59b7j1","executionInfo":{"status":"ok","timestamp":1620707431663,"user_tz":240,"elapsed":2302,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"d50ab270-560d-4a78-d016-d38b080ffbde"},"source":["\n","import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n"," \n","torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=3, pretrained_backbone=True)\n"," \n","def get_object_detection_model(num_classes):\n","    # load an object detection model pre-trained on COCO\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    \n","    # replace the classifier with a new one, that has num_classes which is user-defined\n","    num_classes = 2  # 3 class (mark_type_1，mark_type_2) + background\n"," \n","    # get the number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n"," \n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"," \n","    return model"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ede5910cfaa940c7a7a5f9ceeeb3054c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tOf9Fga1sOg0"},"source":["class RandomHorizontalFlip(object):\n","    def __init__(self, prob):\n","        self.prob = prob\n","\n","    def __call__(self, image, target):\n","        if random.random() < self.prob:\n","            height, width = image.shape[-2:]\n","            image = image.flip(-1)\n","            bbox = target[\"boxes\"]\n","            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n","            target[\"boxes\"] = bbox\n","            if \"masks\" in target:\n","                target[\"masks\"] = target[\"masks\"].flip(-1)\n","            if \"keypoints\" in target:\n","                keypoints = target[\"keypoints\"]\n","                keypoints = _flip_coco_person_keypoints(keypoints, width)\n","                target[\"keypoints\"] = keypoints\n","        return image, target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8UlVNcqJscC2","executionInfo":{"status":"ok","timestamp":1620707774804,"user_tz":240,"elapsed":9257,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"21217201-5109-4b01-c0f9-4fba96d7511c"},"source":["!pip install cython\n","!pip install pycocotools\n","!pip install torchvision\n","!pip install torch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.22)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (2.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.8.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"id":"IOQfPCfTsXZW","executionInfo":{"status":"error","timestamp":1620707776384,"user_tz":240,"elapsed":362,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"7d20adb2-40a3-47c7-9c78-dc593c002af1"},"source":["from torchvision import datasets, transforms\n","import utils\n","import transforms as T\n","from engine import train_one_epoch, evaluate\n","# utils, transforms, engine were just downloadedUtils.py,transforms.py,engine.py\n"," \n","def get_transform(train):\n","    transforms = []\n","    # converts the image, a PIL image, into a PyTorch Tensor\n","    transforms.append(T.ToTensor())\n","    if train:\n","        # during training, randomly flip the training images\n","        # and ground-truth for data augmentation\n","        # 50% chance of flipping horizontally\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n"," \n","    return T.Compose(transforms)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-0ab25fc0e649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# utils, transforms, engine were just downloadedUtils.py,transforms.py,engine.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transforms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msafe_html\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msafe_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbodyfinder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# modules = [\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     'st',             # zopish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     'rest',           # docutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transforms/safe_html.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msgmllib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGMLParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGMLParseError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcgi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msafeToInt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sgmllib'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LBzOkmEdeNxL","executionInfo":{"status":"ok","timestamp":1620737569555,"user_tz":240,"elapsed":8638,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"08592de5-10d3-4c9c-a5ef-a3c0ef545cf3"},"source":["import torchvision\n","dummy_img = torch.zeros((1, 3, 800, 800)).float()\n","print(dummy_img)\n","model = torchvision.models.vgg16(pretrained=True)\n","fe = list(model.features)\n","req_features = []\n","k = dummy_img.clone()\n","for i in fe:\n","    k = i(k)\n","    if k.size()[2] < 800//16:\n","        break\n","    fe.append(i)\n","    out_channels = k.size()[1]\n","print(len(req_features)) #30\n","print(out_channels) # 512\n","faster_rcnn_fe_extractor = nn.Sequential(*req_features)\n","out_map = faster_rcnn_fe_extractor(dummy_img)\n","print(out_map.size())"],"execution_count":16,"outputs":[{"output_type":"stream","text":["tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          ...,\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.]],\n","\n","         [[0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          ...,\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.]],\n","\n","         [[0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          ...,\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n","0\n","512\n","torch.Size([1, 3, 800, 800])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s4QNtR7Lfn1J","executionInfo":{"status":"ok","timestamp":1620737691872,"user_tz":240,"elapsed":527,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["import torchvision.transforms as transforms\n"],"execution_count":17,"outputs":[]}]}