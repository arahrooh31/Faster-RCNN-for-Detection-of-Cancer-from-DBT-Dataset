{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Faster-RCNN-1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPYmECvddJMWGEaSDkeLy7p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nju050s_07eo","executionInfo":{"status":"ok","timestamp":1620262854628,"user_tz":240,"elapsed":9222,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"eca7fdd3-01d0-4bf7-8b02-a88ab6502b5b"},"source":["!pip install pydicom\n","!pip install pickle5\n","!pip install dicom2nifti\n","!pip install hiddenlayer"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pydicom in /usr/local/lib/python3.7/dist-packages (2.1.2)\n","Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n","Requirement already satisfied: dicom2nifti in /usr/local/lib/python3.7/dist-packages (2.2.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from dicom2nifti) (1.19.5)\n","Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (from dicom2nifti) (3.0.2)\n","Requirement already satisfied: pydicom>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from dicom2nifti) (2.1.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from dicom2nifti) (1.4.1)\n","Requirement already satisfied: hiddenlayer in /usr/local/lib/python3.7/dist-packages (0.3)\n","\u001b[31mERROR: Invalid requirement: '._utilis'\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TucEWtfDI8U7","executionInfo":{"status":"ok","timestamp":1620263091207,"user_tz":240,"elapsed":949,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["#Dependencies\n","from torchvision.models.detection import FasterRCNN\n","\n","import os\n","import sys\n","import pandas as pd\n","from glob import glob\n","import pickle5 as pickle\n","import pydicom\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from matplotlib.patches import Rectangle\n","import numpy as np\n","from pathlib import Path\n","import six\n","import csv\n","import logging\n","import math\n","from mpl_toolkits.mplot3d import Axes3D\n","import pandas as pd\n","import dicom2nifti\n","import nibabel as nib\n","from PIL import Image \n","import scipy.misc\n","from scipy import stats\n","from sklearn.model_selection import train_test_split\n","from datetime import datetime\n","from scipy.spatial import distance\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import torchvision\n","from torchvision.models.detection import RetinaNet\n","from torchvision.models.detection.anchor_utils import AnchorGenerator\n","import torch.optim as optim\n","import hiddenlayer as hl\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import torchvision\n","from torchvision.models.detection import RetinaNet\n","from torchvision.models.detection.anchor_utils import AnchorGenerator\n","import torch.optim as optim\n","\n","from torch import nn\n","import torch.nn.functional as F\n","\n","from torchvision.ops import MultiScaleRoIAlign\n","\n","import numpy as np\n","import cv2\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torch.utils.data import DataLoader, Dataset\n","\n"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m2TRJSP8RM1q"},"source":["DATALOADER"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CImN33EmDMEM","executionInfo":{"status":"ok","timestamp":1620262094279,"user_tz":240,"elapsed":48668,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"c53d7f79-28ab-4b07-88fc-40b8a871737c"},"source":["# Mount the google drive so that we can access the data stored in the drive\n","from google.colab import drive\n","from google.colab import files\n","drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9mD7iES3wpMz","executionInfo":{"status":"ok","timestamp":1620262094281,"user_tz":240,"elapsed":48664,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["BATCH_SIZE = 6"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JSnJ9kl66W3","executionInfo":{"status":"ok","timestamp":1620262097402,"user_tz":240,"elapsed":51780,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"c0500625-9d4b-4a22-d1fa-6fc59f9cb369"},"source":["img_arrays_path = '/content/gdrive/MyDrive/BE223C/DBT_DATA/IMG_ARRAYS/'\n","boxes_path = '/content/gdrive/MyDrive/BE223C/DBT_DATA/TRAINING_DATA/BCS-DBT boxes-train-v2.csv'\n","df_boxes = pd.read_csv(boxes_path)\n","sum(df_boxes['Class'] == 'cancer') "],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["87"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"wPJqZcbEwt53","executionInfo":{"status":"ok","timestamp":1620262097403,"user_tz":240,"elapsed":51776,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["labels_unsplit = []\n","for i in range(len(df_boxes)):\n","  if df_boxes['Class'][i] == 'cancer':\n","    labels_unsplit.append(1)\n","  else:\n","    labels_unsplit.append(0)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ip7B4TUQ8se","executionInfo":{"status":"ok","timestamp":1620262195303,"user_tz":240,"elapsed":149671,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["def select_cases_with_boxes(img_arrays_path, boxes_path):\n","  df_boxes = pd.read_csv(boxes_path)\n","  img_names = glob(os.path.join(img_arrays_path, '*.pickle'))\n","  img_names[0].split('_')[5]\n","  img_names_valid = []\n","  for i in range(len(df_boxes)):\n","    for j in range(len(img_names)):\n","      img_studyID = img_names[j].split('_')[3]\n","      view = img_names[j].split('_')[4]\n","      if df_boxes['StudyUID'][i] == img_studyID and df_boxes['View'][i] == view:\n","        img_names_valid.append(img_names[j])\n","  return img_names_valid \n","\n","img_names = glob(os.path.join(img_arrays_path, '*.pickle'))\n","\n","class BreastDataset(Dataset):\n","  def __init__(self, df_boxes, img_names, batch_size, transform=None):\n","    self.df_boxes = df_boxes\n","    self.img_names = img_names\n","    self.batch_size = batch_size\n","\n","  def __len__(self):\n","    return len(self.df_boxes)\n","\n","  def __getitem__(self, index):\n","    def padding(array, xx, yy):\n","    \n","      h = array.shape[0]\n","      w = array.shape[1]\n","\n","      a = (xx - h) // 2\n","      aa = xx - a - h\n","\n","      b = (yy - w) // 2\n","      bb = yy - b - w\n","\n","      return np.pad(array, pad_width=((a, aa), (b, bb)), mode='constant')\n","    # Get the image slice where the bounding box resides\n","    img_array = pickle.load(open(self.img_names[index], \"rb\" ))\n","    box_slice_index = self.df_boxes['Slice'][index]\n","    img_slice = img_array[box_slice_index, :, :]\n","    # Normalize the image (min max norm)\n","    img_slice = (img_slice - img_slice.min()) / (img_slice.max() - img_slice.min())\n","    img_slice = padding(img_slice, 2457, 1996)\n","    # Convert into a 3 channel image\n","    img_slice = np.stack((img_slice,)*3, axis = 0)\n","    img_slice = torch.from_numpy(img_slice.astype(np.float32))\n","    img_slice = img_slice.to(device = torch.device('cuda:0'))\n","\n","    # Create the label based on the file name\n","    if 'Benign' in os.path.basename(self.img_names[index]):\n","      label = torch.tensor([0])\n","      label = label.to(device = torch.device('cuda:0'))\n","    elif 'Cancer' in os.path.basename(self.img_names[index]):\n","      label = torch.tensor([1])\n","      label = label.to(device = torch.device('cuda:0'))\n","    \n","    # Get the bounding box \n","    box = [df_boxes['X'][index], df_boxes['Y'][index], df_boxes['Width'][index], df_boxes['Height'][index]]\n","\n","    # transform from [x, y, w, h] to [x1, y1, x2, y2]\n","    box[2] = box[0] + box[2]\n","    box[3] = box[1] + box[3]\n","    \n","    box = torch.as_tensor(box)\n","    box = box.to(device = torch.device('cuda:0'))\n","    target = {}\n","    target[\"boxes\"] = box\n","    target[\"labels\"] = label\n","\n","    \n","    return img_slice, [target] * self.batch_size"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0JcITJm1PWu","executionInfo":{"status":"ok","timestamp":1620241435758,"user_tz":240,"elapsed":272,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"e8de2a74-b720-4085-a818-81f243ac1513"},"source":["len(img_names)\n","print(img_names[0].split('_')[4])"],"execution_count":41,"outputs":[{"output_type":"stream","text":["rcc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5FMJP9xVw2w3","executionInfo":{"status":"ok","timestamp":1620262198740,"user_tz":240,"elapsed":153104,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["img_names_valid = select_cases_with_boxes(img_arrays_path, boxes_path)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"90rr9tglw4mo"},"source":["img_names_valid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FYEPGIscRfPS","executionInfo":{"status":"ok","timestamp":1620262198746,"user_tz":240,"elapsed":153099,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["breast_dataset = BreastDataset(df_boxes, img_names_valid, BATCH_SIZE)\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"XQd4ne7mxBlK","executionInfo":{"status":"ok","timestamp":1620262198747,"user_tz":240,"elapsed":153098,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["\n","# For testing the code only. Using 2 samples.\n","# toy_dataset = BreastDataset(df_boxes.head(10), img_names_valid[0:10])\n","# toy_dataloader = DataLoader(toy_dataset, batch_size = 2, shuffle = True, num_workers = 0)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"8U33FWK9xDMx","executionInfo":{"status":"ok","timestamp":1620262198747,"user_tz":240,"elapsed":153094,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["# Stratified train, val, test split. First do train-test split, and then take out val from the train\n","train_indices, test_indices = train_test_split(\n","    np.arange(len(labels_unsplit)), test_size = 0.1, shuffle = True, stratify = labels_unsplit, random_state = 123)\n","train_indices, val_indices = train_test_split(\n","    train_indices, test_size = 0.1, shuffle = True, stratify = [labels_unsplit[i] for i in train_indices], random_state = 123)\n","\n","train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n","val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n","test_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n","\n","train_dataloader = DataLoader(breast_dataset, batch_size = BATCH_SIZE, shuffle = False, sampler = train_sampler, num_workers = 0)\n","val_dataloader = DataLoader(breast_dataset, batch_size = BATCH_SIZE, shuffle = False, sampler = val_sampler, num_workers = 0)\n","test_dataloader = DataLoader(breast_dataset, batch_size = BATCH_SIZE, shuffle = False, sampler = test_sampler, num_workers = 0)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nrIrGwsvRFwc"},"source":["TRAINING THE MODEL"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"id":"k8jhBSdcRH9T","executionInfo":{"status":"error","timestamp":1620264549987,"user_tz":240,"elapsed":79954,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}},"outputId":"792f25e8-9103-4460-db0c-640c33fdd90b"},"source":["def train_val(resume_training, num_epochs):\n","  num_epochs = num_epochs\n","  start_epoch = 0  # start from epoch 0 or last epoch\n","\n","  fasterrcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(num_classes = 2, pretrained = False, pretrained_backbone = True) \n","  fasterrcnn = fasterrcnn.to(device = torch.device('cuda:0'))\n","  optimizer = optim.SGD(fasterrcnn.parameters(), lr=0.001, momentum=0.9)\n","\n","  global best_loss\n","  best_loss = float('inf')  # best test loss\n","  n_epochs_stop = 10\n","  epochs_no_improve = 0\n","  min_delta = 0.5\n","  early_stop = False\n","\n","  if resume_training:\n","    print('==> Resuming from checkpoint..')\n","    checkpoint = torch.load('/content/gdrive/MyDrive/223C_breast_cancer/code/model_50_epochs_early_stopping_batch_6_momentum6.pth')\n","    fasterrcnn.load_state_dict(checkpoint['net'])\n","    best_loss = checkpoint['loss']\n","    start_epoch = checkpoint['epoch']\n","\n","  total_epoch_loss_train = []\n","  total_epoch_loss_val = []\n","  start_time = datetime.now()\n","  for epoch in range(start_epoch, start_epoch + num_epochs): \n","    # Training phase\n","    fasterrcnn.train()\n","    epoch_loss = 0\n","    epoch_classification_loss = 0\n","    epoch_regression_loss = 0\n","    for batch_num, (image, targets) in enumerate(train_dataloader):\n","\n","      optimizer.zero_grad()\n","      outputs = fasterrcnn.forward(image, targets)\n","      classification_loss = outputs['classification']\n","      regression_loss = outputs['bbox_regression']\n","      loss = classification_loss + regression_loss \n","      loss.backward()\n","      \n","      optimizer.step()\n","      epoch_classification_loss += float(classification_loss)\n","      epoch_regression_loss += float(epoch_regression_loss)\n","      epoch_loss += float(loss)\n","\n","    total_epoch_loss_train.append(epoch_loss/len(train_dataloader))\n","    print('\\n Training')\n","    print('Epoch: {} | Regression loss: {:1.5f} | Classification loss: {:1.5f} | Epoch loss: {:1.5f}'.format(\n","        epoch, regression_loss, epoch_classification_loss/(batch_num + 1), epoch_loss/(batch_num + 1)))\n","        # Validation phase\n","    epoch_loss_val = 0\n","    epoch_classification_loss_val = 0\n","    epoch_regression_loss_val = 0\n","    for batch_num, (image, targets) in enumerate(val_dataloader):\n","      outputs_val = fasterrcnn.forward(image, targets)\n","      classification_loss_val = outputs_val['classification']\n","      regression_loss_val = outputs_val['bbox_regression']\n","      loss_val = classification_loss_val + regression_loss_val\n","      \n","      epoch_classification_loss_val += float(classification_loss_val)\n","      epoch_regression_loss_val += float(epoch_regression_loss_val)\n","      epoch_loss_val += float(loss_val)\n","\n","    total_epoch_loss_val.append(epoch_loss_val/len(val_dataloader))\n","    print('\\n Validation')\n","    print('Epoch: {} | Regression loss: {:1.5f} | Classification loss: {:1.5f} | Epoch loss: {:1.5f}'.format(\n","        epoch, regression_loss_val, epoch_classification_loss_val/(batch_num + 1), epoch_loss_val/(batch_num + 1)))\n","    \n","    # # Predictions from the model\n","    # fasterrcnn.eval()\n","    # predictions = fasterrcnn.forward(image)\n","    # print(predictions)\n","    fasterrcnn.train()\n","    val_loss = epoch_loss_val/(len(val_dataloader))\n","    \n","    # Plot loss for each epoch\n","    plt.plot(range(epoch+1), total_epoch_loss_train, label = 'Training')\n","    plt.plot(range(epoch+1), total_epoch_loss_val, label = 'Validation')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.show()\n","\n","    # Check if need to do early stopping\n","    if best_loss - val_loss > min_delta:\n","      best_loss = val_loss\n","      epochs_no_improve = 0\n","    else:\n","      epochs_no_improve += 1\n","    if epochs_no_improve == n_epochs_stop:\n","      print('Early stopping!' )\n","      early_stop = True\n","      break\n","    else:\n","      print('Save model')\n","      state = {\n","        'net': fasterrcnn.state_dict(),\n","        'loss': val_loss,\n","        'epoch': epoch,\n","      }\n","      torch.save(state, '/content/gdrive/MyDrive/223C_breast_cancer/code/model_50_epochs_early_stopping_batch_6_momentum6.pth')\n","\n","\n","  # After finishing all the epochs\n","  end_time = datetime.now()\n","  print('Duration: {}'.format(end_time - start_time))\n","\n","train_val(resume_training = False, num_epochs = 50)"],"execution_count":25,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-8fe4f19fd271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Duration: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mtrain_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-8fe4f19fd271>\u001b[0m in \u001b[0;36mtrain_val\u001b[0;34m(resume_training, num_epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasterrcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m       \u001b[0mclassification_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classification'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mregression_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bbox_regression'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mregression_targets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             loss_classifier, loss_box_reg = fastrcnn_loss(\n\u001b[0;32m--> 761\u001b[0;31m                 class_logits, box_regression, labels, regression_targets)\n\u001b[0m\u001b[1;32m    762\u001b[0m             losses = {\n\u001b[1;32m    763\u001b[0m                 \u001b[0;34m\"loss_classifier\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mfastrcnn_loss\u001b[0;34m(class_logits, box_regression, labels, regression_targets)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mregression_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregression_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mclassification_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# get indices that correspond to the regression targets for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2386\u001b[0m         )\n\u001b[1;32m   2387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: multi-target not supported at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15"]}]},{"cell_type":"code","metadata":{"id":"cyTzo1rKPM9F","executionInfo":{"status":"ok","timestamp":1620264094700,"user_tz":240,"elapsed":294,"user":{"displayName":"AL RAHROOH","photoUrl":"","userId":"07687805089168633614"}}},"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n"," \n","      \n","def get_object_detection_model(num_classes):\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    \n","    num_classes = 2  # 3 class (mark_type_1，mark_type_2) + background\n","\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n"," \n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"," \n","    return model\n","\n","\n","\n"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFen4yWrRCyf"},"source":["!pip install engine"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"smaWE_mNPzW2"},"source":["from engine import train_one_epoch, evaluate\n","import utils\n","\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","num_classes = 2\n","\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=num_classes, pretrained_backbone=True)  # Or get_object_detection_model(num_classes)\n","model.to(device)\n","\n","params = [p for p in model.parameters() if p.requires_grad]\n","\n","optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9, weight_decay=0.0005)\n","\n","lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n","\n","\n","num_epochs = 31\n","\n","for epoch in range(num_epochs):\n","    # train for one epoch, printing every 10 iterations\n","    # Engine.pyTrain_ofOne_The epoch function takes both images and targets. to(device)\n","    train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=50)\n","\n","    # update the learning rate\n","    lr_scheduler.step()\n","\n","    # evaluate on the test dataset    \n","    evaluate(model, data_loader_test, device=device)    \n","    \n","    print('')\n","    print('==================================================')\n","    print('')\n","\n","print(\"That's it!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gfM1keT3RBv4"},"source":["!pip install pycocotools"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r1BlptDUTs3A"},"source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n"," \n","\n","# Load the pre-trained pre-trained model on COCO\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","#This operation you really need to have fixed parameters\n","for param in model.parameters():\n","    param.requires_grad = False\n","    \n","# Replace the classifier with a new classifier with user-defined num_classes\n","num_classes = 2  # 1 class (person) + background\n","\n","# Get the number of input parameters of the classifier\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# Replace the pre-trained head with a new head\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","\n","# Load the pre-trained model for classification and return only features\n","backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n","# FasterRCNN needs to know the number of output channels in the backbone network. For mobilenet_v2, it is 1280, so we need to add it here\n","backbone.out_channels = 1280\n"," \n","# We let RPN generate 5 x 3 Anchors (with 5 different sizes and 3 different aspect ratios) at each spatial position\n","# We have a tuple [tuple [int]], because each feature map may have a different size and aspect ratio\n","anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n","                                   aspect_ratios=((0.5, 1.0, 2.0),))\n"," \n","# Define the feature map that we will use to perform the cropping of the region of interest, and the size of the crop after rescaling.\n","# If your trunk returns Tensor, featmap_names should be [0].\n","# More generally, the trunk should return OrderedDict [Tensor]\n","# And in featmap_names, you can choose the feature map you want to use.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],#featmap_names=['0']\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","# Put these pieces in the FasterRCNN model\n","model = FasterRCNN(backbone,\n","                   num_classes=2,\n","                   rpn_anchor_generator=anchor_generator,\n","                   box_roi_pool=roi_pooler)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bZbPOQKQftRM"},"source":["import pathlib\n","\n","import albumentations as A\n","import numpy as np\n","from torch.utils.data import DataLoader\n","\n","from datasets import ObjectDetectionDataSet\n","from transformations import ComposeDouble, Clip, AlbumentationWrapper, FunctionWrapperDouble\n","from transformations import normalize_01\n","from utils import get_filenames_of_path, collate_double\n","\n","params = {'BATCH_SIZE': 2,\n","          'LR': 0.001,\n","          'PRECISION': 32,\n","          'CLASSES': 2,\n","          'SEED': 42,\n","          'PROJECT': 'Heads',\n","          'EXPERIMENT': 'heads',\n","          'MAXEPOCHS': 500,\n","          'BACKBONE': 'resnet34',\n","          'FPN': False,\n","          'ANCHOR_SIZE': ((32, 64, 128, 256, 512),),\n","          'ASPECT_RATIOS': ((0.5, 1.0, 2.0),),\n","          'MIN_SIZE': 1024,\n","          'MAX_SIZE': 1024,\n","          'IMG_MEAN': [0.485, 0.456, 0.406],\n","          'IMG_STD': [0.229, 0.224, 0.225],\n","          'IOU_THRESHOLD': 0.5\n","          }\n","\n","\n","from faster_RCNN import get_fasterRCNN_resnet\n","\n","model = get_fasterRCNN_resnet(num_classes=params['CLASSES'],\n","                              backbone_name=params['BACKBONE'],\n","                              anchor_size=params['ANCHOR_SIZE'],\n","                              aspect_ratios=params['ASPECT_RATIOS'],\n","                              fpn=params['FPN'],\n","                              min_size=params['MIN_SIZE'],\n","                              max_size=params['MAX_SIZE'])\n","\n","from faster_RCNN import FasterRCNN_lightning\n","\n","task = FasterRCNN_lightning(model=model, lr=params['LR'], iou_threshold=params['IOU_THRESHOLD'])\n","\n","from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n","\n","checkpoint_callback = ModelCheckpoint(monitor='Validation_mAP', mode='max')\n","learningrate_callback = LearningRateMonitor(logging_interval='step', log_momentum=False)\n","early_stopping_callback = EarlyStopping(monitor='Validation_mAP', patience=50, mode='max')\n","\n","# trainer init\n","from pytorch_lightning import Trainer\n","\n","trainer = Trainer(gpus=1,\n","                  precision=params['PRECISION'],  # try 16 with enable_pl_optimizer=False\n","                  callbacks=[checkpoint_callback, learningrate_callback, early_stopping_callback],\n","                  default_root_dir='heads',  # where checkpoints are saved to\n","                  logger=neptune_logger,\n","                  log_every_n_steps=1,\n","                  num_sanity_val_steps=0,\n","                  enable_pl_optimizer=False,  # False seems to be necessary for half precision\n","                  )"],"execution_count":null,"outputs":[]}]}