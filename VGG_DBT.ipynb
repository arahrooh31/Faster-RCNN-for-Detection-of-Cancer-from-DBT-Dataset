{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_DBT",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcgh9i3Am3x0KplWYhLZ5g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arahrooh31/UCLA_BE223C/blob/Keane_temp/VGG_DBT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iix74kj_aDNY",
        "outputId": "a1156797-4ee6-4a62-e40f-1af584a6aa49"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import gc  #debug memory leaks in matplotlib\n",
        "import csv #read in description files\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "#\n",
        "# Read Data from google drive\n",
        "#\n",
        "\n",
        "from google.colab import drive #for loading gdrive data\n",
        "from google.colab import files\n",
        "\n",
        "# install dependencies not included by Colab\n",
        "# use pip3 to ensure compatibility w/ Google Deep Learning Images \n",
        "!pip3 install -q pydicom \n",
        "!pip3 install -q tqdm \n",
        "!pip3 install -q imgaug\n",
        "!pip3 install -q pickle5\n",
        "\n",
        "import pydicom #to read dicom files\n",
        "from pydicom import dcmread\n",
        "import pickle5 as pickle; #generic storage of image arra\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "#show model design parameters with torchsummary\n",
        "import torchsummary\n",
        "from torchsummary import summary\n",
        "from torch import FloatTensor\n",
        "from torch import tensor\n",
        "\n",
        "\n",
        "### Enable GPU, if present\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if (train_on_gpu):\n",
        "    !nvidia-smi -L\n",
        "    !nvidia-smi \n",
        "    dev=torch.device(\"cuda\")\n",
        "else:\n",
        "    print('GPU NOT FOUND!!! USING CPU INSTEAD!!!!!')\n",
        "\n",
        "train_on_tpu =0 #enable TPUs\n",
        "if ((train_on_gpu == 0 ) and train_on_tpu == 1):\n",
        "    !pip3 install -q cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl\n",
        "    # imports the torch_xla package\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    dev = xm.xla_device()\n",
        "    print('!!!! USING TPU!!!!')\n",
        "#\n",
        "# Load data from google drive\n",
        "#\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data_dir = '/content/gdrive/My Drive/DBT_DATA/IMG_ARRAYS'\n",
        "patch_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES' \n",
        "model_dir = '/content/gdrive/My Drive/BE223C_SPRING_2021/MODEL_SAVE'\n",
        "tensorboard_dir = '/content/gdrive/My Drive/BE223C_SPRING_2021/TENSORBOARD_SUMMARIES'\n",
        "#DBT_DATA/TRAINING_DATA/manifest-1605042674814/Breast-Cancer-Screening-DBT'\n",
        "#png_dir = '/content/gdrive/My Drive/BE223C_SPRING_2021/PNG_IMAGES'\n",
        "#png_annotation_dir = '/content/gdrive/My Drive/BE223C_SPRING_2021/PNG_ANNOTATION_IMAGES'\n",
        "#image_save_dir = '/content/gdrive/My Drive/DBT_DATA/IMG_ARRAYS' #matrix img data\n",
        "#'/content/gdrive/My Drive/BE223C_SPRING_2021/IMAGE_ARRAY'\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU NOT FOUND!!! USING CPU INSTEAD!!!!!\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G49j_VOIdgA6"
      },
      "source": [
        "#\n",
        "# GET FULL LIST OF FILES IN IMAGE ARRAY DIRECTORY\n",
        "#\n",
        "use_patch_files = 1\n",
        "if (use_patch_files == 0):\n",
        "    raw_files = os.listdir(data_dir)\n",
        "    print('found #files: ',len(raw_files))\n",
        "else: #patches broken up into directories\n",
        "    category_folders = os.listdir(patch_dir)\n",
        "    #raw_files = os.listdir(data_dir)\n",
        "    #print('found #files: ',len(raw_files))    \n",
        "\n",
        "if (0):\n",
        "    #create fake patches for now\n",
        "    patch_dict = {}\n",
        "    for counter,filename in enumerate(raw_files):\n",
        "        #load full array\n",
        "        full_filename = os.path.join(data_dir,filename)\n",
        "        img_data = pickle.load( open( full_filename, \"rb\" ) )\n",
        "        patch_data = img_data[0:3,:,:]\n",
        "        patch_dict[counter]= patch_data\n",
        "        print(full_filename,np.shape(patch_data))\n",
        "\n",
        "        if (counter > 0):\n",
        "            break\n",
        "        \n",
        "### SKIP ACTIONABLE FOLDER\n",
        "### Remove actionable folder item from list, since we're no longer using that data\n",
        "category_folders.remove('ACTIONABLE')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSX1m2-J6Wtv",
        "outputId": "9350b5bb-b70b-421e-c733-16171cc29298"
      },
      "source": [
        "for ii in category_folders:\n",
        "    print(ii)\n",
        "    flist = os.listdir(os.path.join(patch_dir,ii))\n",
        "\n",
        "temp = flist[0].split(sep='_')\n",
        "print(temp[3])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CANCER\n",
            "BENIGN\n",
            "NORMAL\n",
            "Normal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lt9XjPYGlvEm",
        "outputId": "62933d20-204b-44ec-8371-ff985d4ef34b"
      },
      "source": [
        "#generate full file list for use in indexing the dataloader\n",
        "#this replaces the older loader, which was only inputting Normal\n",
        "full_file_list = [] #store the full filename of every file\n",
        "full_category_name = []\n",
        "for category_folder in category_folders:\n",
        "            print('------------- ',category_folder)\n",
        "            file_list = os.listdir(os.path.join(patch_dir,category_folder))\n",
        "            cat_count = 0\n",
        "            for file_name in file_list:\n",
        "                cat_count = cat_count + 1\n",
        "                full_category_name.append(category_folder)\n",
        "                full_file_list.append(file_name)\n",
        "                print(os.path.join(category_folder,file_name))\n",
        "full_file_count = len(full_file_list)\n",
        "print(len(full_file_list))\n",
        "full_file_list[0]\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------  CANCER\n",
            "CANCER/DBT-P03915_DBT-S05004_lcc_Cancer_s25_cx417_cy1039_244_244.pickle\n",
            "CANCER/DBT-P04026_DBT-S01650_rcc1_Cancer_s20_cx1640_cy1074_244_244.pickle\n",
            "CANCER/DBT-P04026_DBT-S01650_rmlo_Cancer_s18_cx1532_cy1382_244_244.pickle\n",
            "CANCER/DBT-P04090_DBT-S01718_rmlo_Cancer_s59_cx1212_cy825_244_244.pickle\n",
            "CANCER/DBT-P04090_DBT-S01718_rcc_Cancer_s63_cx1368_cy1342_244_244.pickle\n",
            "CANCER/DBT-P04372_DBT-S04281_lcc_Cancer_s34_cx963_cy1301_244_244.pickle\n",
            "CANCER/DBT-P04372_DBT-S04281_lmlo_Cancer_s39_cx1197_cy1391_244_244.pickle\n",
            "CANCER/DBT-P04631_DBT-S05515_lcc_Cancer_s9_cx546_cy1059_244_244.pickle\n",
            "CANCER/DBT-P04631_DBT-S05515_lmlo_Cancer_s18_cx692_cy1761_244_244.pickle\n",
            "CANCER/DBT-P04710_DBT-S03227_rmlo_Cancer_s42_cx1467_cy1509_244_244.pickle\n",
            "CANCER/DBT-P04710_DBT-S03227_rcc_Cancer_s19_cx1420_cy1431_244_244.pickle\n",
            "CANCER/DBT-P04901_DBT-S05032_rcc_Cancer_s35_cx1534_cy961_244_244.pickle\n",
            "CANCER/DBT-P04901_DBT-S05032_rmlo_Cancer_s35_cx1252_cy1064_244_244.pickle\n",
            "CANCER/DBT-P05030_DBT-S05569_rcc_Cancer_s28_cx1737_cy698_244_244.pickle\n",
            "CANCER/DBT-P05030_DBT-S05569_rmlo_Cancer_s24_cx1465_cy624_244_244.pickle\n",
            "CANCER/DBT-P05014_DBT-S04931_rmlo_Cancer_s18_cx1231_cy1152_244_244.pickle\n",
            "CANCER/DBT-P05014_DBT-S04931_rcc_Cancer_s29_cx1256_cy705_244_244.pickle\n",
            "CANCER/DBT-P05056_DBT-S01839_rcc_Cancer_s44_cx1642_cy661_244_244.pickle\n",
            "CANCER/DBT-P05056_DBT-S01839_rmlo_Cancer_s22_cx1167_cy499_244_244.pickle\n",
            "CANCER/DBT-P03222_DBT-S00931_rmlo_Cancer_s14_cx1698_cy640_244_244.pickle\n",
            "CANCER/DBT-P03222_DBT-S00931_rcc_Cancer_s26_cx1752_cy602_244_244.pickle\n",
            "CANCER/DBT-P03292_DBT-S03262_lmlo_Cancer_s38_cx812_cy1078_244_244.pickle\n",
            "CANCER/DBT-P03292_DBT-S03262_lcc_Cancer_s45_cx552_cy1060_244_244.pickle\n",
            "CANCER/DBT-P01112_DBT-S04216_lcc_Cancer_s35_cx468_cy1417_244_244.pickle\n",
            "CANCER/DBT-P01112_DBT-S04216_lmlo_Cancer_s33_cx619_cy1150_244_244.pickle\n",
            "CANCER/DBT-P01139_DBT-S01170_rmlo_Cancer_s54_cx1217_cy1472_244_244.pickle\n",
            "CANCER/DBT-P01139_DBT-S01170_rcc_Cancer_s45_cx1233_cy1521_244_244.pickle\n",
            "CANCER/DBT-P01267_DBT-S03415_rmlo_Cancer_s41_cx1446_cy796_244_244.pickle\n",
            "CANCER/DBT-P01267_DBT-S03415_rcc_Cancer_s46_cx1646_cy1123_244_244.pickle\n",
            "CANCER/DBT-P01347_DBT-S02864_rmlo1_Cancer_s32_cx1586_cy1249_244_244.pickle\n",
            "CANCER/DBT-P01347_DBT-S02864_rcc_Cancer_s27_cx1603_cy999_244_244.pickle\n",
            "CANCER/DBT-P01347_DBT-S02864_rmlo_Cancer_s25_cx1641_cy1040_244_244.pickle\n",
            "CANCER/DBT-P01493_DBT-S00432_rcc_Cancer_s7_cx1646_cy570_244_244.pickle\n",
            "CANCER/DBT-P01493_DBT-S00432_rmlo_Cancer_s5_cx1498_cy1643_244_244.pickle\n",
            "CANCER/DBT-P01539_DBT-S03623_rcc_Cancer_s28_cx1040_cy1810_244_244.pickle\n",
            "CANCER/DBT-P01539_DBT-S03623_rmlo_Cancer_s37_cx1126_cy1452_244_244.pickle\n",
            "CANCER/DBT-P01673_DBT-S04347_lcc_Cancer_s47_cx224_cy1043_244_244.pickle\n",
            "CANCER/DBT-P01673_DBT-S04347_lmlo_Cancer_s33_cx498_cy557_244_244.pickle\n",
            "CANCER/DBT-P01624_DBT-S03518_lmlo_Cancer_s34_cx360_cy1417_244_244.pickle\n",
            "CANCER/DBT-P01624_DBT-S03518_lcc_Cancer_s20_cx349_cy1054_244_244.pickle\n",
            "CANCER/DBT-P01712_DBT-S02941_rcc_Cancer_s37_cx1256_cy1233_244_244.pickle\n",
            "CANCER/DBT-P01712_DBT-S02941_rmlo_Cancer_s42_cx1109_cy1092_244_244.pickle\n",
            "CANCER/DBT-P01745_DBT-S03916_lcc_Cancer_s13_cx169_cy722_244_244.pickle\n",
            "CANCER/DBT-P01745_DBT-S03916_lmlo_Cancer_s14_cx181_cy1469_244_244.pickle\n",
            "CANCER/DBT-P01826_DBT-S00710_lcc_Cancer_s29_cx156_cy1004_244_244.pickle\n",
            "CANCER/DBT-P01801_DBT-S04999_rmlo_Cancer_s29_cx1304_cy1878_244_244.pickle\n",
            "CANCER/DBT-P01801_DBT-S04999_rcc1_Cancer_s15_cx1296_cy1551_244_244.pickle\n",
            "CANCER/DBT-P02100_DBT-S02965_rcc_Cancer_s16_cx1563_cy1080_244_244.pickle\n",
            "CANCER/DBT-P02133_DBT-S02082_rmlo1_Cancer_s71_cx1601_cy1355_244_244.pickle\n",
            "CANCER/DBT-P02133_DBT-S02082_rcc_Cancer_s46_cx1642_cy1686_244_244.pickle\n",
            "CANCER/DBT-P02510_DBT-S04417_lcc_Cancer_s34_cx11_cy1066_244_244.pickle\n",
            "CANCER/DBT-P02510_DBT-S04417_lmlo_Cancer_s28_cx351_cy966_244_244.pickle\n",
            "CANCER/DBT-P02532_DBT-S02099_lmlo_Cancer_s36_cx831_cy1229_244_244.pickle\n",
            "CANCER/DBT-P02532_DBT-S02099_lcc_Cancer_s43_cx834_cy1171_244_244.pickle\n",
            "CANCER/DBT-P02582_DBT-S01997_lmlo_Cancer_s32_cx160_cy759_244_244.pickle\n",
            "CANCER/DBT-P02582_DBT-S01997_lcc_Cancer_s40_cx8_cy884_244_244.pickle\n",
            "CANCER/DBT-P02738_DBT-S01258_lmlo_Cancer_s17_cx242_cy1923_244_244.pickle\n",
            "CANCER/DBT-P02738_DBT-S01258_lcc_Cancer_s6_cx121_cy1132_244_244.pickle\n",
            "CANCER/DBT-P02935_DBT-S00614_rmlo_Cancer_s35_cx1738_cy1025_244_244.pickle\n",
            "CANCER/DBT-P02935_DBT-S00614_rcc_Cancer_s28_cx1752_cy991_244_244.pickle\n",
            "CANCER/DBT-P00107_DBT-S05365_lcc_Cancer_s34_cx0_cy1148_244_244.pickle\n",
            "CANCER/DBT-P00107_DBT-S05365_lmlo_Cancer_s45_cx13_cy729_244_244.pickle\n",
            "CANCER/DBT-P00194_DBT-S00645_rmlo_Cancer_s35_cx1347_cy1730_244_244.pickle\n",
            "CANCER/DBT-P00303_DBT-S02436_rmlo_Cancer_s14_cx1536_cy543_244_244.pickle\n",
            "CANCER/DBT-P00303_DBT-S02436_rcc_Cancer_s21_cx1411_cy1080_244_244.pickle\n",
            "CANCER/DBT-P00583_DBT-S00852_lcc_Cancer_s33_cx176_cy1057_244_244.pickle\n",
            "CANCER/DBT-P00583_DBT-S00852_lmlo_Cancer_s31_cx321_cy1092_244_244.pickle\n",
            "CANCER/DBT-P00538_DBT-S01986_rmlo_Cancer_s37_cx1269_cy1389_244_244.pickle\n",
            "CANCER/DBT-P00538_DBT-S01986_lcc_Cancer_s36_cx69_cy1483_244_244.pickle\n",
            "CANCER/DBT-P00538_DBT-S01986_lmlo_Cancer_s42_cx198_cy1015_244_244.pickle\n",
            "CANCER/DBT-P00538_DBT-S01986_rcc_Cancer_s26_cx1512_cy1267_244_244.pickle\n",
            "CANCER/DBT-P00654_DBT-S02571_lcc_Cancer_s44_cx764_cy807_244_244.pickle\n",
            "CANCER/DBT-P02176_DBT-S03078_rmlo_Cancer_s63_cx1014_cy1668_244_244.pickle\n",
            "CANCER/DBT-P02176_DBT-S03078_rcc_Cancer_s41_cx939_cy1469_244_244.pickle\n",
            "CANCER/DBT-P01110_DBT-S04109_lmlo_Cancer_s19_cx503_cy1161_244_244.pickle\n",
            "CANCER/DBT-P01110_DBT-S04109_lcc_Cancer_s34_cx484_cy835_244_244.pickle\n",
            "-------------  BENIGN\n",
            "BENIGN/DBT-P03458_DBT-S03411_lcc_Benign_s27_cx288_cy982_244_244.pickle\n",
            "BENIGN/DBT-P03423_DBT-S05305_rmlo1_Benign_s10_cx1219_cy0_244_244.pickle\n",
            "BENIGN/DBT-P03423_DBT-S05305_rmlo_Benign_s6_cx1412_cy204_244_244.pickle\n",
            "BENIGN/DBT-P03539_DBT-S03755_lmlo_Benign_s46_cx527_cy966_244_244.pickle\n",
            "BENIGN/DBT-P03539_DBT-S03755_lcc_Benign_s44_cx371_cy1218_244_244.pickle\n",
            "BENIGN/DBT-P03677_DBT-S00709_rcc_Benign_s25_cx1419_cy1559_244_244.pickle\n",
            "BENIGN/DBT-P03677_DBT-S00709_lcc_Benign_s42_cx258_cy995_244_244.pickle\n",
            "BENIGN/DBT-P03677_DBT-S00709_lmlo_Benign_s33_cx393_cy1139_244_244.pickle\n",
            "BENIGN/DBT-P03677_DBT-S00709_rmlo_Benign_s32_cx1381_cy1658_244_244.pickle\n",
            "BENIGN/DBT-P03658_DBT-S05241_lcc_Benign_s15_cx680_cy1341_244_244.pickle\n",
            "BENIGN/DBT-P03658_DBT-S05241_rmlo_Benign_s10_cx1265_cy1691_244_244.pickle\n",
            "BENIGN/DBT-P03658_DBT-S05241_rcc_Benign_s5_cx1185_cy1245_244_244.pickle\n",
            "BENIGN/DBT-P03748_DBT-S02094_lmlo_Benign_s42_cx30_cy948_244_244.pickle\n",
            "BENIGN/DBT-P03748_DBT-S02094_lcc_Benign_s45_cx118_cy1656_244_244.pickle\n",
            "BENIGN/DBT-P03816_DBT-S03888_rcc_Benign_s45_cx1752_cy1335_244_244.pickle\n",
            "BENIGN/DBT-P03816_DBT-S03888_rmlo_Benign_s58_cx1353_cy894_244_244.pickle\n",
            "BENIGN/DBT-P03978_DBT-S00442_rmlo_Benign_s10_cx1324_cy819_244_244.pickle\n",
            "BENIGN/DBT-P03978_DBT-S00442_rcc_Benign_s24_cx1592_cy667_244_244.pickle\n",
            "BENIGN/DBT-P04116_DBT-S03961_rmlo_Benign_s18_cx1271_cy1331_244_244.pickle\n",
            "BENIGN/DBT-P04116_DBT-S03961_rcc_Benign_s30_cx1523_cy560_244_244.pickle\n",
            "BENIGN/DBT-P04326_DBT-S03750_lcc_Benign_s12_cx8_cy1093_244_244.pickle\n",
            "BENIGN/DBT-P04326_DBT-S03750_lmlo_Benign_s29_cx2_cy1593_244_244.pickle\n",
            "BENIGN/DBT-P04429_DBT-S00568_lmlo_Benign_s34_cx672_cy707_244_244.pickle\n",
            "BENIGN/DBT-P04429_DBT-S00568_lcc_Benign_s57_cx215_cy882_244_244.pickle\n",
            "BENIGN/DBT-P04721_DBT-S01833_lmlo_Benign_s24_cx475_cy1430_244_244.pickle\n",
            "BENIGN/DBT-P04721_DBT-S01833_lcc_Benign_s27_cx218_cy1201_244_244.pickle\n",
            "BENIGN/DBT-P04750_DBT-S00052_rcc_Benign_s15_cx1322_cy730_244_244.pickle\n",
            "BENIGN/DBT-P04858_DBT-S04555_lmlo_Benign_s18_cx448_cy1244_244_244.pickle\n",
            "BENIGN/DBT-P04858_DBT-S04555_rcc_Benign_s31_cx1258_cy1992_244_244.pickle\n",
            "BENIGN/DBT-P04818_DBT-S02975_rcc_Benign_s27_cx1515_cy930_244_244.pickle\n",
            "BENIGN/DBT-P04818_DBT-S02975_rmlo_Benign_s26_cx1510_cy1211_244_244.pickle\n",
            "BENIGN/DBT-P05047_DBT-S05588_rmlo_Benign_s41_cx1192_cy1019_244_244.pickle\n",
            "BENIGN/DBT-P05047_DBT-S05588_rcc_Benign_s41_cx1341_cy1073_244_244.pickle\n",
            "BENIGN/DBT-P05022_DBT-S05195_rmlo_Benign_s35_cx1634_cy1271_244_244.pickle\n",
            "BENIGN/DBT-P05022_DBT-S05195_rcc_Benign_s30_cx1655_cy1460_244_244.pickle\n",
            "BENIGN/DBT-P03009_DBT-S01465_rmlo_Benign_s16_cx984_cy1107_244_244.pickle\n",
            "BENIGN/DBT-P03009_DBT-S01465_rcc_Benign_s27_cx1067_cy799_244_244.pickle\n",
            "BENIGN/DBT-P03085_DBT-S00863_lmlo_Benign_s35_cx167_cy1156_244_244.pickle\n",
            "BENIGN/DBT-P03085_DBT-S00863_lcc_Benign_s26_cx165_cy1359_244_244.pickle\n",
            "BENIGN/DBT-P03073_DBT-S04591_lcc_Benign_s35_cx271_cy1057_244_244.pickle\n",
            "BENIGN/DBT-P03073_DBT-S04591_lmlo_Benign_s19_cx236_cy612_244_244.pickle\n",
            "BENIGN/DBT-P03218_DBT-S04050_lmlo_Benign_s35_cx365_cy193_244_244.pickle\n",
            "BENIGN/DBT-P03218_DBT-S04050_lcc_Benign_s51_cx339_cy1642_244_244.pickle\n",
            "BENIGN/DBT-P03176_DBT-S03730_lmlo_Benign_s12_cx76_cy1665_244_244.pickle\n",
            "BENIGN/DBT-P03176_DBT-S03730_lcc_Benign_s11_cx43_cy1216_244_244.pickle\n",
            "BENIGN/DBT-P03203_DBT-S04862_lcc_Benign_s22_cx753_cy1238_244_244.pickle\n",
            "BENIGN/DBT-P03203_DBT-S04862_lmlo_Benign_s38_cx773_cy1558_244_244.pickle\n",
            "BENIGN/DBT-P03212_DBT-S02198_lcc_Benign_s25_cx5_cy302_244_244.pickle\n",
            "BENIGN/DBT-P03212_DBT-S02198_lmlo_Benign_s11_cx210_cy695_244_244.pickle\n",
            "BENIGN/DBT-P01130_DBT-S01935_lmlo_Benign_s10_cx121_cy1385_244_244.pickle\n",
            "BENIGN/DBT-P01130_DBT-S01935_lcc_Benign_s12_cx79_cy922_244_244.pickle\n",
            "BENIGN/DBT-P01181_DBT-S04901_lmlo_Benign_s46_cx477_cy1437_244_244.pickle\n",
            "BENIGN/DBT-P01181_DBT-S04901_lcc_Benign_s36_cx459_cy1692_244_244.pickle\n",
            "BENIGN/DBT-P01282_DBT-S01508_lmlo1_Benign_s29_cx8_cy601_244_244.pickle\n",
            "BENIGN/DBT-P01282_DBT-S01508_lcc2_Benign_s41_cx4_cy1075_244_244.pickle\n",
            "BENIGN/DBT-P01262_DBT-S00537_rcc_Benign_s39_cx1398_cy647_244_244.pickle\n",
            "BENIGN/DBT-P01262_DBT-S00537_rmlo_Benign_s27_cx1244_cy858_244_244.pickle\n",
            "BENIGN/DBT-P01241_DBT-S03180_lcc_Benign_s2_cx838_cy1091_244_244.pickle\n",
            "BENIGN/DBT-P01241_DBT-S03180_lmlo_Benign_s9_cx631_cy1788_244_244.pickle\n",
            "BENIGN/DBT-P01439_DBT-S01096_lmlo_Benign_s37_cx164_cy815_244_244.pickle\n",
            "BENIGN/DBT-P01461_DBT-S00251_lcc_Benign_s23_cx170_cy776_244_244.pickle\n",
            "BENIGN/DBT-P01461_DBT-S00251_lmlo_Benign_s26_cx248_cy1152_244_244.pickle\n",
            "BENIGN/DBT-P01488_DBT-S00017_lcc_Benign_s23_cx773_cy1158_244_244.pickle\n",
            "BENIGN/DBT-P01488_DBT-S00017_lmlo_Benign_s23_cx925_cy1057_244_244.pickle\n",
            "BENIGN/DBT-P01587_DBT-S04115_rcc_Benign_s34_cx1342_cy839_244_244.pickle\n",
            "BENIGN/DBT-P01587_DBT-S04115_rmlo_Benign_s17_cx1184_cy1048_244_244.pickle\n",
            "BENIGN/DBT-P01626_DBT-S02188_lmlo_Benign_s33_cx824_cy1092_244_244.pickle\n",
            "BENIGN/DBT-P01626_DBT-S02188_lcc_Benign_s37_cx541_cy1176_244_244.pickle\n",
            "BENIGN/DBT-P01718_DBT-S01120_rcc_Benign_s42_cx1752_cy685_244_244.pickle\n",
            "BENIGN/DBT-P01718_DBT-S01120_rmlo_Benign_s29_cx1201_cy404_244_244.pickle\n",
            "BENIGN/DBT-P01753_DBT-S04069_lmlo_Benign_s22_cx849_cy1263_244_244.pickle\n",
            "BENIGN/DBT-P01753_DBT-S04069_lcc_Benign_s32_cx1026_cy808_244_244.pickle\n",
            "BENIGN/DBT-P01839_DBT-S03748_lcc_Benign_s18_cx1395_cy779_244_244.pickle\n",
            "BENIGN/DBT-P01839_DBT-S03748_lmlo_Benign_s12_cx1317_cy1645_244_244.pickle\n",
            "BENIGN/DBT-P01817_DBT-S01841_lcc_Benign_s20_cx516_cy1777_244_244.pickle\n",
            "BENIGN/DBT-P01817_DBT-S01841_lmlo_Benign_s38_cx517_cy1869_244_244.pickle\n",
            "BENIGN/DBT-P02065_DBT-S04959_lcc_Benign_s35_cx32_cy865_244_244.pickle\n",
            "BENIGN/DBT-P02065_DBT-S04959_lmlo_Benign_s17_cx421_cy1088_244_244.pickle\n",
            "BENIGN/DBT-P02171_DBT-S04537_rcc_Benign_s12_cx1752_cy711_244_244.pickle\n",
            "BENIGN/DBT-P02171_DBT-S04537_rmlo_Benign_s14_cx1752_cy1247_244_244.pickle\n",
            "BENIGN/DBT-P02227_DBT-S05014_rcc_Benign_s32_cx1159_cy636_244_244.pickle\n",
            "BENIGN/DBT-P02227_DBT-S05014_rmlo_Benign_s30_cx873_cy722_244_244.pickle\n",
            "BENIGN/DBT-P02380_DBT-S01233_lcc_Benign_s15_cx165_cy1499_244_244.pickle\n",
            "BENIGN/DBT-P02380_DBT-S01233_lmlo_Benign_s29_cx61_cy1795_244_244.pickle\n",
            "BENIGN/DBT-P02471_DBT-S03894_lcc_Benign_s33_cx282_cy1513_244_244.pickle\n",
            "BENIGN/DBT-P02471_DBT-S03894_lmlo_Benign_s39_cx219_cy1499_244_244.pickle\n",
            "BENIGN/DBT-P02493_DBT-S03027_lcc_Benign_s49_cx124_cy831_244_244.pickle\n",
            "BENIGN/DBT-P02493_DBT-S03027_lmlo_Benign_s27_cx87_cy1_244_244.pickle\n",
            "BENIGN/DBT-P02588_DBT-S04431_lcc_Benign_s24_cx464_cy1595_244_244.pickle\n",
            "BENIGN/DBT-P02588_DBT-S04431_lmlo_Benign_s41_cx415_cy1375_244_244.pickle\n",
            "BENIGN/DBT-P02798_DBT-S01770_lmlo_Benign_s57_cx1187_cy1751_244_244.pickle\n",
            "BENIGN/DBT-P02798_DBT-S01770_lmlo1_Benign_s58_cx859_cy1109_244_244.pickle\n",
            "BENIGN/DBT-P02798_DBT-S01770_lcc_Benign_s50_cx1018_cy1322_244_244.pickle\n",
            "BENIGN/DBT-P02750_DBT-S00905_rmlo1_Benign_s32_cx1646_cy2137_244_244.pickle\n",
            "BENIGN/DBT-P02750_DBT-S00905_rmlo_Benign_s33_cx1646_cy1963_244_244.pickle\n",
            "BENIGN/DBT-P02736_DBT-S03542_lcc_Benign_s15_cx552_cy932_244_244.pickle\n",
            "BENIGN/DBT-P02736_DBT-S03542_lmlo_Benign_s16_cx568_cy1464_244_244.pickle\n",
            "BENIGN/DBT-P02843_DBT-S01347_lcc_Benign_s22_cx958_cy1559_244_244.pickle\n",
            "BENIGN/DBT-P02843_DBT-S01347_lmlo_Benign_s34_cx1085_cy1660_244_244.pickle\n",
            "BENIGN/DBT-P02919_DBT-S01259_rcc_Benign_s48_cx667_cy1133_244_244.pickle\n",
            "BENIGN/DBT-P02919_DBT-S01259_lcc_Benign_s29_cx288_cy761_244_244.pickle\n",
            "BENIGN/DBT-P02919_DBT-S01259_rmlo_Benign_s26_cx605_cy1559_244_244.pickle\n",
            "BENIGN/DBT-P02919_DBT-S01259_lmlo_Benign_s33_cx474_cy1070_244_244.pickle\n",
            "BENIGN/DBT-P03017_DBT-S04280_rmlo_Benign_s31_cx1128_cy979_244_244.pickle\n",
            "BENIGN/DBT-P03017_DBT-S04280_rcc_Benign_s17_cx1296_cy1052_244_244.pickle\n",
            "BENIGN/DBT-P00013_DBT-S00163_rmlo_Benign_s16_cx1116_cy1724_244_244.pickle\n",
            "BENIGN/DBT-P00060_DBT-S00787_rcc_Benign_s21_cx1276_cy672_244_244.pickle\n",
            "BENIGN/DBT-P00024_DBT-S03255_lmlo_Benign_s11_cx471_cy1060_244_244.pickle\n",
            "BENIGN/DBT-P00024_DBT-S03255_lcc_Benign_s19_cx267_cy488_244_244.pickle\n",
            "BENIGN/DBT-P00225_DBT-S02346_lcc_Benign_s54_cx266_cy1180_244_244.pickle\n",
            "BENIGN/DBT-P00225_DBT-S02346_lmlo_Benign_s52_cx423_cy758_244_244.pickle\n",
            "BENIGN/DBT-P00361_DBT-S00216_rmlo_Benign_s25_cx1393_cy1011_244_244.pickle\n",
            "BENIGN/DBT-P00361_DBT-S00216_rcc_Benign_s17_cx1396_cy1041_244_244.pickle\n",
            "BENIGN/DBT-P01751_DBT-S01332_rmlo_Benign_s8_cx1462_cy5_244_244.pickle\n",
            "BENIGN/DBT-P02579_DBT-S00929_lmlo_Benign_s14_cx395_cy1268_244_244.pickle\n",
            "BENIGN/DBT-P02579_DBT-S00929_lcc_Benign_s18_cx342_cy975_244_244.pickle\n",
            "BENIGN/DBT-P00684_DBT-S02691_lmlo_Benign_s6_cx318_cy1715_244_244.pickle\n",
            "BENIGN/DBT-P00684_DBT-S02691_lcc_Benign_s15_cx347_cy1416_244_244.pickle\n",
            "BENIGN/DBT-P00818_DBT-S02315_rcc_Benign_s44_cx600_cy1242_244_244.pickle\n",
            "BENIGN/DBT-P00818_DBT-S02315_rmlo_Benign_s42_cx571_cy1335_244_244.pickle\n",
            "BENIGN/DBT-P00827_DBT-S01731_rmlo_Benign_s36_cx1550_cy1690_244_244.pickle\n",
            "BENIGN/DBT-P00827_DBT-S01731_rcc_Benign_s33_cx1527_cy1725_244_244.pickle\n",
            "BENIGN/DBT-P00784_DBT-S05205_rcc_Benign_s19_cx1492_cy1379_244_244.pickle\n",
            "BENIGN/DBT-P00784_DBT-S05205_rmlo_Benign_s27_cx1415_cy1418_244_244.pickle\n",
            "-------------  NORMAL\n",
            "NORMAL/DBT-P03261_DBT-S05042_rmlo_Normal_s43_cx1582_cy1161_244_244.pickle\n",
            "NORMAL/DBT-P03261_DBT-S05042_lcc_Normal_s85_cx591_cy1367_244_244.pickle\n",
            "NORMAL/DBT-P03261_DBT-S05042_lmlo_Normal_s6_cx655_cy1197_244_244.pickle\n",
            "NORMAL/DBT-P03281_DBT-S05018_lcc_Normal_s40_cx613_cy1128_244_244.pickle\n",
            "NORMAL/DBT-P03281_DBT-S05018_rmlo_Normal_s58_cx1448_cy1189_244_244.pickle\n",
            "NORMAL/DBT-P01007_DBT-S03475_lmlo_Normal_s15_cx675_cy1146_244_244.pickle\n",
            "NORMAL/DBT-P01007_DBT-S03475_lcc_Normal_s34_cx597_cy1290_244_244.pickle\n",
            "NORMAL/DBT-P01007_DBT-S03475_rcc_Normal_s30_cx1451_cy1260_244_244.pickle\n",
            "NORMAL/DBT-P01032_DBT-S03686_lcc_Normal_s4_cx438_cy1427_244_244.pickle\n",
            "NORMAL/DBT-P01032_DBT-S03686_rcc_Normal_s40_cx1471_cy1344_244_244.pickle\n",
            "NORMAL/DBT-P01032_DBT-S03686_rmlo_Normal_s49_cx1448_cy1321_244_244.pickle\n",
            "NORMAL/DBT-P01032_DBT-S03686_lmlo_Normal_s33_cx468_cy1222_244_244.pickle\n",
            "NORMAL/DBT-P01035_DBT-S03678_rmlo_Normal_s29_cx1361_cy1275_244_244.pickle\n",
            "NORMAL/DBT-P01035_DBT-S03678_lmlo_Normal_s19_cx536_cy1067_244_244.pickle\n",
            "NORMAL/DBT-P01035_DBT-S03678_lcc_Normal_s59_cx479_cy1289_244_244.pickle\n",
            "NORMAL/DBT-P01035_DBT-S03678_rcc_Normal_s15_cx1370_cy1239_244_244.pickle\n",
            "NORMAL/DBT-P01017_DBT-S03945_lmlo_Normal_s31_cx414_cy1055_244_244.pickle\n",
            "NORMAL/DBT-P01017_DBT-S03945_lcc_Normal_s22_cx303_cy1197_244_244.pickle\n",
            "NORMAL/DBT-P01017_DBT-S03945_rmlo_Normal_s45_cx1508_cy1055_244_244.pickle\n",
            "NORMAL/DBT-P01017_DBT-S03945_rcc_Normal_s17_cx1574_cy1221_244_244.pickle\n",
            "NORMAL/DBT-P01108_DBT-S01517_rmlo_Normal_s42_cx1616_cy981_244_244.pickle\n",
            "NORMAL/DBT-P01108_DBT-S01517_rcc_Normal_s7_cx1694_cy1314_244_244.pickle\n",
            "NORMAL/DBT-P01108_DBT-S01517_lmlo_Normal_s53_cx369_cy993_244_244.pickle\n",
            "NORMAL/DBT-P01108_DBT-S01517_lcc_Normal_s57_cx306_cy1411_244_244.pickle\n",
            "NORMAL/DBT-P01156_DBT-S01722_rmlo_Normal_s2_cx1546_cy1270_244_244.pickle\n",
            "NORMAL/DBT-P01156_DBT-S01722_rcc_Normal_s7_cx1442_cy1202_244_244.pickle\n",
            "NORMAL/DBT-P01156_DBT-S01722_lcc_Normal_s22_cx321_cy1205_244_244.pickle\n",
            "NORMAL/DBT-P01156_DBT-S01722_lmlo_Normal_s23_cx345_cy1314_244_244.pickle\n",
            "NORMAL/DBT-P01095_DBT-S00834_rcc_Normal_s2_cx1316_cy1319_244_244.pickle\n",
            "NORMAL/DBT-P01095_DBT-S00834_lmlo_Normal_s53_cx664_cy1129_244_244.pickle\n",
            "NORMAL/DBT-P01095_DBT-S00834_rmlo_Normal_s49_cx1333_cy1106_244_244.pickle\n",
            "NORMAL/DBT-P01095_DBT-S00834_lcc_Normal_s41_cx687_cy1337_244_244.pickle\n",
            "NORMAL/DBT-P01078_DBT-S00542_lmlo_Normal_s36_cx812_cy1373_244_244.pickle\n",
            "NORMAL/DBT-P01078_DBT-S00542_lcc_Normal_s85_cx743_cy1208_244_244.pickle\n",
            "NORMAL/DBT-P01078_DBT-S00542_rcc_Normal_s33_cx1293_cy1228_244_244.pickle\n",
            "NORMAL/DBT-P01078_DBT-S00542_rmlo_Normal_s100_cx1167_cy1378_244_244.pickle\n",
            "NORMAL/DBT-P01098_DBT-S05492_rmlo_Normal_s33_cx1589_cy1034_244_244.pickle\n",
            "NORMAL/DBT-P01098_DBT-S05492_lcc_Normal_s15_cx280_cy1206_244_244.pickle\n",
            "NORMAL/DBT-P01098_DBT-S05492_rcc_Normal_s37_cx1627_cy1285_244_244.pickle\n",
            "NORMAL/DBT-P01098_DBT-S05492_lmlo_Normal_s33_cx338_cy975_244_244.pickle\n",
            "NORMAL/DBT-P01217_DBT-S04154_lmlo_Normal_s6_cx259_cy894_244_244.pickle\n",
            "NORMAL/DBT-P01217_DBT-S04154_rcc_Normal_s32_cx1701_cy1306_244_244.pickle\n",
            "NORMAL/DBT-P01217_DBT-S04154_rmlo_Normal_s36_cx1649_cy908_244_244.pickle\n",
            "NORMAL/DBT-P01217_DBT-S04154_lcc_Normal_s62_cx237_cy1281_244_244.pickle\n",
            "NORMAL/DBT-P01192_DBT-S01596_lcc_Normal_s37_cx384_cy1109_244_244.pickle\n",
            "NORMAL/DBT-P01192_DBT-S01596_rmlo_Normal_s6_cx1571_cy1033_244_244.pickle\n",
            "NORMAL/DBT-P01192_DBT-S01596_lmlo_Normal_s45_cx410_cy1001_244_244.pickle\n",
            "NORMAL/DBT-P01192_DBT-S01596_rcc_Normal_s21_cx1594_cy1215_244_244.pickle\n",
            "NORMAL/DBT-P01221_DBT-S03063_rcc_Normal_s30_cx1504_cy1361_244_244.pickle\n",
            "NORMAL/DBT-P01221_DBT-S03063_rmlo_Normal_s6_cx1483_cy1037_244_244.pickle\n",
            "NORMAL/DBT-P01221_DBT-S03063_lmlo_Normal_s2_cx447_cy1150_244_244.pickle\n",
            "NORMAL/DBT-P01221_DBT-S03063_lcc_Normal_s15_cx443_cy1337_244_244.pickle\n",
            "NORMAL/DBT-P01203_DBT-S03506_lmlo_Normal_s39_cx460_cy1176_244_244.pickle\n",
            "NORMAL/DBT-P01203_DBT-S03506_lcc_Normal_s4_cx480_cy1357_244_244.pickle\n",
            "NORMAL/DBT-P01203_DBT-S03506_rmlo_Normal_s17_cx1516_cy1068_244_244.pickle\n",
            "NORMAL/DBT-P01203_DBT-S03506_rcc_Normal_s18_cx1507_cy1296_244_244.pickle\n",
            "NORMAL/DBT-P01197_DBT-S00302_rcc_Normal_s46_cx1541_cy1329_244_244.pickle\n",
            "NORMAL/DBT-P01197_DBT-S00302_rmlo_Normal_s46_cx1506_cy953_244_244.pickle\n",
            "NORMAL/DBT-P01197_DBT-S00302_lcc_Normal_s4_cx351_cy1318_244_244.pickle\n",
            "NORMAL/DBT-P01197_DBT-S00302_lmlo_Normal_s15_cx378_cy1047_244_244.pickle\n",
            "NORMAL/DBT-P01223_DBT-S00807_lmlo_Normal_s14_cx429_cy903_244_244.pickle\n",
            "NORMAL/DBT-P01223_DBT-S00807_lcc_Normal_s48_cx417_cy1379_244_244.pickle\n",
            "NORMAL/DBT-P01223_DBT-S00807_rmlo_Normal_s16_cx1549_cy1009_244_244.pickle\n",
            "NORMAL/DBT-P01223_DBT-S00807_rcc_Normal_s10_cx1595_cy1351_244_244.pickle\n",
            "NORMAL/DBT-P01259_DBT-S03361_rcc_Normal_s55_cx1534_cy1279_244_244.pickle\n",
            "NORMAL/DBT-P01259_DBT-S03361_lmlo_Normal_s29_cx453_cy1025_244_244.pickle\n",
            "NORMAL/DBT-P01259_DBT-S03361_rmlo_Normal_s42_cx1518_cy1001_244_244.pickle\n",
            "NORMAL/DBT-P01259_DBT-S03361_lcc_Normal_s30_cx430_cy1392_244_244.pickle\n",
            "NORMAL/DBT-P01271_DBT-S02840_rmlo_Normal_s5_cx1463_cy978_244_244.pickle\n",
            "NORMAL/DBT-P01271_DBT-S02840_rcc_Normal_s15_cx1539_cy1195_244_244.pickle\n",
            "NORMAL/DBT-P01271_DBT-S02840_lcc_Normal_s5_cx319_cy1242_244_244.pickle\n",
            "NORMAL/DBT-P01271_DBT-S02840_lmlo_Normal_s9_cx395_cy961_244_244.pickle\n",
            "NORMAL/DBT-P01253_DBT-S01082_lcc_Normal_s7_cx285_cy1109_244_244.pickle\n",
            "NORMAL/DBT-P01253_DBT-S01082_rcc_Normal_s53_cx1648_cy969_244_244.pickle\n",
            "NORMAL/DBT-P01253_DBT-S01082_rmlo_Normal_s29_cx1545_cy1009_244_244.pickle\n",
            "NORMAL/DBT-P01253_DBT-S01082_lmlo_Normal_s45_cx400_cy1094_244_244.pickle\n",
            "NORMAL/DBT-P01231_DBT-S01461_rmlo_Normal_s14_cx1542_cy991_244_244.pickle\n",
            "NORMAL/DBT-P01231_DBT-S01461_lcc_Normal_s21_cx447_cy1176_244_244.pickle\n",
            "NORMAL/DBT-P01231_DBT-S01461_lmlo_Normal_s39_cx489_cy1073_244_244.pickle\n",
            "NORMAL/DBT-P01231_DBT-S01461_rcc_Normal_s34_cx1560_cy1327_244_244.pickle\n",
            "NORMAL/DBT-P01383_DBT-S03711_rcc_Normal_s32_cx1587_cy1283_244_244.pickle\n",
            "NORMAL/DBT-P01383_DBT-S03711_lmlo_Normal_s27_cx439_cy1032_244_244.pickle\n",
            "NORMAL/DBT-P01383_DBT-S03711_rmlo_Normal_s4_cx1577_cy955_244_244.pickle\n",
            "NORMAL/DBT-P01383_DBT-S03711_lcc_Normal_s3_cx398_cy1152_244_244.pickle\n",
            "NORMAL/DBT-P01429_DBT-S02992_rmlo_Normal_s2_cx1579_cy1016_244_244.pickle\n",
            "NORMAL/DBT-P01429_DBT-S02992_lcc_Normal_s40_cx390_cy1174_244_244.pickle\n",
            "NORMAL/DBT-P01429_DBT-S02992_lmlo_Normal_s10_cx432_cy980_244_244.pickle\n",
            "NORMAL/DBT-P01429_DBT-S02992_rcc_Normal_s56_cx1634_cy1285_244_244.pickle\n",
            "NORMAL/DBT-P01325_DBT-S02133_lcc_Normal_s80_cx523_cy1379_244_244.pickle\n",
            "NORMAL/DBT-P01325_DBT-S02133_rmlo_Normal_s76_cx1419_cy1152_244_244.pickle\n",
            "NORMAL/DBT-P01325_DBT-S02133_lmlo_Normal_s45_cx601_cy1133_244_244.pickle\n",
            "NORMAL/DBT-P01325_DBT-S02133_rcc_Normal_s75_cx1508_cy1362_244_244.pickle\n",
            "NORMAL/DBT-P01333_DBT-S00835_rcc_Normal_s9_cx1647_cy1338_244_244.pickle\n",
            "NORMAL/DBT-P01333_DBT-S00835_lcc_Normal_s19_cx360_cy1192_244_244.pickle\n",
            "NORMAL/DBT-P01333_DBT-S00835_lmlo_Normal_s53_cx319_cy1139_244_244.pickle\n",
            "NORMAL/DBT-P01333_DBT-S00835_rmlo_Normal_s54_cx1608_cy1117_244_244.pickle\n",
            "NORMAL/DBT-P01503_DBT-S03127_rcc_Normal_s50_cx1457_cy1097_244_244.pickle\n",
            "NORMAL/DBT-P01503_DBT-S03127_lcc_Normal_s56_cx461_cy1364_244_244.pickle\n",
            "NORMAL/DBT-P01503_DBT-S03127_rmlo_Normal_s47_cx1414_cy1239_244_244.pickle\n",
            "NORMAL/DBT-P01503_DBT-S03127_lmlo_Normal_s24_cx464_cy1194_244_244.pickle\n",
            "NORMAL/DBT-P01475_DBT-S04974_rcc_Normal_s43_cx1682_cy1169_244_244.pickle\n",
            "NORMAL/DBT-P01475_DBT-S04974_rmlo_Normal_s33_cx1592_cy943_244_244.pickle\n",
            "NORMAL/DBT-P01475_DBT-S04974_lcc_Normal_s48_cx233_cy1048_244_244.pickle\n",
            "NORMAL/DBT-P01475_DBT-S04974_lmlo_Normal_s23_cx353_cy956_244_244.pickle\n",
            "NORMAL/DBT-P01472_DBT-S04351_lcc_Normal_s60_cx345_cy1222_244_244.pickle\n",
            "NORMAL/DBT-P01472_DBT-S04351_lmlo_Normal_s30_cx373_cy1022_244_244.pickle\n",
            "NORMAL/DBT-P01472_DBT-S04351_rmlo_Normal_s47_cx1633_cy1055_244_244.pickle\n",
            "NORMAL/DBT-P01472_DBT-S04351_rcc_Normal_s30_cx1683_cy1189_244_244.pickle\n",
            "NORMAL/DBT-P01508_DBT-S00772_lcc_Normal_s28_cx347_cy1270_244_244.pickle\n",
            "NORMAL/DBT-P01508_DBT-S00772_rcc_Normal_s37_cx1569_cy1271_244_244.pickle\n",
            "NORMAL/DBT-P01508_DBT-S00772_lmlo_Normal_s38_cx409_cy1102_244_244.pickle\n",
            "NORMAL/DBT-P01508_DBT-S00772_rmlo_Normal_s61_cx1513_cy1117_244_244.pickle\n",
            "NORMAL/DBT-P01543_DBT-S05452_lmlo_Normal_s29_cx417_cy1085_244_244.pickle\n",
            "NORMAL/DBT-P01543_DBT-S05452_rmlo_Normal_s33_cx1511_cy1156_244_244.pickle\n",
            "NORMAL/DBT-P01543_DBT-S05452_rcc_Normal_s28_cx1526_cy1400_244_244.pickle\n",
            "NORMAL/DBT-P01543_DBT-S05452_lcc_Normal_s2_cx374_cy1376_244_244.pickle\n",
            "NORMAL/DBT-P01579_DBT-S05150_rcc_Normal_s26_cx1592_cy1157_244_244.pickle\n",
            "NORMAL/DBT-P01579_DBT-S05150_rmlo_Normal_s49_cx1554_cy1029_244_244.pickle\n",
            "NORMAL/DBT-P01579_DBT-S05150_lcc_Normal_s24_cx434_cy1152_244_244.pickle\n",
            "NORMAL/DBT-P01579_DBT-S05150_lmlo_Normal_s32_cx442_cy902_244_244.pickle\n",
            "NORMAL/DBT-P01514_DBT-S05325_lcc_Normal_s44_cx441_cy1226_244_244.pickle\n",
            "NORMAL/DBT-P01514_DBT-S05325_rmlo_Normal_s51_cx1372_cy1124_244_244.pickle\n",
            "NORMAL/DBT-P01514_DBT-S05325_lmlo_Normal_s25_cx498_cy1210_244_244.pickle\n",
            "NORMAL/DBT-P01514_DBT-S05325_rcc_Normal_s54_cx1421_cy1226_244_244.pickle\n",
            "NORMAL/DBT-P01583_DBT-S02065_lcc_Normal_s47_cx498_cy1206_244_244.pickle\n",
            "NORMAL/DBT-P01583_DBT-S02065_rcc_Normal_s64_cx1498_cy1253_244_244.pickle\n",
            "NORMAL/DBT-P01583_DBT-S02065_rmlo_Normal_s20_cx1444_cy1167_244_244.pickle\n",
            "NORMAL/DBT-P01583_DBT-S02065_lmlo_Normal_s21_cx545_cy1137_244_244.pickle\n",
            "NORMAL/DBT-P01541_DBT-S04507_lmlo_Normal_s11_cx445_cy984_244_244.pickle\n",
            "NORMAL/DBT-P01541_DBT-S04507_lcc_Normal_s8_cx406_cy1229_244_244.pickle\n",
            "NORMAL/DBT-P01541_DBT-S04507_rmlo_Normal_s25_cx1541_cy1095_244_244.pickle\n",
            "NORMAL/DBT-P01541_DBT-S04507_rcc_Normal_s57_cx1589_cy1293_244_244.pickle\n",
            "NORMAL/DBT-P01581_DBT-S02789_lmlo_Normal_s8_cx537_cy1070_244_244.pickle\n",
            "NORMAL/DBT-P01581_DBT-S02789_rmlo_Normal_s66_cx1447_cy988_244_244.pickle\n",
            "NORMAL/DBT-P01581_DBT-S02789_rcc_Normal_s30_cx1488_cy1317_244_244.pickle\n",
            "NORMAL/DBT-P01581_DBT-S02789_lcc_Normal_s8_cx497_cy1325_244_244.pickle\n",
            "NORMAL/DBT-P01603_DBT-S05312_rmlo_Normal_s48_cx1319_cy1306_244_244.pickle\n",
            "NORMAL/DBT-P01603_DBT-S05312_lmlo_Normal_s8_cx534_cy1280_244_244.pickle\n",
            "NORMAL/DBT-P01603_DBT-S05312_rcc_Normal_s27_cx1382_cy1227_244_244.pickle\n",
            "NORMAL/DBT-P01603_DBT-S05312_lcc_Normal_s1_cx514_cy1311_244_244.pickle\n",
            "NORMAL/DBT-P01672_DBT-S03310_rcc_Normal_s31_cx1475_cy1257_244_244.pickle\n",
            "NORMAL/DBT-P01672_DBT-S03310_rmlo_Normal_s59_cx1445_cy1157_244_244.pickle\n",
            "NORMAL/DBT-P01672_DBT-S03310_lmlo_Normal_s22_cx630_cy1139_244_244.pickle\n",
            "NORMAL/DBT-P01672_DBT-S03310_lcc_Normal_s10_cx571_cy1412_244_244.pickle\n",
            "NORMAL/DBT-P01660_DBT-S04622_lmlo_Normal_s3_cx328_cy1005_244_244.pickle\n",
            "NORMAL/DBT-P01660_DBT-S04622_rmlo_Normal_s3_cx1572_cy1025_244_244.pickle\n",
            "NORMAL/DBT-P01660_DBT-S04622_lcc_Normal_s21_cx271_cy1208_244_244.pickle\n",
            "NORMAL/DBT-P01660_DBT-S04622_rcc_Normal_s12_cx1632_cy1246_244_244.pickle\n",
            "NORMAL/DBT-P01639_DBT-S01287_rmlo_Normal_s5_cx1422_cy1175_244_244.pickle\n",
            "NORMAL/DBT-P01639_DBT-S01287_rcc_Normal_s12_cx1428_cy1288_244_244.pickle\n",
            "NORMAL/DBT-P01639_DBT-S01287_lmlo_Normal_s34_cx456_cy1132_244_244.pickle\n",
            "NORMAL/DBT-P01639_DBT-S01287_lcc_Normal_s37_cx388_cy1182_244_244.pickle\n",
            "NORMAL/DBT-P01682_DBT-S00474_lcc_Normal_s2_cx858_cy1193_244_244.pickle\n",
            "NORMAL/DBT-P01682_DBT-S00474_rcc_Normal_s43_cx1133_cy1229_244_244.pickle\n",
            "NORMAL/DBT-P01682_DBT-S00474_lmlo_Normal_s8_cx827_cy1273_244_244.pickle\n",
            "NORMAL/DBT-P01682_DBT-S00474_rmlo_Normal_s20_cx1090_cy1242_244_244.pickle\n",
            "NORMAL/DBT-P01687_DBT-S02168_rcc_Normal_s15_cx1522_cy1197_244_244.pickle\n",
            "NORMAL/DBT-P01687_DBT-S02168_lcc_Normal_s27_cx363_cy1206_244_244.pickle\n",
            "NORMAL/DBT-P01687_DBT-S02168_lmlo_Normal_s4_cx404_cy1004_244_244.pickle\n",
            "NORMAL/DBT-P01687_DBT-S02168_rmlo_Normal_s36_cx1462_cy1067_244_244.pickle\n",
            "NORMAL/DBT-P01740_DBT-S04363_rmlo_Normal_s13_cx1394_cy1055_244_244.pickle\n",
            "NORMAL/DBT-P01740_DBT-S04363_rcc_Normal_s16_cx1471_cy1400_244_244.pickle\n",
            "NORMAL/DBT-P01740_DBT-S04363_lmlo_Normal_s4_cx488_cy1158_244_244.pickle\n",
            "NORMAL/DBT-P01740_DBT-S04363_lcc_Normal_s4_cx451_cy1389_244_244.pickle\n",
            "NORMAL/DBT-P01724_DBT-S04550_rmlo_Normal_s38_cx1560_cy1125_244_244.pickle\n",
            "NORMAL/DBT-P01724_DBT-S04550_lmlo_Normal_s2_cx415_cy1111_244_244.pickle\n",
            "NORMAL/DBT-P01724_DBT-S04550_lcc_Normal_s71_cx367_cy1399_244_244.pickle\n",
            "NORMAL/DBT-P01724_DBT-S04550_rcc_Normal_s18_cx1631_cy1320_244_244.pickle\n",
            "NORMAL/DBT-P01731_DBT-S00410_rcc_Normal_s22_cx1611_cy1322_244_244.pickle\n",
            "NORMAL/DBT-P01731_DBT-S00410_lmlo_Normal_s59_cx400_cy1072_244_244.pickle\n",
            "NORMAL/DBT-P01731_DBT-S00410_rmlo_Normal_s2_cx1578_cy1042_244_244.pickle\n",
            "NORMAL/DBT-P01731_DBT-S00410_lcc_Normal_s44_cx377_cy1282_244_244.pickle\n",
            "NORMAL/DBT-P01694_DBT-S03053_rmlo_Normal_s89_cx1294_cy1187_244_244.pickle\n",
            "NORMAL/DBT-P01694_DBT-S03053_rcc_Normal_s62_cx1353_cy1178_244_244.pickle\n",
            "NORMAL/DBT-P01694_DBT-S03053_lmlo_Normal_s82_cx714_cy1174_244_244.pickle\n",
            "NORMAL/DBT-P01694_DBT-S03053_lcc_Normal_s78_cx612_cy1080_244_244.pickle\n",
            "NORMAL/DBT-P01701_DBT-S03620_rmlo_Normal_s41_cx1543_cy992_244_244.pickle\n",
            "NORMAL/DBT-P01701_DBT-S03620_lcc_Normal_s23_cx300_cy1309_244_244.pickle\n",
            "NORMAL/DBT-P01701_DBT-S03620_lmlo_Normal_s19_cx348_cy919_244_244.pickle\n",
            "NORMAL/DBT-P01701_DBT-S03620_rcc_Normal_s19_cx1596_cy1309_244_244.pickle\n",
            "NORMAL/DBT-P01717_DBT-S05186_lcc_Normal_s51_cx250_cy939_244_244.pickle\n",
            "NORMAL/DBT-P01717_DBT-S05186_lmlo_Normal_s29_cx363_cy974_244_244.pickle\n",
            "NORMAL/DBT-P01717_DBT-S05186_rcc_Normal_s7_cx1747_cy1195_244_244.pickle\n",
            "NORMAL/DBT-P01717_DBT-S05186_rmlo_Normal_s21_cx1614_cy1103_244_244.pickle\n",
            "NORMAL/DBT-P01734_DBT-S00891_lcc_Normal_s40_cx344_cy1215_244_244.pickle\n",
            "NORMAL/DBT-P01734_DBT-S00891_rmlo_Normal_s33_cx1478_cy1197_244_244.pickle\n",
            "NORMAL/DBT-P01734_DBT-S00891_lmlo_Normal_s58_cx419_cy1090_244_244.pickle\n",
            "NORMAL/DBT-P01734_DBT-S00891_rcc_Normal_s69_cx1573_cy1174_244_244.pickle\n",
            "NORMAL/DBT-P01732_DBT-S05468_lmlo_Normal_s17_cx310_cy945_244_244.pickle\n",
            "NORMAL/DBT-P01732_DBT-S05468_rmlo_Normal_s55_cx1661_cy821_244_244.pickle\n",
            "NORMAL/DBT-P01732_DBT-S05468_lcc_Normal_s21_cx287_cy1241_244_244.pickle\n",
            "NORMAL/DBT-P01732_DBT-S05468_rcc_Normal_s22_cx1718_cy1257_244_244.pickle\n",
            "NORMAL/DBT-P01758_DBT-S00779_lmlo_Normal_s32_cx583_cy1119_244_244.pickle\n",
            "NORMAL/DBT-P01758_DBT-S00779_lcc_Normal_s40_cx514_cy1311_244_244.pickle\n",
            "NORMAL/DBT-P01758_DBT-S00779_rmlo_Normal_s8_cx1423_cy1120_244_244.pickle\n",
            "NORMAL/DBT-P01758_DBT-S00779_rcc_Normal_s24_cx1484_cy1244_244_244.pickle\n",
            "NORMAL/DBT-P01760_DBT-S04428_lmlo_Normal_s50_cx430_cy1040_244_244.pickle\n",
            "NORMAL/DBT-P01760_DBT-S04428_lcc_Normal_s51_cx359_cy1283_244_244.pickle\n",
            "NORMAL/DBT-P01760_DBT-S04428_rmlo_Normal_s62_cx1603_cy1020_244_244.pickle\n",
            "NORMAL/DBT-P01760_DBT-S04428_rcc_Normal_s66_cx1646_cy1352_244_244.pickle\n",
            "NORMAL/DBT-P01742_DBT-S02833_lmlo_Normal_s39_cx520_cy1077_244_244.pickle\n",
            "NORMAL/DBT-P01742_DBT-S02833_rcc_Normal_s38_cx1542_cy1221_244_244.pickle\n",
            "NORMAL/DBT-P01742_DBT-S02833_rmlo_Normal_s13_cx1370_cy1082_244_244.pickle\n",
            "NORMAL/DBT-P01742_DBT-S02833_lcc_Normal_s65_cx377_cy1321_244_244.pickle\n",
            "NORMAL/DBT-P01775_DBT-S05003_lmlo_Normal_s34_cx515_cy1177_244_244.pickle\n",
            "NORMAL/DBT-P01775_DBT-S05003_rmlo_Normal_s2_cx1472_cy1173_244_244.pickle\n",
            "NORMAL/DBT-P01775_DBT-S05003_lcc_Normal_s42_cx485_cy1244_244_244.pickle\n",
            "NORMAL/DBT-P01775_DBT-S05003_rcc_Normal_s41_cx1509_cy1234_244_244.pickle\n",
            "NORMAL/DBT-P01774_DBT-S00424_rmlo_Normal_s38_cx1654_cy987_244_244.pickle\n",
            "NORMAL/DBT-P01774_DBT-S00424_lmlo_Normal_s30_cx330_cy946_244_244.pickle\n",
            "NORMAL/DBT-P01774_DBT-S00424_lcc_Normal_s52_cx290_cy1157_244_244.pickle\n",
            "NORMAL/DBT-P01774_DBT-S00424_rcc_Normal_s8_cx1704_cy1132_244_244.pickle\n",
            "NORMAL/DBT-P01785_DBT-S04701_rmlo_Normal_s19_cx1500_cy1061_244_244.pickle\n",
            "NORMAL/DBT-P01785_DBT-S04701_lcc_Normal_s68_cx424_cy1272_244_244.pickle\n",
            "NORMAL/DBT-P01785_DBT-S04701_rcc_Normal_s17_cx1523_cy1355_244_244.pickle\n",
            "NORMAL/DBT-P01785_DBT-S04701_lmlo_Normal_s7_cx499_cy1088_244_244.pickle\n",
            "NORMAL/DBT-P01762_DBT-S03327_lcc_Normal_s20_cx278_cy1136_244_244.pickle\n",
            "NORMAL/DBT-P01762_DBT-S03327_rmlo_Normal_s52_cx1470_cy1055_244_244.pickle\n",
            "NORMAL/DBT-P01762_DBT-S03327_rcc_Normal_s2_cx1607_cy1204_244_244.pickle\n",
            "NORMAL/DBT-P01762_DBT-S03327_lmlo_Normal_s15_cx361_cy983_244_244.pickle\n",
            "NORMAL/DBT-P01811_DBT-S03098_rmlo_Normal_s54_cx1542_cy1075_244_244.pickle\n",
            "NORMAL/DBT-P01811_DBT-S03098_rcc_Normal_s43_cx1618_cy1196_244_244.pickle\n",
            "NORMAL/DBT-P01811_DBT-S03098_lcc_Normal_s6_cx409_cy1126_244_244.pickle\n",
            "NORMAL/DBT-P01811_DBT-S03098_lmlo_Normal_s46_cx494_cy1113_244_244.pickle\n",
            "NORMAL/DBT-P01816_DBT-S05240_lmlo_Normal_s39_cx484_cy1132_244_244.pickle\n",
            "NORMAL/DBT-P01816_DBT-S05240_rmlo_Normal_s24_cx1406_cy1140_244_244.pickle\n",
            "NORMAL/DBT-P01816_DBT-S05240_lcc_Normal_s25_cx470_cy1340_244_244.pickle\n",
            "NORMAL/DBT-P01816_DBT-S05240_rcc_Normal_s56_cx1430_cy1122_244_244.pickle\n",
            "NORMAL/DBT-P01848_DBT-S01523_lmlo_Normal_s38_cx551_cy1141_244_244.pickle\n",
            "NORMAL/DBT-P01848_DBT-S01523_rcc_Normal_s21_cx1446_cy1207_244_244.pickle\n",
            "NORMAL/DBT-P01848_DBT-S01523_lcc_Normal_s32_cx492_cy1316_244_244.pickle\n",
            "NORMAL/DBT-P01848_DBT-S01523_rmlo_Normal_s46_cx1397_cy1156_244_244.pickle\n",
            "NORMAL/DBT-P01852_DBT-S02287_lcc_Normal_s13_cx606_cy1180_244_244.pickle\n",
            "NORMAL/DBT-P01852_DBT-S02287_rcc_Normal_s32_cx1387_cy1174_244_244.pickle\n",
            "NORMAL/DBT-P01852_DBT-S02287_lmlo_Normal_s6_cx666_cy1101_244_244.pickle\n",
            "NORMAL/DBT-P01852_DBT-S02287_rmlo_Normal_s43_cx1324_cy1132_244_244.pickle\n",
            "NORMAL/DBT-P01937_DBT-S00298_lmlo_Normal_s18_cx333_cy1044_244_244.pickle\n",
            "NORMAL/DBT-P01937_DBT-S00298_lcc_Normal_s42_cx346_cy1180_244_244.pickle\n",
            "NORMAL/DBT-P01937_DBT-S00298_rcc_Normal_s35_cx1688_cy1368_244_244.pickle\n",
            "NORMAL/DBT-P01937_DBT-S00298_rmlo_Normal_s10_cx1682_cy1230_244_244.pickle\n",
            "NORMAL/DBT-P01863_DBT-S00467_lcc_Normal_s1_cx701_cy1258_244_244.pickle\n",
            "NORMAL/DBT-P01863_DBT-S00467_rmlo_Normal_s40_cx1320_cy1147_244_244.pickle\n",
            "NORMAL/DBT-P01863_DBT-S00467_rcc_Normal_s32_cx1322_cy1369_244_244.pickle\n",
            "NORMAL/DBT-P01863_DBT-S00467_lmlo_Normal_s10_cx757_cy1173_244_244.pickle\n",
            "NORMAL/DBT-P01917_DBT-S02116_rmlo_Normal_s61_cx1556_cy1025_244_244.pickle\n",
            "NORMAL/DBT-P01917_DBT-S02116_rcc_Normal_s8_cx1625_cy1260_244_244.pickle\n",
            "NORMAL/DBT-P01917_DBT-S02116_lmlo_Normal_s34_cx438_cy1011_244_244.pickle\n",
            "NORMAL/DBT-P01917_DBT-S02116_lcc_Normal_s29_cx407_cy1295_244_244.pickle\n",
            "NORMAL/DBT-P01949_DBT-S02185_rmlo_Normal_s16_cx1608_cy1069_244_244.pickle\n",
            "NORMAL/DBT-P01949_DBT-S02185_lmlo_Normal_s77_cx402_cy1015_244_244.pickle\n",
            "NORMAL/DBT-P01949_DBT-S02185_lcc_Normal_s23_cx421_cy1330_244_244.pickle\n",
            "NORMAL/DBT-P01949_DBT-S02185_rcc_Normal_s6_cx1618_cy1445_244_244.pickle\n",
            "NORMAL/DBT-P01929_DBT-S01891_lcc_Normal_s30_cx331_cy1193_244_244.pickle\n",
            "NORMAL/DBT-P01929_DBT-S01891_rcc_Normal_s83_cx1708_cy1180_244_244.pickle\n",
            "NORMAL/DBT-P01929_DBT-S01891_lmlo_Normal_s53_cx404_cy952_244_244.pickle\n",
            "NORMAL/DBT-P01929_DBT-S01891_rmlo_Normal_s30_cx1635_cy934_244_244.pickle\n",
            "NORMAL/DBT-P01952_DBT-S03207_lmlo_Normal_s22_cx526_cy1038_244_244.pickle\n",
            "NORMAL/DBT-P01952_DBT-S03207_rmlo_Normal_s68_cx1507_cy1037_244_244.pickle\n",
            "NORMAL/DBT-P01952_DBT-S03207_lcc_Normal_s18_cx518_cy1118_244_244.pickle\n",
            "NORMAL/DBT-P01952_DBT-S03207_rcc_Normal_s48_cx1518_cy1103_244_244.pickle\n",
            "NORMAL/DBT-P01924_DBT-S01952_lmlo_Normal_s3_cx529_cy1054_244_244.pickle\n",
            "NORMAL/DBT-P01924_DBT-S01952_lcc_Normal_s22_cx483_cy1145_244_244.pickle\n",
            "NORMAL/DBT-P01924_DBT-S01952_rmlo_Normal_s2_cx1458_cy992_244_244.pickle\n",
            "NORMAL/DBT-P01924_DBT-S01952_rcc_Normal_s21_cx1505_cy1261_244_244.pickle\n",
            "NORMAL/DBT-P01883_DBT-S01815_lmlo_Normal_s4_cx480_cy1094_244_244.pickle\n",
            "NORMAL/DBT-P01883_DBT-S01815_rcc_Normal_s16_cx1592_cy1256_244_244.pickle\n",
            "NORMAL/DBT-P01883_DBT-S01815_lcc_Normal_s18_cx457_cy1306_244_244.pickle\n",
            "NORMAL/DBT-P01883_DBT-S01815_rmlo_Normal_s50_cx1532_cy980_244_244.pickle\n",
            "NORMAL/DBT-P01913_DBT-S05076_rcc_Normal_s56_cx1661_cy1225_244_244.pickle\n",
            "NORMAL/DBT-P01913_DBT-S05076_rmlo_Normal_s38_cx1550_cy1019_244_244.pickle\n",
            "NORMAL/DBT-P01913_DBT-S05076_lcc_Normal_s2_cx330_cy1208_244_244.pickle\n",
            "NORMAL/DBT-P01913_DBT-S05076_lmlo_Normal_s59_cx397_cy1024_244_244.pickle\n",
            "NORMAL/DBT-P01977_DBT-S03666_rmlo_Normal_s35_cx1394_cy1127_244_244.pickle\n",
            "NORMAL/DBT-P01977_DBT-S03666_lmlo_Normal_s82_cx478_cy1210_244_244.pickle\n",
            "NORMAL/DBT-P01977_DBT-S03666_rcc_Normal_s66_cx1517_cy1469_244_244.pickle\n",
            "NORMAL/DBT-P01977_DBT-S03666_lcc_Normal_s64_cx359_cy1316_244_244.pickle\n",
            "NORMAL/DBT-P01975_DBT-S00392_lcc_Normal_s4_cx330_cy1210_244_244.pickle\n",
            "NORMAL/DBT-P01975_DBT-S00392_lmlo_Normal_s34_cx373_cy984_244_244.pickle\n",
            "NORMAL/DBT-P01975_DBT-S00392_rcc_Normal_s21_cx1669_cy1308_244_244.pickle\n",
            "NORMAL/DBT-P01975_DBT-S00392_rmlo_Normal_s39_cx1652_cy1037_244_244.pickle\n",
            "NORMAL/DBT-P01959_DBT-S05576_lmlo_Normal_s67_cx559_cy1139_244_244.pickle\n",
            "NORMAL/DBT-P01959_DBT-S05576_rmlo_Normal_s58_cx1414_cy1140_244_244.pickle\n",
            "NORMAL/DBT-P01959_DBT-S05576_rcc_Normal_s37_cx1495_cy1345_244_244.pickle\n",
            "NORMAL/DBT-P01959_DBT-S05576_lcc_Normal_s14_cx502_cy1312_244_244.pickle\n",
            "NORMAL/DBT-P02000_DBT-S00466_lmlo_Normal_s41_cx534_cy1127_244_244.pickle\n",
            "NORMAL/DBT-P02000_DBT-S00466_lcc_Normal_s21_cx502_cy1265_244_244.pickle\n",
            "NORMAL/DBT-P02000_DBT-S00466_rmlo_Normal_s49_cx1497_cy1200_244_244.pickle\n",
            "NORMAL/DBT-P02000_DBT-S00466_rcc_Normal_s26_cx1505_cy1257_244_244.pickle\n",
            "NORMAL/DBT-P01974_DBT-S02228_rcc_Normal_s28_cx1588_cy1127_244_244.pickle\n",
            "NORMAL/DBT-P01974_DBT-S02228_lcc_Normal_s9_cx316_cy1209_244_244.pickle\n",
            "NORMAL/DBT-P01974_DBT-S02228_lmlo_Normal_s3_cx331_cy1125_244_244.pickle\n",
            "NORMAL/DBT-P01974_DBT-S02228_rmlo_Normal_s1_cx1564_cy1144_244_244.pickle\n",
            "NORMAL/DBT-P02013_DBT-S00844_rcc_Normal_s28_cx1186_cy1281_244_244.pickle\n",
            "NORMAL/DBT-P02013_DBT-S00844_lcc_Normal_s12_cx725_cy1373_244_244.pickle\n",
            "NORMAL/DBT-P02013_DBT-S00844_lmlo_Normal_s25_cx729_cy1274_244_244.pickle\n",
            "NORMAL/DBT-P02013_DBT-S00844_rmlo_Normal_s18_cx1216_cy1377_244_244.pickle\n",
            "NORMAL/DBT-P01992_DBT-S00415_lcc_Normal_s48_cx413_cy1447_244_244.pickle\n",
            "NORMAL/DBT-P01992_DBT-S00415_rcc_Normal_s48_cx1575_cy1273_244_244.pickle\n",
            "NORMAL/DBT-P01992_DBT-S00415_lmlo_Normal_s59_cx473_cy1010_244_244.pickle\n",
            "NORMAL/DBT-P01992_DBT-S00415_rmlo_Normal_s32_cx1505_cy1047_244_244.pickle\n",
            "NORMAL/DBT-P02119_DBT-S02718_rcc_Normal_s14_cx1441_cy1318_244_244.pickle\n",
            "NORMAL/DBT-P02119_DBT-S02718_lcc_Normal_s23_cx587_cy1287_244_244.pickle\n",
            "NORMAL/DBT-P02119_DBT-S02718_rmlo_Normal_s12_cx1367_cy1166_244_244.pickle\n",
            "NORMAL/DBT-P02119_DBT-S02718_lmlo_Normal_s41_cx650_cy1154_244_244.pickle\n",
            "NORMAL/DBT-P02037_DBT-S01561_lcc_Normal_s30_cx373_cy1202_244_244.pickle\n",
            "NORMAL/DBT-P02037_DBT-S01561_rcc_Normal_s55_cx1557_cy1230_244_244.pickle\n",
            "NORMAL/DBT-P02037_DBT-S01561_lmlo_Normal_s54_cx425_cy1079_244_244.pickle\n",
            "NORMAL/DBT-P02037_DBT-S01561_rmlo_Normal_s10_cx1502_cy1124_244_244.pickle\n",
            "NORMAL/DBT-P02059_DBT-S04081_lcc_Normal_s50_cx506_cy1266_244_244.pickle\n",
            "NORMAL/DBT-P02059_DBT-S04081_rcc_Normal_s14_cx1481_cy1345_244_244.pickle\n",
            "NORMAL/DBT-P02059_DBT-S04081_rmlo_Normal_s18_cx1449_cy1129_244_244.pickle\n",
            "NORMAL/DBT-P02059_DBT-S04081_lmlo_Normal_s2_cx551_cy1232_244_244.pickle\n",
            "NORMAL/DBT-P02030_DBT-S01137_lcc_Normal_s44_cx398_cy1295_244_244.pickle\n",
            "NORMAL/DBT-P02030_DBT-S01137_rmlo_Normal_s33_cx1424_cy1030_244_244.pickle\n",
            "NORMAL/DBT-P02030_DBT-S01137_lmlo_Normal_s23_cx433_cy1053_244_244.pickle\n",
            "NORMAL/DBT-P02030_DBT-S01137_rcc_Normal_s52_cx1496_cy1237_244_244.pickle\n",
            "NORMAL/DBT-P02048_DBT-S00127_rcc_Normal_s10_cx1465_cy1314_244_244.pickle\n",
            "NORMAL/DBT-P02048_DBT-S00127_rmlo_Normal_s73_cx1426_cy1057_244_244.pickle\n",
            "NORMAL/DBT-P02048_DBT-S00127_lmlo_Normal_s17_cx617_cy1118_244_244.pickle\n",
            "NORMAL/DBT-P02048_DBT-S00127_lcc_Normal_s3_cx580_cy1281_244_244.pickle\n",
            "NORMAL/DBT-P02082_DBT-S04717_lcc_Normal_s52_cx308_cy1217_244_244.pickle\n",
            "NORMAL/DBT-P02082_DBT-S04717_lmlo_Normal_s58_cx402_cy1000_244_244.pickle\n",
            "NORMAL/DBT-P02082_DBT-S04717_rcc_Normal_s36_cx1568_cy1322_244_244.pickle\n",
            "NORMAL/DBT-P02082_DBT-S04717_rmlo_Normal_s15_cx1504_cy1121_244_244.pickle\n",
            "NORMAL/DBT-P02063_DBT-S04332_rmlo_Normal_s33_cx1364_cy1105_244_244.pickle\n",
            "NORMAL/DBT-P02063_DBT-S04332_rcc_Normal_s41_cx1441_cy1310_244_244.pickle\n",
            "NORMAL/DBT-P02063_DBT-S04332_lmlo_Normal_s46_cx527_cy1115_244_244.pickle\n",
            "NORMAL/DBT-P02063_DBT-S04332_lcc_Normal_s13_cx482_cy1142_244_244.pickle\n",
            "NORMAL/DBT-P02156_DBT-S05384_lcc_Normal_s11_cx272_cy1321_244_244.pickle\n",
            "NORMAL/DBT-P02156_DBT-S05384_lmlo_Normal_s11_cx278_cy1103_244_244.pickle\n",
            "NORMAL/DBT-P02156_DBT-S05384_rmlo_Normal_s23_cx1589_cy1102_244_244.pickle\n",
            "NORMAL/DBT-P02156_DBT-S05384_rcc_Normal_s3_cx1613_cy1154_244_244.pickle\n",
            "NORMAL/DBT-P02174_DBT-S04323_rcc_Normal_s10_cx1581_cy1259_244_244.pickle\n",
            "NORMAL/DBT-P02174_DBT-S04323_lmlo_Normal_s10_cx317_cy1038_244_244.pickle\n",
            "NORMAL/DBT-P02174_DBT-S04323_rmlo_Normal_s42_cx1552_cy1063_244_244.pickle\n",
            "NORMAL/DBT-P02174_DBT-S04323_lcc_Normal_s12_cx301_cy1272_244_244.pickle\n",
            "NORMAL/DBT-P02129_DBT-S02816_lcc_Normal_s40_cx431_cy1329_244_244.pickle\n",
            "NORMAL/DBT-P02129_DBT-S02816_lmlo_Normal_s26_cx484_cy955_244_244.pickle\n",
            "NORMAL/DBT-P02129_DBT-S02816_rmlo_Normal_s31_cx1424_cy1235_244_244.pickle\n",
            "NORMAL/DBT-P02129_DBT-S02816_rcc_Normal_s74_cx1490_cy1298_244_244.pickle\n",
            "NORMAL/DBT-P02177_DBT-S03363_rmlo_Normal_s68_cx1267_cy1236_244_244.pickle\n",
            "NORMAL/DBT-P02177_DBT-S03363_lmlo_Normal_s70_cx788_cy1234_244_244.pickle\n",
            "NORMAL/DBT-P02177_DBT-S03363_rcc_Normal_s27_cx1246_cy1338_244_244.pickle\n",
            "NORMAL/DBT-P02177_DBT-S03363_lcc_Normal_s50_cx770_cy1254_244_244.pickle\n",
            "NORMAL/DBT-P02137_DBT-S01430_rmlo_Normal_s8_cx1569_cy922_244_244.pickle\n",
            "NORMAL/DBT-P02137_DBT-S01430_rcc_Normal_s35_cx1645_cy1191_244_244.pickle\n",
            "NORMAL/DBT-P02137_DBT-S01430_lmlo_Normal_s50_cx449_cy939_244_244.pickle\n",
            "NORMAL/DBT-P02137_DBT-S01430_lcc_Normal_s23_cx375_cy1172_244_244.pickle\n",
            "NORMAL/DBT-P02192_DBT-S00355_rmlo_Normal_s64_cx1509_cy1018_244_244.pickle\n",
            "NORMAL/DBT-P02192_DBT-S00355_rcc_Normal_s58_cx1550_cy1192_244_244.pickle\n",
            "NORMAL/DBT-P02192_DBT-S00355_lmlo_Normal_s81_cx369_cy988_244_244.pickle\n",
            "NORMAL/DBT-P02192_DBT-S00355_lcc_Normal_s58_cx339_cy1400_244_244.pickle\n",
            "NORMAL/DBT-P02197_DBT-S00416_lmlo_Normal_s23_cx663_cy1215_244_244.pickle\n",
            "NORMAL/DBT-P02197_DBT-S00416_rmlo_Normal_s8_cx1313_cy1060_244_244.pickle\n",
            "NORMAL/DBT-P02197_DBT-S00416_rcc_Normal_s71_cx1439_cy1096_244_244.pickle\n",
            "NORMAL/DBT-P02197_DBT-S00416_lcc_Normal_s36_cx598_cy1285_244_244.pickle\n",
            "NORMAL/DBT-P02182_DBT-S04796_rmlo_Normal_s13_cx1662_cy1059_244_244.pickle\n",
            "NORMAL/DBT-P02182_DBT-S04796_lcc_Normal_s64_cx279_cy1131_244_244.pickle\n",
            "NORMAL/DBT-P02182_DBT-S04796_lmlo_Normal_s24_cx329_cy899_244_244.pickle\n",
            "NORMAL/DBT-P02182_DBT-S04796_rcc_Normal_s5_cx1668_cy1177_244_244.pickle\n",
            "NORMAL/DBT-P02218_DBT-S03643_lmlo_Normal_s19_cx316_cy940_244_244.pickle\n",
            "NORMAL/DBT-P02218_DBT-S03643_rcc_Normal_s16_cx1584_cy1235_244_244.pickle\n",
            "NORMAL/DBT-P02218_DBT-S03643_lcc_Normal_s8_cx318_cy1300_244_244.pickle\n",
            "NORMAL/DBT-P02218_DBT-S03643_rmlo_Normal_s18_cx1561_cy977_244_244.pickle\n",
            "NORMAL/DBT-P02215_DBT-S02270_rcc_Normal_s25_cx1529_cy1176_244_244.pickle\n",
            "NORMAL/DBT-P02215_DBT-S02270_rmlo_Normal_s23_cx1439_cy1203_244_244.pickle\n",
            "NORMAL/DBT-P02215_DBT-S02270_lcc_Normal_s18_cx471_cy1391_244_244.pickle\n",
            "NORMAL/DBT-P02215_DBT-S02270_lmlo_Normal_s75_cx564_cy1105_244_244.pickle\n",
            "NORMAL/DBT-P02269_DBT-S02831_lmlo_Normal_s27_cx403_cy1188_244_244.pickle\n",
            "NORMAL/DBT-P02269_DBT-S02831_rmlo_Normal_s8_cx1499_cy1122_244_244.pickle\n",
            "NORMAL/DBT-P02269_DBT-S02831_rcc_Normal_s61_cx1557_cy1309_244_244.pickle\n",
            "NORMAL/DBT-P02269_DBT-S02831_lcc_Normal_s42_cx346_cy1132_244_244.pickle\n",
            "NORMAL/DBT-P02240_DBT-S05481_lmlo_Normal_s32_cx514_cy1122_244_244.pickle\n",
            "NORMAL/DBT-P02240_DBT-S05481_rcc_Normal_s67_cx1612_cy1360_244_244.pickle\n",
            "NORMAL/DBT-P02240_DBT-S05481_rmlo_Normal_s1_cx1579_cy1107_244_244.pickle\n",
            "NORMAL/DBT-P02240_DBT-S05481_lcc_Normal_s9_cx438_cy1155_244_244.pickle\n",
            "NORMAL/DBT-P02271_DBT-S02656_lmlo_Normal_s37_cx423_cy1080_244_244.pickle\n",
            "NORMAL/DBT-P02271_DBT-S02656_rcc_Normal_s30_cx1517_cy1307_244_244.pickle\n",
            "NORMAL/DBT-P02271_DBT-S02656_rmlo_Normal_s71_cx1469_cy1113_244_244.pickle\n",
            "NORMAL/DBT-P02271_DBT-S02656_lcc_Normal_s5_cx389_cy1355_244_244.pickle\n",
            "NORMAL/DBT-P02255_DBT-S04459_lcc_Normal_s58_cx456_cy1374_244_244.pickle\n",
            "NORMAL/DBT-P02255_DBT-S04459_rmlo_Normal_s72_cx1463_cy1083_244_244.pickle\n",
            "NORMAL/DBT-P02255_DBT-S04459_rcc_Normal_s46_cx1495_cy1321_244_244.pickle\n",
            "NORMAL/DBT-P02255_DBT-S04459_lmlo_Normal_s53_cx434_cy1223_244_244.pickle\n",
            "NORMAL/DBT-P02302_DBT-S02981_lcc_Normal_s35_cx542_cy1345_244_244.pickle\n",
            "NORMAL/DBT-P02302_DBT-S02981_rcc_Normal_s23_cx1494_cy1288_244_244.pickle\n",
            "NORMAL/DBT-P02302_DBT-S02981_rmlo_Normal_s21_cx1434_cy1251_244_244.pickle\n",
            "NORMAL/DBT-P02302_DBT-S02981_lmlo_Normal_s72_cx612_cy1067_244_244.pickle\n",
            "NORMAL/DBT-P02254_DBT-S04505_lmlo_Normal_s28_cx510_cy1052_244_244.pickle\n",
            "NORMAL/DBT-P02254_DBT-S04505_lcc_Normal_s23_cx480_cy1236_244_244.pickle\n",
            "NORMAL/DBT-P02254_DBT-S04505_rmlo_Normal_s23_cx1503_cy1089_244_244.pickle\n",
            "NORMAL/DBT-P02254_DBT-S04505_rcc_Normal_s64_cx1545_cy1186_244_244.pickle\n",
            "NORMAL/DBT-P02305_DBT-S03699_rmlo_Normal_s20_cx1508_cy1079_244_244.pickle\n",
            "NORMAL/DBT-P02305_DBT-S03699_lmlo_Normal_s51_cx471_cy1070_244_244.pickle\n",
            "NORMAL/DBT-P02305_DBT-S03699_rcc_Normal_s1_cx1554_cy1270_244_244.pickle\n",
            "NORMAL/DBT-P02305_DBT-S03699_lcc_Normal_s43_cx439_cy1319_244_244.pickle\n",
            "NORMAL/DBT-P02371_DBT-S01999_rmlo_Normal_s53_cx1512_cy1167_244_244.pickle\n",
            "NORMAL/DBT-P02371_DBT-S01999_lcc_Normal_s50_cx529_cy1314_244_244.pickle\n",
            "NORMAL/DBT-P02371_DBT-S01999_lmlo_Normal_s11_cx534_cy1240_244_244.pickle\n",
            "NORMAL/DBT-P02371_DBT-S01999_rcc_Normal_s18_cx1516_cy1271_244_244.pickle\n",
            "NORMAL/DBT-P02364_DBT-S00172_lmlo_Normal_s25_cx449_cy1190_244_244.pickle\n",
            "NORMAL/DBT-P02364_DBT-S00172_lcc_Normal_s17_cx419_cy1298_244_244.pickle\n",
            "NORMAL/DBT-P02364_DBT-S00172_rcc_Normal_s42_cx1491_cy1276_244_244.pickle\n",
            "NORMAL/DBT-P02364_DBT-S00172_rmlo_Normal_s13_cx1461_cy1182_244_244.pickle\n",
            "NORMAL/DBT-P02317_DBT-S03099_lmlo_Normal_s2_cx532_cy1140_244_244.pickle\n",
            "NORMAL/DBT-P02317_DBT-S03099_lcc_Normal_s46_cx492_cy1120_244_244.pickle\n",
            "NORMAL/DBT-P02317_DBT-S03099_rmlo_Normal_s10_cx1461_cy1153_244_244.pickle\n",
            "NORMAL/DBT-P02317_DBT-S03099_rcc_Normal_s32_cx1487_cy1213_244_244.pickle\n",
            "NORMAL/DBT-P02391_DBT-S03933_rcc_Normal_s24_cx1567_cy1315_244_244.pickle\n",
            "NORMAL/DBT-P02391_DBT-S03933_rmlo_Normal_s33_cx1524_cy912_244_244.pickle\n",
            "NORMAL/DBT-P02391_DBT-S03933_lcc_Normal_s16_cx419_cy1303_244_244.pickle\n",
            "NORMAL/DBT-P02391_DBT-S03933_lmlo_Normal_s24_cx453_cy996_244_244.pickle\n",
            "NORMAL/DBT-P02472_DBT-S05157_lcc_Normal_s40_cx439_cy1291_244_244.pickle\n",
            "NORMAL/DBT-P02472_DBT-S05157_lmlo_Normal_s34_cx466_cy1081_244_244.pickle\n",
            "NORMAL/DBT-P02472_DBT-S05157_rcc_Normal_s39_cx1486_cy1216_244_244.pickle\n",
            "NORMAL/DBT-P02472_DBT-S05157_rmlo_Normal_s12_cx1455_cy1101_244_244.pickle\n",
            "NORMAL/DBT-P02423_DBT-S02516_lcc_Normal_s23_cx462_cy1313_244_244.pickle\n",
            "NORMAL/DBT-P02423_DBT-S02516_rmlo_Normal_s16_cx1478_cy1094_244_244.pickle\n",
            "NORMAL/DBT-P02423_DBT-S02516_rcc_Normal_s17_cx1498_cy1235_244_244.pickle\n",
            "NORMAL/DBT-P02423_DBT-S02516_lmlo_Normal_s40_cx520_cy1025_244_244.pickle\n",
            "NORMAL/DBT-P02443_DBT-S01504_lcc_Normal_s13_cx421_cy1275_244_244.pickle\n",
            "NORMAL/DBT-P02443_DBT-S01504_rmlo_Normal_s36_cx1419_cy1210_244_244.pickle\n",
            "NORMAL/DBT-P02443_DBT-S01504_lmlo_Normal_s27_cx463_cy1332_244_244.pickle\n",
            "NORMAL/DBT-P02443_DBT-S01504_rcc_Normal_s42_cx1456_cy1259_244_244.pickle\n",
            "NORMAL/DBT-P02424_DBT-S02100_rcc_Normal_s10_cx1637_cy1204_244_244.pickle\n",
            "NORMAL/DBT-P02424_DBT-S02100_lmlo_Normal_s53_cx376_cy1079_244_244.pickle\n",
            "NORMAL/DBT-P02424_DBT-S02100_lcc_Normal_s62_cx347_cy1278_244_244.pickle\n",
            "NORMAL/DBT-P02424_DBT-S02100_rmlo_Normal_s4_cx1620_cy925_244_244.pickle\n",
            "NORMAL/DBT-P02412_DBT-S03486_lmlo_Normal_s28_cx488_cy1083_244_244.pickle\n",
            "NORMAL/DBT-P02412_DBT-S03486_rcc_Normal_s3_cx1595_cy1301_244_244.pickle\n",
            "NORMAL/DBT-P02412_DBT-S03486_lcc_Normal_s2_cx469_cy1293_244_244.pickle\n",
            "NORMAL/DBT-P02412_DBT-S03486_rmlo_Normal_s8_cx1557_cy1087_244_244.pickle\n",
            "NORMAL/DBT-P02535_DBT-S03208_lmlo_Normal_s5_cx396_cy1053_244_244.pickle\n",
            "NORMAL/DBT-P02535_DBT-S03208_rcc_Normal_s37_cx1514_cy1245_244_244.pickle\n",
            "NORMAL/DBT-P02535_DBT-S03208_lcc_Normal_s12_cx395_cy1159_244_244.pickle\n",
            "NORMAL/DBT-P02535_DBT-S03208_rmlo_Normal_s23_cx1515_cy1161_244_244.pickle\n",
            "NORMAL/DBT-P02475_DBT-S03162_rmlo_Normal_s19_cx1600_cy973_244_244.pickle\n",
            "NORMAL/DBT-P02475_DBT-S03162_rcc_Normal_s15_cx1685_cy1252_244_244.pickle\n",
            "NORMAL/DBT-P02475_DBT-S03162_lcc_Normal_s48_cx320_cy1250_244_244.pickle\n",
            "NORMAL/DBT-P02475_DBT-S03162_lmlo_Normal_s57_cx365_cy1034_244_244.pickle\n",
            "NORMAL/DBT-P02520_DBT-S04771_rcc_Normal_s69_cx1739_cy1249_244_244.pickle\n",
            "NORMAL/DBT-P02520_DBT-S04771_lmlo_Normal_s33_cx411_cy968_244_244.pickle\n",
            "NORMAL/DBT-P02520_DBT-S04771_rmlo_Normal_s74_cx1623_cy876_244_244.pickle\n",
            "NORMAL/DBT-P02520_DBT-S04771_lcc_Normal_s16_cx362_cy1066_244_244.pickle\n",
            "NORMAL/DBT-P02552_DBT-S02995_rmlo_Normal_s25_cx1528_cy1080_244_244.pickle\n",
            "NORMAL/DBT-P02552_DBT-S02995_lcc_Normal_s19_cx351_cy1266_244_244.pickle\n",
            "NORMAL/DBT-P02552_DBT-S02995_lmlo_Normal_s11_cx391_cy1130_244_244.pickle\n",
            "NORMAL/DBT-P02552_DBT-S02995_rcc_Normal_s7_cx1570_cy1270_244_244.pickle\n",
            "NORMAL/DBT-P02553_DBT-S00031_rmlo_Normal_s2_cx1511_cy1182_244_244.pickle\n",
            "NORMAL/DBT-P02553_DBT-S00031_lmlo_Normal_s28_cx394_cy1078_244_244.pickle\n",
            "NORMAL/DBT-P02553_DBT-S00031_lcc_Normal_s45_cx385_cy1318_244_244.pickle\n",
            "NORMAL/DBT-P02553_DBT-S00031_rcc_Normal_s56_cx1513_cy1273_244_244.pickle\n",
            "NORMAL/DBT-P02567_DBT-S02130_lcc_Normal_s32_cx577_cy1295_244_244.pickle\n",
            "NORMAL/DBT-P02567_DBT-S02130_lmlo_Normal_s77_cx594_cy1059_244_244.pickle\n",
            "NORMAL/DBT-P02567_DBT-S02130_rmlo_Normal_s39_cx1385_cy1076_244_244.pickle\n",
            "NORMAL/DBT-P02567_DBT-S02130_rcc_Normal_s70_cx1407_cy1363_244_244.pickle\n",
            "NORMAL/DBT-P02547_DBT-S04623_lcc_Normal_s8_cx547_cy1168_244_244.pickle\n",
            "NORMAL/DBT-P02547_DBT-S04623_rmlo_Normal_s68_cx1498_cy1137_244_244.pickle\n",
            "NORMAL/DBT-P02547_DBT-S04623_rcc_Normal_s28_cx1471_cy1182_244_244.pickle\n",
            "NORMAL/DBT-P02547_DBT-S04623_lmlo_Normal_s8_cx555_cy1153_244_244.pickle\n",
            "NORMAL/DBT-P02594_DBT-S03777_rcc_Normal_s11_cx1569_cy1242_244_244.pickle\n",
            "NORMAL/DBT-P02594_DBT-S03777_lcc_Normal_s33_cx415_cy1336_244_244.pickle\n",
            "NORMAL/DBT-P02594_DBT-S03777_lmlo_Normal_s23_cx456_cy1100_244_244.pickle\n",
            "NORMAL/DBT-P02594_DBT-S03777_rmlo_Normal_s56_cx1583_cy1125_244_244.pickle\n",
            "NORMAL/DBT-P02717_DBT-S01383_lcc_Normal_s20_cx285_cy1227_244_244.pickle\n",
            "NORMAL/DBT-P02717_DBT-S01383_lmlo_Normal_s6_cx360_cy1074_244_244.pickle\n",
            "NORMAL/DBT-P02717_DBT-S01383_rmlo_Normal_s48_cx1534_cy1082_244_244.pickle\n",
            "NORMAL/DBT-P02717_DBT-S01383_rcc_Normal_s38_cx1580_cy1200_244_244.pickle\n",
            "NORMAL/DBT-P02691_DBT-S01480_rmlo_Normal_s26_cx1521_cy1037_244_244.pickle\n",
            "NORMAL/DBT-P02691_DBT-S01480_rcc_Normal_s19_cx1599_cy1184_244_244.pickle\n",
            "NORMAL/DBT-P02691_DBT-S01480_lcc_Normal_s2_cx314_cy1228_244_244.pickle\n",
            "NORMAL/DBT-P02691_DBT-S01480_lmlo_Normal_s36_cx373_cy1091_244_244.pickle\n",
            "NORMAL/DBT-P02630_DBT-S02064_lcc_Normal_s21_cx319_cy1259_244_244.pickle\n",
            "NORMAL/DBT-P02630_DBT-S02064_rcc_Normal_s41_cx1580_cy1152_244_244.pickle\n",
            "NORMAL/DBT-P02630_DBT-S02064_lmlo_Normal_s34_cx370_cy1096_244_244.pickle\n",
            "NORMAL/DBT-P02630_DBT-S02064_rmlo_Normal_s11_cx1543_cy1126_244_244.pickle\n",
            "NORMAL/DBT-P02611_DBT-S02594_rmlo_Normal_s51_cx1452_cy1148_244_244.pickle\n",
            "NORMAL/DBT-P02611_DBT-S02594_rcc_Normal_s53_cx1512_cy1041_244_244.pickle\n",
            "NORMAL/DBT-P02611_DBT-S02594_lcc_Normal_s21_cx384_cy1292_244_244.pickle\n",
            "NORMAL/DBT-P02611_DBT-S02594_lmlo_Normal_s28_cx429_cy1157_244_244.pickle\n",
            "NORMAL/DBT-P02605_DBT-S03937_rmlo_Normal_s1_cx1559_cy1069_244_244.pickle\n",
            "NORMAL/DBT-P02605_DBT-S03937_lmlo_Normal_s33_cx428_cy1013_244_244.pickle\n",
            "NORMAL/DBT-P02605_DBT-S03937_lcc_Normal_s27_cx393_cy1250_244_244.pickle\n",
            "NORMAL/DBT-P02605_DBT-S03937_rcc_Normal_s43_cx1614_cy1219_244_244.pickle\n",
            "NORMAL/DBT-P02683_DBT-S03951_rcc_Normal_s42_cx1613_cy1270_244_244.pickle\n",
            "NORMAL/DBT-P02683_DBT-S03951_lmlo_Normal_s31_cx307_cy976_244_244.pickle\n",
            "NORMAL/DBT-P02683_DBT-S03951_lcc_Normal_s1_cx258_cy1333_244_244.pickle\n",
            "NORMAL/DBT-P02683_DBT-S03951_rmlo_Normal_s27_cx1584_cy1098_244_244.pickle\n",
            "NORMAL/DBT-P02655_DBT-S04512_rcc_Normal_s23_cx1722_cy1002_244_244.pickle\n",
            "NORMAL/DBT-P02655_DBT-S04512_lmlo_Normal_s14_cx286_cy918_244_244.pickle\n",
            "NORMAL/DBT-P02655_DBT-S04512_rmlo_Normal_s40_cx1727_cy1131_244_244.pickle\n",
            "NORMAL/DBT-P02655_DBT-S04512_lcc_Normal_s5_cx184_cy1142_244_244.pickle\n",
            "NORMAL/DBT-P02793_DBT-S03105_lcc_Normal_s37_cx311_cy1332_244_244.pickle\n",
            "NORMAL/DBT-P02793_DBT-S03105_rcc_Normal_s58_cx1662_cy1368_244_244.pickle\n",
            "NORMAL/DBT-P02793_DBT-S03105_rmlo_Normal_s51_cx1647_cy984_244_244.pickle\n",
            "NORMAL/DBT-P02793_DBT-S03105_lmlo_Normal_s40_cx333_cy956_244_244.pickle\n",
            "NORMAL/DBT-P02770_DBT-S03504_rcc_Normal_s21_cx1511_cy1175_244_244.pickle\n",
            "NORMAL/DBT-P02770_DBT-S03504_rmlo_Normal_s72_cx1401_cy1167_244_244.pickle\n",
            "NORMAL/DBT-P02770_DBT-S03504_lcc_Normal_s25_cx474_cy1184_244_244.pickle\n",
            "NORMAL/DBT-P02770_DBT-S03504_lmlo_Normal_s44_cx549_cy1178_244_244.pickle\n",
            "NORMAL/DBT-P02810_DBT-S00194_lmlo_Normal_s8_cx413_cy1065_244_244.pickle\n",
            "NORMAL/DBT-P02810_DBT-S00194_rmlo_Normal_s18_cx1454_cy1151_244_244.pickle\n",
            "NORMAL/DBT-P02810_DBT-S00194_rcc_Normal_s66_cx1518_cy1045_244_244.pickle\n",
            "NORMAL/DBT-P02810_DBT-S00194_lcc_Normal_s3_cx333_cy1322_244_244.pickle\n",
            "NORMAL/DBT-P02722_DBT-S04111_rcc_Normal_s21_cx1389_cy1268_244_244.pickle\n",
            "NORMAL/DBT-P02722_DBT-S04111_rmlo_Normal_s25_cx1413_cy1247_244_244.pickle\n",
            "NORMAL/DBT-P02722_DBT-S04111_lcc_Normal_s67_cx539_cy1291_244_244.pickle\n",
            "NORMAL/DBT-P02722_DBT-S04111_lmlo_Normal_s48_cx573_cy1285_244_244.pickle\n",
            "NORMAL/DBT-P02788_DBT-S05433_rcc_Normal_s17_cx1803_cy1361_244_244.pickle\n",
            "NORMAL/DBT-P02788_DBT-S05433_rmlo_Normal_s21_cx1793_cy1262_244_244.pickle\n",
            "NORMAL/DBT-P02788_DBT-S05433_lmlo_Normal_s45_cx189_cy1247_244_244.pickle\n",
            "NORMAL/DBT-P02788_DBT-S05433_lcc_Normal_s50_cx143_cy1420_244_244.pickle\n",
            "NORMAL/DBT-P02785_DBT-S05543_lmlo_Normal_s45_cx472_cy954_244_244.pickle\n",
            "NORMAL/DBT-P02785_DBT-S05543_rmlo_Normal_s24_cx1584_cy1060_244_244.pickle\n",
            "NORMAL/DBT-P02785_DBT-S05543_lcc_Normal_s42_cx417_cy1191_244_244.pickle\n",
            "NORMAL/DBT-P02785_DBT-S05543_rcc_Normal_s25_cx1604_cy1159_244_244.pickle\n",
            "NORMAL/DBT-P02850_DBT-S05299_rmlo_Normal_s33_cx1620_cy1061_244_244.pickle\n",
            "NORMAL/DBT-P02850_DBT-S05299_rcc_Normal_s8_cx1598_cy1078_244_244.pickle\n",
            "NORMAL/DBT-P02850_DBT-S05299_lmlo_Normal_s30_cx427_cy1113_244_244.pickle\n",
            "NORMAL/DBT-P02850_DBT-S05299_lcc_Normal_s9_cx303_cy1120_244_244.pickle\n",
            "NORMAL/DBT-P02835_DBT-S05478_lcc_Normal_s27_cx319_cy1360_244_244.pickle\n",
            "NORMAL/DBT-P02835_DBT-S05478_lmlo_Normal_s39_cx383_cy1054_244_244.pickle\n",
            "NORMAL/DBT-P02835_DBT-S05478_rmlo_Normal_s53_cx1518_cy962_244_244.pickle\n",
            "NORMAL/DBT-P02835_DBT-S05478_rcc_Normal_s54_cx1616_cy1151_244_244.pickle\n",
            "NORMAL/DBT-P02815_DBT-S05016_lmlo_Normal_s72_cx356_cy918_244_244.pickle\n",
            "NORMAL/DBT-P02815_DBT-S05016_rcc_Normal_s22_cx1632_cy1357_244_244.pickle\n",
            "NORMAL/DBT-P02815_DBT-S05016_rmlo_Normal_s13_cx1597_cy879_244_244.pickle\n",
            "NORMAL/DBT-P02815_DBT-S05016_lcc_Normal_s61_cx336_cy1169_244_244.pickle\n",
            "NORMAL/DBT-P02866_DBT-S03080_lcc_Normal_s18_cx453_cy1381_244_244.pickle\n",
            "NORMAL/DBT-P02866_DBT-S03080_rcc_Normal_s65_cx1458_cy1342_244_244.pickle\n",
            "NORMAL/DBT-P02866_DBT-S03080_rmlo_Normal_s8_cx1393_cy1051_244_244.pickle\n",
            "NORMAL/DBT-P02866_DBT-S03080_lmlo_Normal_s20_cx461_cy1047_244_244.pickle\n",
            "NORMAL/DBT-P02819_DBT-S03142_lmlo_Normal_s10_cx457_cy1194_244_244.pickle\n",
            "NORMAL/DBT-P02819_DBT-S03142_rmlo_Normal_s8_cx1462_cy1229_244_244.pickle\n",
            "NORMAL/DBT-P02819_DBT-S03142_lcc_Normal_s64_cx398_cy1120_244_244.pickle\n",
            "NORMAL/DBT-P02819_DBT-S03142_rcc_Normal_s5_cx1503_cy1173_244_244.pickle\n",
            "NORMAL/DBT-P02858_DBT-S05489_rmlo_Normal_s56_cx1425_cy1265_244_244.pickle\n",
            "NORMAL/DBT-P02858_DBT-S05489_lcc_Normal_s38_cx666_cy1186_244_244.pickle\n",
            "NORMAL/DBT-P02858_DBT-S05489_lmlo_Normal_s30_cx618_cy1255_244_244.pickle\n",
            "NORMAL/DBT-P02858_DBT-S05489_rcc_Normal_s26_cx1401_cy1150_244_244.pickle\n",
            "NORMAL/DBT-P02856_DBT-S02098_rcc_Normal_s44_cx1552_cy1196_244_244.pickle\n",
            "NORMAL/DBT-P02856_DBT-S02098_rmlo_Normal_s11_cx1554_cy1051_244_244.pickle\n",
            "NORMAL/DBT-P02856_DBT-S02098_lcc_Normal_s8_cx344_cy1350_244_244.pickle\n",
            "NORMAL/DBT-P02856_DBT-S02098_lmlo_Normal_s7_cx345_cy1062_244_244.pickle\n",
            "NORMAL/DBT-P02888_DBT-S02676_rcc_Normal_s5_cx1655_cy1295_244_244.pickle\n",
            "NORMAL/DBT-P02888_DBT-S02676_lcc_Normal_s25_cx340_cy1358_244_244.pickle\n",
            "NORMAL/DBT-P02888_DBT-S02676_rmlo_Normal_s32_cx1609_cy949_244_244.pickle\n",
            "NORMAL/DBT-P02888_DBT-S02676_lmlo_Normal_s26_cx397_cy948_244_244.pickle\n",
            "NORMAL/DBT-P02907_DBT-S05500_lmlo_Normal_s51_cx439_cy1020_244_244.pickle\n",
            "NORMAL/DBT-P02907_DBT-S05500_lcc_Normal_s1_cx369_cy1223_244_244.pickle\n",
            "NORMAL/DBT-P02907_DBT-S05500_rcc_Normal_s9_cx1545_cy1217_244_244.pickle\n",
            "NORMAL/DBT-P02907_DBT-S05500_rmlo_Normal_s46_cx1475_cy1053_244_244.pickle\n",
            "NORMAL/DBT-P02890_DBT-S05488_lmlo_Normal_s47_cx337_cy1165_244_244.pickle\n",
            "NORMAL/DBT-P02890_DBT-S05488_rcc_Normal_s2_cx1606_cy1250_244_244.pickle\n",
            "NORMAL/DBT-P02890_DBT-S05488_rmlo_Normal_s23_cx1516_cy1133_244_244.pickle\n",
            "NORMAL/DBT-P02890_DBT-S05488_lcc_Normal_s12_cx298_cy1319_244_244.pickle\n",
            "NORMAL/DBT-P02872_DBT-S02928_lmlo_Normal_s42_cx691_cy1340_244_244.pickle\n",
            "NORMAL/DBT-P02872_DBT-S02928_rmlo_Normal_s65_cx1246_cy1368_244_244.pickle\n",
            "NORMAL/DBT-P02872_DBT-S02928_rcc_Normal_s39_cx1241_cy1393_244_244.pickle\n",
            "NORMAL/DBT-P02872_DBT-S02928_lcc_Normal_s28_cx654_cy1273_244_244.pickle\n",
            "NORMAL/DBT-P02918_DBT-S05518_lmlo_Normal_s30_cx447_cy984_244_244.pickle\n",
            "NORMAL/DBT-P02918_DBT-S05518_lcc_Normal_s39_cx376_cy1214_244_244.pickle\n",
            "NORMAL/DBT-P02918_DBT-S05518_rcc_Normal_s84_cx1659_cy1162_244_244.pickle\n",
            "NORMAL/DBT-P02918_DBT-S05518_rmlo_Normal_s2_cx1607_cy933_244_244.pickle\n",
            "NORMAL/DBT-P02917_DBT-S00264_lmlo_Normal_s60_cx792_cy1278_244_244.pickle\n",
            "NORMAL/DBT-P02917_DBT-S00264_rcc_Normal_s62_cx1257_cy1160_244_244.pickle\n",
            "NORMAL/DBT-P02917_DBT-S00264_lcc_Normal_s46_cx782_cy1327_244_244.pickle\n",
            "NORMAL/DBT-P02917_DBT-S00264_rmlo_Normal_s21_cx1206_cy1280_244_244.pickle\n",
            "NORMAL/DBT-P02949_DBT-S05119_rmlo_Normal_s13_cx1511_cy1119_244_244.pickle\n",
            "NORMAL/DBT-P02949_DBT-S05119_lmlo_Normal_s57_cx432_cy1046_244_244.pickle\n",
            "NORMAL/DBT-P02949_DBT-S05119_lcc_Normal_s10_cx381_cy1211_244_244.pickle\n",
            "NORMAL/DBT-P02949_DBT-S05119_rcc_Normal_s65_cx1590_cy1258_244_244.pickle\n",
            "NORMAL/DBT-P02996_DBT-S00232_rcc_Normal_s40_cx1386_cy1244_244_244.pickle\n",
            "NORMAL/DBT-P02996_DBT-S00232_lmlo_Normal_s41_cx535_cy1270_244_244.pickle\n",
            "NORMAL/DBT-P02996_DBT-S00232_rmlo_Normal_s47_cx1383_cy1209_244_244.pickle\n",
            "NORMAL/DBT-P02996_DBT-S00232_lcc_Normal_s36_cx516_cy1360_244_244.pickle\n",
            "NORMAL/DBT-P02929_DBT-S00519_lcc_Normal_s66_cx443_cy1240_244_244.pickle\n",
            "NORMAL/DBT-P02929_DBT-S00519_lmlo_Normal_s33_cx500_cy1163_244_244.pickle\n",
            "NORMAL/DBT-P02929_DBT-S00519_rcc_Normal_s40_cx1562_cy1166_244_244.pickle\n",
            "NORMAL/DBT-P02929_DBT-S00519_rmlo_Normal_s83_cx1476_cy1105_244_244.pickle\n",
            "NORMAL/DBT-P02988_DBT-S00313_rmlo_Normal_s44_cx1384_cy1217_244_244.pickle\n",
            "NORMAL/DBT-P02988_DBT-S00313_rcc_Normal_s76_cx1433_cy1407_244_244.pickle\n",
            "NORMAL/DBT-P02988_DBT-S00313_lmlo_Normal_s34_cx508_cy1195_244_244.pickle\n",
            "NORMAL/DBT-P02988_DBT-S00313_lcc_Normal_s43_cx504_cy1230_244_244.pickle\n",
            "NORMAL/DBT-P02956_DBT-S00235_rmlo_Normal_s8_cx1563_cy939_244_244.pickle\n",
            "NORMAL/DBT-P02956_DBT-S00235_rcc_Normal_s32_cx1611_cy1283_244_244.pickle\n",
            "NORMAL/DBT-P02956_DBT-S00235_lmlo_Normal_s34_cx328_cy918_244_244.pickle\n",
            "NORMAL/DBT-P02956_DBT-S00235_lcc_Normal_s13_cx287_cy1227_244_244.pickle\n",
            "NORMAL/DBT-P03052_DBT-S02554_rmlo_Normal_s38_cx1468_cy991_244_244.pickle\n",
            "NORMAL/DBT-P03052_DBT-S02554_lcc_Normal_s18_cx345_cy1346_244_244.pickle\n",
            "NORMAL/DBT-P03052_DBT-S02554_lmlo_Normal_s36_cx378_cy1039_244_244.pickle\n",
            "NORMAL/DBT-P03052_DBT-S02554_rcc_Normal_s38_cx1548_cy1138_244_244.pickle\n",
            "NORMAL/DBT-P03000_DBT-S03387_rcc_Normal_s48_cx1459_cy1075_244_244.pickle\n",
            "NORMAL/DBT-P03000_DBT-S03387_lcc_Normal_s48_cx415_cy1213_244_244.pickle\n",
            "NORMAL/DBT-P03000_DBT-S03387_lmlo_Normal_s18_cx407_cy1151_244_244.pickle\n",
            "NORMAL/DBT-P03000_DBT-S03387_rmlo_Normal_s44_cx1477_cy1186_244_244.pickle\n",
            "NORMAL/DBT-P03045_DBT-S01882_rmlo_Normal_s65_cx1549_cy1129_244_244.pickle\n",
            "NORMAL/DBT-P03045_DBT-S01882_lmlo_Normal_s22_cx446_cy1141_244_244.pickle\n",
            "NORMAL/DBT-P03045_DBT-S01882_lcc_Normal_s53_cx418_cy1204_244_244.pickle\n",
            "NORMAL/DBT-P03045_DBT-S01882_rcc_Normal_s16_cx1544_cy1199_244_244.pickle\n",
            "NORMAL/DBT-P03021_DBT-S03028_rmlo_Normal_s42_cx1456_cy1198_244_244.pickle\n",
            "NORMAL/DBT-P03021_DBT-S03028_rcc_Normal_s55_cx1454_cy1281_244_244.pickle\n",
            "NORMAL/DBT-P03021_DBT-S03028_lmlo_Normal_s55_cx602_cy1289_244_244.pickle\n",
            "NORMAL/DBT-P03021_DBT-S03028_lcc_Normal_s4_cx535_cy1236_244_244.pickle\n",
            "NORMAL/DBT-P02999_DBT-S00491_lcc_Normal_s50_cx317_cy1173_244_244.pickle\n",
            "NORMAL/DBT-P02999_DBT-S00491_rmlo_Normal_s7_cx1514_cy1133_244_244.pickle\n",
            "NORMAL/DBT-P02999_DBT-S00491_lmlo_Normal_s1_cx396_cy1154_244_244.pickle\n",
            "NORMAL/DBT-P02999_DBT-S00491_rcc_Normal_s26_cx1604_cy1129_244_244.pickle\n",
            "NORMAL/DBT-P03055_DBT-S02964_rmlo_Normal_s31_cx1421_cy1197_244_244.pickle\n",
            "NORMAL/DBT-P03055_DBT-S02964_lmlo_Normal_s13_cx626_cy1141_244_244.pickle\n",
            "NORMAL/DBT-P03055_DBT-S02964_lcc_Normal_s85_cx554_cy1303_244_244.pickle\n",
            "NORMAL/DBT-P03055_DBT-S02964_rcc_Normal_s1_cx1435_cy1301_244_244.pickle\n",
            "NORMAL/DBT-P03281_DBT-S05018_rcc_Normal_s7_cx1394_cy1150_244_244.pickle\n",
            "NORMAL/DBT-P03281_DBT-S05018_lmlo_Normal_s61_cx597_cy1351_244_244.pickle\n",
            "NORMAL/DBT-P03276_DBT-S03596_rmlo_Normal_s50_cx1463_cy1113_244_244.pickle\n",
            "NORMAL/DBT-P03276_DBT-S03596_rcc_Normal_s16_cx1488_cy1196_244_244.pickle\n",
            "NORMAL/DBT-P03276_DBT-S03596_lmlo_Normal_s42_cx426_cy1083_244_244.pickle\n",
            "NORMAL/DBT-P03276_DBT-S03596_lcc_Normal_s66_cx353_cy1311_244_244.pickle\n",
            "NORMAL/DBT-P03374_DBT-S03321_rmlo_Normal_s55_cx1423_cy1134_244_244.pickle\n",
            "NORMAL/DBT-P03374_DBT-S03321_rcc_Normal_s63_cx1497_cy1369_244_244.pickle\n",
            "NORMAL/DBT-P03374_DBT-S03321_lmlo_Normal_s19_cx584_cy1201_244_244.pickle\n",
            "NORMAL/DBT-P03374_DBT-S03321_lcc_Normal_s14_cx537_cy1220_244_244.pickle\n",
            "NORMAL/DBT-P03301_DBT-S05332_rmlo_Normal_s17_cx1546_cy1029_244_244.pickle\n",
            "NORMAL/DBT-P03301_DBT-S05332_lcc_Normal_s9_cx332_cy1145_244_244.pickle\n",
            "NORMAL/DBT-P03301_DBT-S05332_lmlo_Normal_s33_cx378_cy1059_244_244.pickle\n",
            "NORMAL/DBT-P03301_DBT-S05332_rcc_Normal_s9_cx1565_cy1161_244_244.pickle\n",
            "NORMAL/DBT-P03342_DBT-S04344_rcc_Normal_s36_cx1587_cy1236_244_244.pickle\n",
            "NORMAL/DBT-P03342_DBT-S04344_lcc_Normal_s34_cx423_cy1220_244_244.pickle\n",
            "NORMAL/DBT-P03342_DBT-S04344_lmlo_Normal_s5_cx463_cy1096_244_244.pickle\n",
            "NORMAL/DBT-P03342_DBT-S04344_rmlo_Normal_s20_cx1537_cy1086_244_244.pickle\n",
            "NORMAL/DBT-P03324_DBT-S03541_lmlo_Normal_s28_cx651_cy1287_244_244.pickle\n",
            "NORMAL/DBT-P03324_DBT-S03541_rmlo_Normal_s23_cx1295_cy1171_244_244.pickle\n",
            "NORMAL/DBT-P00064_DBT-S02651_lcc_Normal_s71_cx384_cy1242_244_244.pickle\n",
            "NORMAL/DBT-P00064_DBT-S02651_lmlo_Normal_s46_cx411_cy1097_244_244.pickle\n",
            "NORMAL/DBT-P00064_DBT-S02651_rcc_Normal_s75_cx1644_cy1372_244_244.pickle\n",
            "NORMAL/DBT-P00064_DBT-S02651_rmlo_Normal_s65_cx1620_cy1017_244_244.pickle\n",
            "NORMAL/DBT-P00052_DBT-S00420_rcc_Normal_s43_cx1548_cy1249_244_244.pickle\n",
            "NORMAL/DBT-P00052_DBT-S00420_rmlo_Normal_s4_cx1512_cy1104_244_244.pickle\n",
            "NORMAL/DBT-P00052_DBT-S00420_lcc_Normal_s59_cx482_cy1340_244_244.pickle\n",
            "NORMAL/DBT-P00052_DBT-S00420_lmlo_Normal_s16_cx477_cy1260_244_244.pickle\n",
            "NORMAL/DBT-P00076_DBT-S00996_lmlo_Normal_s23_cx361_cy1007_244_244.pickle\n",
            "NORMAL/DBT-P00076_DBT-S00996_lcc_Normal_s6_cx324_cy1293_244_244.pickle\n",
            "NORMAL/DBT-P00076_DBT-S00996_rmlo_Normal_s58_cx1635_cy958_244_244.pickle\n",
            "NORMAL/DBT-P00076_DBT-S00996_rcc_Normal_s1_cx1671_cy1185_244_244.pickle\n",
            "NORMAL/DBT-P00029_DBT-S00044_rcc_Normal_s62_cx1546_cy1336_244_244.pickle\n",
            "NORMAL/DBT-P00029_DBT-S00044_lcc_Normal_s68_cx356_cy1338_244_244.pickle\n",
            "NORMAL/DBT-P00029_DBT-S00044_lmlo_Normal_s47_cx432_cy1139_244_244.pickle\n",
            "NORMAL/DBT-P00029_DBT-S00044_rmlo_Normal_s58_cx1450_cy1078_244_244.pickle\n",
            "NORMAL/DBT-P00045_DBT-S03478_rmlo_Normal_s21_cx1666_cy786_244_244.pickle\n",
            "NORMAL/DBT-P00045_DBT-S03478_lcc_Normal_s23_cx137_cy1145_244_244.pickle\n",
            "NORMAL/DBT-P00045_DBT-S03478_lmlo_Normal_s14_cx219_cy835_244_244.pickle\n",
            "NORMAL/DBT-P00032_DBT-S04706_rcc_Normal_s32_cx1567_cy1503_244_244.pickle\n",
            "NORMAL/DBT-P00032_DBT-S04706_lmlo_Normal_s15_cx480_cy1029_244_244.pickle\n",
            "NORMAL/DBT-P00032_DBT-S04706_lcc_Normal_s50_cx444_cy1369_244_244.pickle\n",
            "NORMAL/DBT-P00032_DBT-S04706_rmlo_Normal_s23_cx1507_cy984_244_244.pickle\n",
            "NORMAL/DBT-P00109_DBT-S02595_rcc_Normal_s57_cx1672_cy1211_244_244.pickle\n",
            "NORMAL/DBT-P00109_DBT-S02595_lcc_Normal_s16_cx225_cy1306_244_244.pickle\n",
            "NORMAL/DBT-P00109_DBT-S02595_rmlo_Normal_s20_cx1558_cy921_244_244.pickle\n",
            "NORMAL/DBT-P00109_DBT-S02595_lmlo_Normal_s45_cx328_cy956_244_244.pickle\n",
            "NORMAL/DBT-P00116_DBT-S03877_lmlo_Normal_s37_cx248_cy884_244_244.pickle\n",
            "NORMAL/DBT-P00116_DBT-S03877_lcc_Normal_s31_cx219_cy1182_244_244.pickle\n",
            "NORMAL/DBT-P00116_DBT-S03877_rmlo_Normal_s2_cx1736_cy893_244_244.pickle\n",
            "NORMAL/DBT-P00116_DBT-S03877_rcc_Normal_s29_cx1756_cy1190_244_244.pickle\n",
            "NORMAL/DBT-P00125_DBT-S04096_rcc_Normal_s21_cx1612_cy1094_244_244.pickle\n",
            "NORMAL/DBT-P00125_DBT-S04096_lmlo_Normal_s43_cx424_cy980_244_244.pickle\n",
            "NORMAL/DBT-P00125_DBT-S04096_rmlo_Normal_s10_cx1599_cy933_244_244.pickle\n",
            "NORMAL/DBT-P00125_DBT-S04096_lcc_Normal_s13_cx428_cy1155_244_244.pickle\n",
            "NORMAL/DBT-P00113_DBT-S01549_rmlo_Normal_s5_cx1693_cy796_244_244.pickle\n",
            "NORMAL/DBT-P00113_DBT-S01549_lcc_Normal_s1_cx244_cy1285_244_244.pickle\n",
            "NORMAL/DBT-P00113_DBT-S01549_rcc_Normal_s57_cx1762_cy1101_244_244.pickle\n",
            "NORMAL/DBT-P00113_DBT-S01549_lmlo_Normal_s20_cx308_cy850_244_244.pickle\n",
            "NORMAL/DBT-P00128_DBT-S04364_rcc_Normal_s4_cx1520_cy1182_244_244.pickle\n",
            "NORMAL/DBT-P00128_DBT-S04364_lcc_Normal_s38_cx474_cy1295_244_244.pickle\n",
            "NORMAL/DBT-P00128_DBT-S04364_lmlo_Normal_s44_cx495_cy1074_244_244.pickle\n",
            "NORMAL/DBT-P00128_DBT-S04364_rmlo_Normal_s32_cx1492_cy1003_244_244.pickle\n",
            "NORMAL/DBT-P00130_DBT-S04691_rmlo_Normal_s20_cx1285_cy1282_244_244.pickle\n",
            "NORMAL/DBT-P00130_DBT-S04691_lcc_Normal_s24_cx651_cy1144_244_244.pickle\n",
            "NORMAL/DBT-P00130_DBT-S04691_rcc_Normal_s44_cx1354_cy1120_244_244.pickle\n",
            "NORMAL/DBT-P00130_DBT-S04691_lmlo_Normal_s5_cx676_cy1204_244_244.pickle\n",
            "NORMAL/DBT-P00141_DBT-S01200_rcc_Normal_s42_cx1480_cy1106_244_244.pickle\n",
            "NORMAL/DBT-P00141_DBT-S01200_rmlo_Normal_s33_cx1433_cy1202_244_244.pickle\n",
            "NORMAL/DBT-P00141_DBT-S01200_lcc_Normal_s33_cx350_cy1086_244_244.pickle\n",
            "NORMAL/DBT-P00141_DBT-S01200_lmlo_Normal_s49_cx429_cy1186_244_244.pickle\n",
            "NORMAL/DBT-P00120_DBT-S00429_rcc_Normal_s45_cx1649_cy1203_244_244.pickle\n",
            "NORMAL/DBT-P00120_DBT-S00429_rmlo_Normal_s18_cx1573_cy958_244_244.pickle\n",
            "NORMAL/DBT-P00120_DBT-S00429_lmlo_Normal_s47_cx322_cy862_244_244.pickle\n",
            "NORMAL/DBT-P00120_DBT-S00429_lcc_Normal_s48_cx248_cy1239_244_244.pickle\n",
            "NORMAL/DBT-P00122_DBT-S02054_rcc_Normal_s30_cx1555_cy1231_244_244.pickle\n",
            "NORMAL/DBT-P00122_DBT-S02054_rmlo_Normal_s18_cx1531_cy1081_244_244.pickle\n",
            "NORMAL/DBT-P00122_DBT-S02054_lmlo_Normal_s41_cx442_cy1055_244_244.pickle\n",
            "NORMAL/DBT-P00122_DBT-S02054_lcc_Normal_s21_cx402_cy1278_244_244.pickle\n",
            "NORMAL/DBT-P00156_DBT-S03841_rmlo_Normal_s13_cx1665_cy854_244_244.pickle\n",
            "NORMAL/DBT-P00156_DBT-S03841_lcc_Normal_s4_cx290_cy1183_244_244.pickle\n",
            "NORMAL/DBT-P00156_DBT-S03841_lmlo_Normal_s55_cx374_cy911_244_244.pickle\n",
            "NORMAL/DBT-P00156_DBT-S03841_rcc_Normal_s13_cx1736_cy1175_244_244.pickle\n",
            "NORMAL/DBT-P00149_DBT-S02770_lcc_Normal_s84_cx170_cy1233_244_244.pickle\n",
            "NORMAL/DBT-P00149_DBT-S02770_rcc_Normal_s68_cx1727_cy1385_244_244.pickle\n",
            "NORMAL/DBT-P00149_DBT-S02770_rmlo_Normal_s37_cx1497_cy1045_244_244.pickle\n",
            "NORMAL/DBT-P00149_DBT-S02770_lmlo_Normal_s52_cx381_cy1072_244_244.pickle\n",
            "NORMAL/DBT-P00152_DBT-S03088_lmlo_Normal_s53_cx489_cy1108_244_244.pickle\n",
            "NORMAL/DBT-P00152_DBT-S03088_lcc_Normal_s17_cx448_cy1301_244_244.pickle\n",
            "NORMAL/DBT-P00152_DBT-S03088_rcc_Normal_s14_cx1545_cy1205_244_244.pickle\n",
            "NORMAL/DBT-P00152_DBT-S03088_rmlo_Normal_s53_cx1509_cy1098_244_244.pickle\n",
            "NORMAL/DBT-P00167_DBT-S03949_lcc_Normal_s82_cx403_cy1334_244_244.pickle\n",
            "NORMAL/DBT-P00167_DBT-S03949_rmlo_Normal_s84_cx1621_cy1004_244_244.pickle\n",
            "NORMAL/DBT-P00167_DBT-S03949_rcc_Normal_s33_cx1639_cy1331_244_244.pickle\n",
            "NORMAL/DBT-P00167_DBT-S03949_lmlo_Normal_s86_cx433_cy1069_244_244.pickle\n",
            "NORMAL/DBT-P00204_DBT-S02921_rcc_Normal_s35_cx1583_cy1322_244_244.pickle\n",
            "NORMAL/DBT-P00204_DBT-S02921_lmlo_Normal_s4_cx243_cy1050_244_244.pickle\n",
            "NORMAL/DBT-P00204_DBT-S02921_lcc_Normal_s42_cx211_cy1082_244_244.pickle\n",
            "NORMAL/DBT-P00204_DBT-S02921_rmlo_Normal_s14_cx1496_cy1136_244_244.pickle\n",
            "NORMAL/DBT-P00143_DBT-S00131_lmlo_Normal_s3_cx353_cy1264_244_244.pickle\n",
            "NORMAL/DBT-P00143_DBT-S00131_lcc_Normal_s60_cx347_cy1163_244_244.pickle\n",
            "NORMAL/DBT-P00143_DBT-S00131_rcc_Normal_s71_cx1555_cy1188_244_244.pickle\n",
            "NORMAL/DBT-P00143_DBT-S00131_rmlo_Normal_s12_cx1516_cy1141_244_244.pickle\n",
            "NORMAL/DBT-P00209_DBT-S03217_lmlo_Normal_s32_cx376_cy1098_244_244.pickle\n",
            "NORMAL/DBT-P00209_DBT-S03217_lcc_Normal_s23_cx354_cy1283_244_244.pickle\n",
            "NORMAL/DBT-P00209_DBT-S03217_rcc_Normal_s6_cx1485_cy1225_244_244.pickle\n",
            "NORMAL/DBT-P00209_DBT-S03217_rmlo_Normal_s12_cx1476_cy1205_244_244.pickle\n",
            "NORMAL/DBT-P00242_DBT-S04152_rcc_Normal_s10_cx1605_cy1203_244_244.pickle\n",
            "NORMAL/DBT-P00242_DBT-S04152_lcc_Normal_s17_cx445_cy1106_244_244.pickle\n",
            "NORMAL/DBT-P00242_DBT-S04152_lmlo_Normal_s42_cx455_cy1038_244_244.pickle\n",
            "NORMAL/DBT-P00242_DBT-S04152_rmlo_Normal_s10_cx1572_cy1021_244_244.pickle\n",
            "NORMAL/DBT-P00230_DBT-S04178_rcc_Normal_s22_cx1541_cy1186_244_244.pickle\n",
            "NORMAL/DBT-P00230_DBT-S04178_rmlo_Normal_s27_cx1481_cy1079_244_244.pickle\n",
            "NORMAL/DBT-P00230_DBT-S04178_lcc_Normal_s9_cx350_cy1237_244_244.pickle\n",
            "NORMAL/DBT-P00230_DBT-S04178_lmlo_Normal_s6_cx393_cy1098_244_244.pickle\n",
            "NORMAL/DBT-P00229_DBT-S03606_rcc_Normal_s38_cx1376_cy1252_244_244.pickle\n",
            "NORMAL/DBT-P00229_DBT-S03606_rmlo_Normal_s56_cx1361_cy1223_244_244.pickle\n",
            "NORMAL/DBT-P00229_DBT-S03606_lmlo_Normal_s4_cx688_cy1174_244_244.pickle\n",
            "NORMAL/DBT-P00229_DBT-S03606_lcc_Normal_s15_cx693_cy1335_244_244.pickle\n",
            "NORMAL/DBT-P00218_DBT-S03713_lmlo_Normal_s19_cx398_cy1031_244_244.pickle\n",
            "NORMAL/DBT-P00218_DBT-S03713_lcc_Normal_s42_cx392_cy1377_244_244.pickle\n",
            "NORMAL/DBT-P00218_DBT-S03713_rmlo_Normal_s45_cx1561_cy1015_244_244.pickle\n",
            "NORMAL/DBT-P00218_DBT-S03713_rcc_Normal_s47_cx1639_cy1222_244_244.pickle\n",
            "NORMAL/DBT-P00285_DBT-S04373_rcc_Normal_s59_cx1563_cy1131_244_244.pickle\n",
            "NORMAL/DBT-P00285_DBT-S04373_lmlo_Normal_s25_cx393_cy1068_244_244.pickle\n",
            "NORMAL/DBT-P00285_DBT-S04373_lcc_Normal_s1_cx391_cy1292_244_244.pickle\n",
            "NORMAL/DBT-P00285_DBT-S04373_rmlo_Normal_s24_cx1530_cy1069_244_244.pickle\n",
            "NORMAL/DBT-P00349_DBT-S04242_rmlo_Normal_s31_cx1470_cy1153_244_244.pickle\n",
            "NORMAL/DBT-P00349_DBT-S04242_lmlo_Normal_s53_cx523_cy1074_244_244.pickle\n",
            "NORMAL/DBT-P00349_DBT-S04242_lcc_Normal_s73_cx472_cy1285_244_244.pickle\n",
            "NORMAL/DBT-P00349_DBT-S04242_rcc_Normal_s17_cx1507_cy1382_244_244.pickle\n",
            "NORMAL/DBT-P00421_DBT-S01429_rcc_Normal_s50_cx1427_cy1170_244_244.pickle\n",
            "NORMAL/DBT-P00421_DBT-S01429_rmlo_Normal_s43_cx1375_cy1196_244_244.pickle\n",
            "NORMAL/DBT-P00421_DBT-S01429_lcc_Normal_s7_cx525_cy1136_244_244.pickle\n",
            "NORMAL/DBT-P00421_DBT-S01429_lmlo_Normal_s47_cx535_cy1154_244_244.pickle\n",
            "NORMAL/DBT-P00398_DBT-S02600_rcc_Normal_s43_cx1573_cy1133_244_244.pickle\n",
            "NORMAL/DBT-P00398_DBT-S02600_lcc_Normal_s27_cx342_cy1232_244_244.pickle\n",
            "NORMAL/DBT-P00398_DBT-S02600_lmlo_Normal_s47_cx402_cy1126_244_244.pickle\n",
            "NORMAL/DBT-P00398_DBT-S02600_rmlo_Normal_s37_cx1506_cy1115_244_244.pickle\n",
            "NORMAL/DBT-P00428_DBT-S05146_lmlo_Normal_s32_cx298_cy1229_244_244.pickle\n",
            "NORMAL/DBT-P00428_DBT-S05146_rcc_Normal_s34_cx1531_cy1159_244_244.pickle\n",
            "NORMAL/DBT-P00428_DBT-S05146_rmlo_Normal_s1_cx1594_cy1181_244_244.pickle\n",
            "NORMAL/DBT-P00428_DBT-S05146_lcc_Normal_s17_cx309_cy1400_244_244.pickle\n",
            "NORMAL/DBT-P00405_DBT-S03041_lcc_Normal_s58_cx356_cy1287_244_244.pickle\n",
            "NORMAL/DBT-P00405_DBT-S03041_rmlo_Normal_s53_cx1550_cy1062_244_244.pickle\n",
            "NORMAL/DBT-P00405_DBT-S03041_rcc_Normal_s80_cx1554_cy1296_244_244.pickle\n",
            "NORMAL/DBT-P00405_DBT-S03041_lmlo_Normal_s18_cx359_cy995_244_244.pickle\n",
            "NORMAL/DBT-P00502_DBT-S02550_lcc_Normal_s22_cx262_cy1233_244_244.pickle\n",
            "NORMAL/DBT-P00502_DBT-S02550_rmlo_Normal_s7_cx1591_cy873_244_244.pickle\n",
            "NORMAL/DBT-P00502_DBT-S02550_lmlo_Normal_s28_cx329_cy844_244_244.pickle\n",
            "NORMAL/DBT-P00502_DBT-S02550_rcc_Normal_s5_cx1624_cy1245_244_244.pickle\n",
            "NORMAL/DBT-P00444_DBT-S05529_rcc_Normal_s16_cx1521_cy1221_244_244.pickle\n",
            "NORMAL/DBT-P00444_DBT-S05529_rmlo_Normal_s70_cx1519_cy1090_244_244.pickle\n",
            "NORMAL/DBT-P00444_DBT-S05529_lcc_Normal_s2_cx536_cy1355_244_244.pickle\n",
            "NORMAL/DBT-P00444_DBT-S05529_lmlo_Normal_s50_cx470_cy1089_244_244.pickle\n",
            "NORMAL/DBT-P00465_DBT-S00464_rmlo_Normal_s37_cx1572_cy895_244_244.pickle\n",
            "NORMAL/DBT-P00465_DBT-S00464_rcc_Normal_s6_cx1646_cy1117_244_244.pickle\n",
            "NORMAL/DBT-P00465_DBT-S00464_lcc_Normal_s27_cx226_cy1259_244_244.pickle\n",
            "NORMAL/DBT-P00465_DBT-S00464_lmlo_Normal_s14_cx282_cy1004_244_244.pickle\n",
            "NORMAL/DBT-P00510_DBT-S03151_lcc_Normal_s36_cx246_cy1108_244_244.pickle\n",
            "NORMAL/DBT-P00510_DBT-S03151_lmlo_Normal_s53_cx323_cy1028_244_244.pickle\n",
            "NORMAL/DBT-P00510_DBT-S03151_rmlo_Normal_s29_cx1680_cy1085_244_244.pickle\n",
            "NORMAL/DBT-P00510_DBT-S03151_rcc_Normal_s39_cx1752_cy1295_244_244.pickle\n",
            "NORMAL/DBT-P00429_DBT-S04020_rcc_Normal_s12_cx1465_cy1327_244_244.pickle\n",
            "NORMAL/DBT-P00429_DBT-S04020_lmlo_Normal_s10_cx511_cy1210_244_244.pickle\n",
            "NORMAL/DBT-P00429_DBT-S04020_lcc_Normal_s69_cx469_cy1238_244_244.pickle\n",
            "NORMAL/DBT-P00429_DBT-S04020_rmlo_Normal_s88_cx1434_cy1283_244_244.pickle\n",
            "NORMAL/DBT-P00520_DBT-S00326_rmlo_Normal_s64_cx1527_cy979_244_244.pickle\n",
            "NORMAL/DBT-P00520_DBT-S00326_lmlo_Normal_s35_cx368_cy983_244_244.pickle\n",
            "NORMAL/DBT-P00520_DBT-S00326_rcc_Normal_s28_cx1623_cy1047_244_244.pickle\n",
            "NORMAL/DBT-P00520_DBT-S00326_lcc_Normal_s42_cx270_cy1099_244_244.pickle\n",
            "NORMAL/DBT-P00514_DBT-S00487_rcc_Normal_s43_cx1473_cy1190_244_244.pickle\n",
            "NORMAL/DBT-P00514_DBT-S00487_lmlo_Normal_s66_cx564_cy1151_244_244.pickle\n",
            "NORMAL/DBT-P00514_DBT-S00487_lcc_Normal_s66_cx551_cy1187_244_244.pickle\n",
            "NORMAL/DBT-P00514_DBT-S00487_rmlo_Normal_s24_cx1470_cy1134_244_244.pickle\n",
            "NORMAL/DBT-P00624_DBT-S05566_lcc_Normal_s6_cx407_cy1349_244_244.pickle\n",
            "NORMAL/DBT-P00624_DBT-S05566_rcc_Normal_s5_cx1440_cy1311_244_244.pickle\n",
            "NORMAL/DBT-P00624_DBT-S05566_lmlo_Normal_s1_cx397_cy1244_244_244.pickle\n",
            "NORMAL/DBT-P00624_DBT-S05566_rmlo_Normal_s25_cx1451_cy1220_244_244.pickle\n",
            "NORMAL/DBT-P00595_DBT-S02136_lmlo_Normal_s11_cx402_cy1089_244_244.pickle\n",
            "NORMAL/DBT-P00595_DBT-S02136_rcc_Normal_s9_cx1582_cy1206_244_244.pickle\n",
            "NORMAL/DBT-P00595_DBT-S02136_lcc_Normal_s14_cx326_cy1251_244_244.pickle\n",
            "NORMAL/DBT-P00595_DBT-S02136_rmlo_Normal_s9_cx1515_cy1037_244_244.pickle\n",
            "NORMAL/DBT-P00617_DBT-S05011_lcc_Normal_s53_cx577_cy1413_244_244.pickle\n",
            "NORMAL/DBT-P00617_DBT-S05011_rcc_Normal_s67_cx1453_cy1376_244_244.pickle\n",
            "NORMAL/DBT-P00617_DBT-S05011_rmlo_Normal_s64_cx1394_cy1131_244_244.pickle\n",
            "NORMAL/DBT-P00617_DBT-S05011_lmlo_Normal_s65_cx617_cy1087_244_244.pickle\n",
            "NORMAL/DBT-P00610_DBT-S05162_lcc_Normal_s49_cx334_cy1348_244_244.pickle\n",
            "NORMAL/DBT-P00610_DBT-S05162_rmlo_Normal_s19_cx1551_cy1036_244_244.pickle\n",
            "NORMAL/DBT-P00610_DBT-S05162_lmlo_Normal_s13_cx378_cy1035_244_244.pickle\n",
            "NORMAL/DBT-P00610_DBT-S05162_rcc_Normal_s32_cx1571_cy1332_244_244.pickle\n",
            "NORMAL/DBT-P00523_DBT-S05217_rcc_Normal_s24_cx1464_cy1319_244_244.pickle\n",
            "NORMAL/DBT-P00523_DBT-S05217_lcc_Normal_s5_cx529_cy1255_244_244.pickle\n",
            "NORMAL/DBT-P00523_DBT-S05217_rmlo_Normal_s21_cx1440_cy1177_244_244.pickle\n",
            "NORMAL/DBT-P00523_DBT-S05217_lmlo_Normal_s48_cx540_cy1217_244_244.pickle\n",
            "NORMAL/DBT-P00531_DBT-S02058_rmlo_Normal_s45_cx1342_cy1323_244_244.pickle\n",
            "NORMAL/DBT-P00531_DBT-S02058_lcc_Normal_s2_cx661_cy1165_244_244.pickle\n",
            "NORMAL/DBT-P00531_DBT-S02058_lmlo_Normal_s6_cx650_cy1149_244_244.pickle\n",
            "NORMAL/DBT-P00531_DBT-S02058_rcc_Normal_s25_cx1303_cy1338_244_244.pickle\n",
            "NORMAL/DBT-P00552_DBT-S04412_rcc_Normal_s38_cx1514_cy1179_244_244.pickle\n",
            "NORMAL/DBT-P00552_DBT-S04412_rmlo_Normal_s35_cx1444_cy1160_244_244.pickle\n",
            "NORMAL/DBT-P00552_DBT-S04412_lmlo_Normal_s37_cx482_cy1155_244_244.pickle\n",
            "NORMAL/DBT-P00552_DBT-S04412_lcc_Normal_s13_cx443_cy1158_244_244.pickle\n",
            "NORMAL/DBT-P00562_DBT-S01683_lcc_Normal_s50_cx393_cy1169_244_244.pickle\n",
            "NORMAL/DBT-P00562_DBT-S01683_rcc_Normal_s55_cx1517_cy1295_244_244.pickle\n",
            "NORMAL/DBT-P00562_DBT-S01683_rmlo_Normal_s5_cx1461_cy1077_244_244.pickle\n",
            "NORMAL/DBT-P00562_DBT-S01683_lmlo_Normal_s7_cx466_cy1123_244_244.pickle\n",
            "NORMAL/DBT-P00632_DBT-S02092_rcc_Normal_s37_cx1554_cy1197_244_244.pickle\n",
            "NORMAL/DBT-P00632_DBT-S02092_lcc_Normal_s20_cx324_cy1238_244_244.pickle\n",
            "NORMAL/DBT-P00632_DBT-S02092_rmlo_Normal_s26_cx1508_cy957_244_244.pickle\n",
            "NORMAL/DBT-P00632_DBT-S02092_lmlo_Normal_s9_cx377_cy1029_244_244.pickle\n",
            "NORMAL/DBT-P00296_DBT-S03031_lmlo_Normal_s18_cx537_cy1075_244_244.pickle\n",
            "NORMAL/DBT-P00296_DBT-S03031_rmlo_Normal_s31_cx1431_cy1163_244_244.pickle\n",
            "NORMAL/DBT-P00296_DBT-S03031_lcc_Normal_s20_cx468_cy1136_244_244.pickle\n",
            "NORMAL/DBT-P00296_DBT-S03031_rcc_Normal_s35_cx1469_cy1324_244_244.pickle\n",
            "NORMAL/DBT-P00672_DBT-S01663_rcc_Normal_s55_cx1639_cy1324_244_244.pickle\n",
            "NORMAL/DBT-P00672_DBT-S01663_lcc_Normal_s19_cx363_cy1177_244_244.pickle\n",
            "NORMAL/DBT-P00672_DBT-S01663_rmlo_Normal_s79_cx1573_cy1086_244_244.pickle\n",
            "NORMAL/DBT-P00672_DBT-S01663_lmlo_Normal_s73_cx481_cy1080_244_244.pickle\n",
            "NORMAL/DBT-P00140_DBT-S00083_rmlo_Normal_s33_cx1462_cy1065_244_244.pickle\n",
            "NORMAL/DBT-P00140_DBT-S00083_lmlo_Normal_s20_cx436_cy1150_244_244.pickle\n",
            "NORMAL/DBT-P00140_DBT-S00083_lcc_Normal_s13_cx391_cy1191_244_244.pickle\n",
            "NORMAL/DBT-P00140_DBT-S00083_rcc_Normal_s50_cx1555_cy1195_244_244.pickle\n",
            "NORMAL/DBT-P00508_DBT-S04838_lcc_Normal_s21_cx501_cy1368_244_244.pickle\n",
            "NORMAL/DBT-P00508_DBT-S04838_rcc_Normal_s43_cx1540_cy1295_244_244.pickle\n",
            "NORMAL/DBT-P00508_DBT-S04838_lmlo_Normal_s4_cx489_cy1198_244_244.pickle\n",
            "NORMAL/DBT-P00508_DBT-S04838_rmlo_Normal_s1_cx1478_cy1205_244_244.pickle\n",
            "NORMAL/DBT-P00681_DBT-S02311_lmlo_Normal_s12_cx346_cy1036_244_244.pickle\n",
            "NORMAL/DBT-P00681_DBT-S02311_rmlo_Normal_s39_cx1544_cy1148_244_244.pickle\n",
            "NORMAL/DBT-P00681_DBT-S02311_lcc_Normal_s29_cx294_cy1307_244_244.pickle\n",
            "NORMAL/DBT-P00681_DBT-S02311_rcc_Normal_s21_cx1619_cy1470_244_244.pickle\n",
            "NORMAL/DBT-P00954_DBT-S02976_rcc_Normal_s5_cx1575_cy1363_244_244.pickle\n",
            "NORMAL/DBT-P00954_DBT-S02976_lmlo_Normal_s17_cx422_cy1048_244_244.pickle\n",
            "NORMAL/DBT-P00954_DBT-S02976_rmlo_Normal_s37_cx1548_cy1082_244_244.pickle\n",
            "NORMAL/DBT-P00954_DBT-S02976_lcc_Normal_s18_cx405_cy1364_244_244.pickle\n",
            "NORMAL/DBT-P01254_DBT-S03416_lmlo_Normal_s23_cx380_cy1178_244_244.pickle\n",
            "NORMAL/DBT-P01254_DBT-S03416_lcc_Normal_s54_cx338_cy1230_244_244.pickle\n",
            "NORMAL/DBT-P01254_DBT-S03416_rcc_Normal_s24_cx1491_cy1092_244_244.pickle\n",
            "NORMAL/DBT-P01254_DBT-S03416_rmlo_Normal_s37_cx1493_cy1118_244_244.pickle\n",
            "NORMAL/DBT-P02022_DBT-S00916_lmlo_Normal_s15_cx483_cy1274_244_244.pickle\n",
            "NORMAL/DBT-P02022_DBT-S00916_rcc_Normal_s59_cx1523_cy1387_244_244.pickle\n",
            "NORMAL/DBT-P02022_DBT-S00916_rmlo_Normal_s73_cx1465_cy1232_244_244.pickle\n",
            "NORMAL/DBT-P02022_DBT-S00916_lcc_Normal_s82_cx439_cy1188_244_244.pickle\n",
            "NORMAL/DBT-P01632_DBT-S04656_lmlo_Normal_s36_cx396_cy1059_244_244.pickle\n",
            "NORMAL/DBT-P01632_DBT-S04656_rmlo_Normal_s53_cx1529_cy942_244_244.pickle\n",
            "NORMAL/DBT-P01632_DBT-S04656_rcc_Normal_s64_cx1605_cy1027_244_244.pickle\n",
            "NORMAL/DBT-P01632_DBT-S04656_lcc_Normal_s17_cx312_cy1195_244_244.pickle\n",
            "NORMAL/DBT-P02889_DBT-S03550_lmlo_Normal_s29_cx445_cy1078_244_244.pickle\n",
            "NORMAL/DBT-P02889_DBT-S03550_rmlo_Normal_s39_cx1524_cy1049_244_244.pickle\n",
            "NORMAL/DBT-P02889_DBT-S03550_lcc_Normal_s25_cx398_cy1317_244_244.pickle\n",
            "NORMAL/DBT-P02889_DBT-S03550_rcc_Normal_s22_cx1581_cy1292_244_244.pickle\n",
            "NORMAL/DBT-P03556_DBT-S00022_lmlo_Normal_s7_cx463_cy1220_244_244.pickle\n",
            "NORMAL/DBT-P03556_DBT-S00022_rmlo_Normal_s15_cx1431_cy1176_244_244.pickle\n",
            "NORMAL/DBT-P03556_DBT-S00022_lcc_Normal_s29_cx410_cy1418_244_244.pickle\n",
            "NORMAL/DBT-P03556_DBT-S00022_rcc_Normal_s70_cx1508_cy1428_244_244.pickle\n",
            "NORMAL/DBT-P02297_DBT-S04592_lcc_Normal_s31_cx408_cy1206_244_244.pickle\n",
            "NORMAL/DBT-P02297_DBT-S04592_rcc_Normal_s9_cx1592_cy1282_244_244.pickle\n",
            "NORMAL/DBT-P02297_DBT-S04592_lmlo_Normal_s33_cx442_cy1120_244_244.pickle\n",
            "NORMAL/DBT-P02297_DBT-S04592_rmlo_Normal_s27_cx1518_cy1093_244_244.pickle\n",
            "NORMAL/DBT-P02451_DBT-S03130_lcc_Normal_s29_cx419_cy1347_244_244.pickle\n",
            "NORMAL/DBT-P02451_DBT-S03130_lmlo_Normal_s47_cx438_cy1113_244_244.pickle\n",
            "NORMAL/DBT-P02451_DBT-S03130_rcc_Normal_s34_cx1466_cy1255_244_244.pickle\n",
            "NORMAL/DBT-P02451_DBT-S03130_rmlo_Normal_s22_cx1470_cy1169_244_244.pickle\n",
            "NORMAL/DBT-P02773_DBT-S00544_rcc_Normal_s67_cx1489_cy1252_244_244.pickle\n",
            "NORMAL/DBT-P02773_DBT-S00544_rmlo_Normal_s52_cx1425_cy1164_244_244.pickle\n",
            "NORMAL/DBT-P02773_DBT-S00544_lcc_Normal_s80_cx516_cy1238_244_244.pickle\n",
            "NORMAL/DBT-P02773_DBT-S00544_lmlo_Normal_s26_cx575_cy1100_244_244.pickle\n",
            "NORMAL/DBT-P03408_DBT-S03470_rcc_Normal_s65_cx1465_cy1281_244_244.pickle\n",
            "NORMAL/DBT-P03408_DBT-S03470_lmlo_Normal_s3_cx516_cy1089_244_244.pickle\n",
            "NORMAL/DBT-P03408_DBT-S03470_rmlo_Normal_s47_cx1441_cy1099_244_244.pickle\n",
            "NORMAL/DBT-P03408_DBT-S03470_lcc_Normal_s65_cx514_cy1296_244_244.pickle\n",
            "NORMAL/DBT-P03913_DBT-S03132_lmlo_Normal_s16_cx441_cy1043_244_244.pickle\n",
            "NORMAL/DBT-P03913_DBT-S03132_rcc_Normal_s45_cx1603_cy1269_244_244.pickle\n",
            "NORMAL/DBT-P03913_DBT-S03132_rmlo_Normal_s27_cx1576_cy1074_244_244.pickle\n",
            "NORMAL/DBT-P03913_DBT-S03132_lcc_Normal_s51_cx359_cy1238_244_244.pickle\n",
            "NORMAL/DBT-P04688_DBT-S03825_lmlo_Normal_s46_cx426_cy1044_244_244.pickle\n",
            "NORMAL/DBT-P04688_DBT-S03825_rcc_Normal_s42_cx1504_cy1252_244_244.pickle\n",
            "NORMAL/DBT-P04688_DBT-S03825_rmlo_Normal_s54_cx1446_cy1038_244_244.pickle\n",
            "NORMAL/DBT-P04688_DBT-S03825_lcc_Normal_s3_cx326_cy1234_244_244.pickle\n",
            "NORMAL/DBT-P04058_DBT-S04408_rcc_Normal_s25_cx1522_cy1339_244_244.pickle\n",
            "NORMAL/DBT-P04058_DBT-S04408_lcc_Normal_s45_cx379_cy1210_244_244.pickle\n",
            "NORMAL/DBT-P04058_DBT-S04408_rmlo_Normal_s25_cx1435_cy1137_244_244.pickle\n",
            "NORMAL/DBT-P04058_DBT-S04408_lmlo_Normal_s32_cx457_cy1080_244_244.pickle\n",
            "NORMAL/DBT-P04578_DBT-S01641_lcc_Normal_s57_cx408_cy1268_244_244.pickle\n",
            "NORMAL/DBT-P04578_DBT-S01641_rcc_Normal_s31_cx1633_cy1293_244_244.pickle\n",
            "NORMAL/DBT-P04578_DBT-S01641_rmlo_Normal_s32_cx1585_cy1116_244_244.pickle\n",
            "NORMAL/DBT-P04578_DBT-S01641_lmlo_Normal_s71_cx464_cy1117_244_244.pickle\n",
            "NORMAL/DBT-P04426_DBT-S04494_rcc_Normal_s9_cx1529_cy1183_244_244.pickle\n",
            "NORMAL/DBT-P04426_DBT-S04494_rmlo_Normal_s39_cx1472_cy1046_244_244.pickle\n",
            "NORMAL/DBT-P04426_DBT-S04494_lcc_Normal_s29_cx477_cy1247_244_244.pickle\n",
            "NORMAL/DBT-P04426_DBT-S04494_lmlo_Normal_s49_cx498_cy1041_244_244.pickle\n",
            "NORMAL/DBT-P04797_DBT-S01116_rcc_Normal_s12_cx1562_cy1390_244_244.pickle\n",
            "NORMAL/DBT-P04797_DBT-S01116_rmlo_Normal_s51_cx1500_cy1033_244_244.pickle\n",
            "NORMAL/DBT-P04797_DBT-S01116_lmlo_Normal_s57_cx428_cy1041_244_244.pickle\n",
            "NORMAL/DBT-P04797_DBT-S01116_lcc_Normal_s37_cx370_cy1288_244_244.pickle\n",
            "NORMAL/DBT-P00737_DBT-S02721_lcc_Normal_s28_cx552_cy1345_244_244.pickle\n",
            "NORMAL/DBT-P00737_DBT-S02721_rcc_Normal_s66_cx1423_cy1324_244_244.pickle\n",
            "NORMAL/DBT-P00737_DBT-S02721_lmlo_Normal_s56_cx592_cy1079_244_244.pickle\n",
            "NORMAL/DBT-P00737_DBT-S02721_rmlo_Normal_s8_cx1337_cy1154_244_244.pickle\n",
            "NORMAL/DBT-P00751_DBT-S02449_lcc_Normal_s29_cx238_cy1187_244_244.pickle\n",
            "NORMAL/DBT-P00751_DBT-S02449_lmlo_Normal_s52_cx290_cy895_244_244.pickle\n",
            "NORMAL/DBT-P00751_DBT-S02449_rcc_Normal_s26_cx1733_cy1240_244_244.pickle\n",
            "NORMAL/DBT-P00751_DBT-S02449_rmlo_Normal_s44_cx1654_cy943_244_244.pickle\n",
            "NORMAL/DBT-P00775_DBT-S03991_lmlo_Normal_s25_cx440_cy1005_244_244.pickle\n",
            "NORMAL/DBT-P00775_DBT-S03991_rmlo_Normal_s54_cx1528_cy1029_244_244.pickle\n",
            "NORMAL/DBT-P00775_DBT-S03991_rcc_Normal_s73_cx1639_cy1435_244_244.pickle\n",
            "NORMAL/DBT-P00775_DBT-S03991_lcc_Normal_s54_cx359_cy1261_244_244.pickle\n",
            "NORMAL/DBT-P00722_DBT-S03947_lmlo_Normal_s27_cx380_cy1039_244_244.pickle\n",
            "NORMAL/DBT-P00722_DBT-S03947_rmlo_Normal_s12_cx1534_cy1094_244_244.pickle\n",
            "NORMAL/DBT-P00722_DBT-S03947_rcc_Normal_s39_cx1551_cy1256_244_244.pickle\n",
            "NORMAL/DBT-P00722_DBT-S03947_lcc_Normal_s15_cx375_cy1180_244_244.pickle\n",
            "NORMAL/DBT-P00781_DBT-S00758_lcc_Normal_s30_cx362_cy1310_244_244.pickle\n",
            "NORMAL/DBT-P00781_DBT-S00758_rcc_Normal_s22_cx1668_cy1314_244_244.pickle\n",
            "NORMAL/DBT-P00781_DBT-S00758_rmlo_Normal_s24_cx1611_cy940_244_244.pickle\n",
            "NORMAL/DBT-P00781_DBT-S00758_lmlo_Normal_s30_cx399_cy922_244_244.pickle\n",
            "NORMAL/DBT-P00732_DBT-S04021_rmlo_Normal_s47_cx1575_cy1077_244_244.pickle\n",
            "NORMAL/DBT-P00732_DBT-S04021_lmlo_Normal_s41_cx352_cy1221_244_244.pickle\n",
            "NORMAL/DBT-P00732_DBT-S04021_lcc_Normal_s32_cx372_cy1272_244_244.pickle\n",
            "NORMAL/DBT-P00732_DBT-S04021_rcc_Normal_s6_cx1585_cy1294_244_244.pickle\n",
            "NORMAL/DBT-P00733_DBT-S01044_lmlo_Normal_s27_cx527_cy1077_244_244.pickle\n",
            "NORMAL/DBT-P00733_DBT-S01044_lcc_Normal_s60_cx484_cy1214_244_244.pickle\n",
            "NORMAL/DBT-P00733_DBT-S01044_rmlo_Normal_s32_cx1492_cy1197_244_244.pickle\n",
            "NORMAL/DBT-P00733_DBT-S01044_rcc_Normal_s45_cx1480_cy1347_244_244.pickle\n",
            "NORMAL/DBT-P00795_DBT-S03926_rcc_Normal_s22_cx1724_cy1295_244_244.pickle\n",
            "NORMAL/DBT-P00795_DBT-S03926_lcc_Normal_s48_cx172_cy1156_244_244.pickle\n",
            "NORMAL/DBT-P00795_DBT-S03926_rmlo_Normal_s21_cx1627_cy1036_244_244.pickle\n",
            "NORMAL/DBT-P00795_DBT-S03926_lmlo_Normal_s15_cx282_cy966_244_244.pickle\n",
            "NORMAL/DBT-P00810_DBT-S04314_lcc_Normal_s47_cx353_cy1342_244_244.pickle\n",
            "NORMAL/DBT-P00810_DBT-S04314_rcc_Normal_s62_cx1561_cy1435_244_244.pickle\n",
            "NORMAL/DBT-P00810_DBT-S04314_rmlo_Normal_s56_cx1511_cy1059_244_244.pickle\n",
            "NORMAL/DBT-P00810_DBT-S04314_lmlo_Normal_s75_cx424_cy959_244_244.pickle\n",
            "NORMAL/DBT-P00806_DBT-S00262_rcc_Normal_s38_cx1749_cy1185_244_244.pickle\n",
            "NORMAL/DBT-P00806_DBT-S00262_rmlo_Normal_s46_cx1752_cy971_244_244.pickle\n",
            "NORMAL/DBT-P00806_DBT-S00262_lcc_Normal_s24_cx368_cy1131_244_244.pickle\n",
            "NORMAL/DBT-P00806_DBT-S00262_lmlo_Normal_s49_cx375_cy1078_244_244.pickle\n",
            "NORMAL/DBT-P00845_DBT-S05135_lcc_Normal_s22_cx328_cy1271_244_244.pickle\n",
            "NORMAL/DBT-P00845_DBT-S05135_rcc_Normal_s36_cx1559_cy1231_244_244.pickle\n",
            "NORMAL/DBT-P00845_DBT-S05135_lmlo_Normal_s38_cx373_cy1023_244_244.pickle\n",
            "NORMAL/DBT-P00845_DBT-S05135_rmlo_Normal_s10_cx1492_cy1031_244_244.pickle\n",
            "NORMAL/DBT-P00897_DBT-S02252_rmlo_Normal_s40_cx1366_cy1105_244_244.pickle\n",
            "NORMAL/DBT-P00897_DBT-S02252_rcc_Normal_s62_cx1495_cy1348_244_244.pickle\n",
            "NORMAL/DBT-P00897_DBT-S02252_lmlo_Normal_s17_cx462_cy1114_244_244.pickle\n",
            "NORMAL/DBT-P00897_DBT-S02252_lcc_Normal_s54_cx385_cy1290_244_244.pickle\n",
            "NORMAL/DBT-P00887_DBT-S00135_lmlo_Normal_s20_cx462_cy1074_244_244.pickle\n",
            "NORMAL/DBT-P00887_DBT-S00135_rcc_Normal_s42_cx1632_cy1201_244_244.pickle\n",
            "NORMAL/DBT-P00887_DBT-S00135_rmlo_Normal_s29_cx1503_cy949_244_244.pickle\n",
            "NORMAL/DBT-P00887_DBT-S00135_lcc_Normal_s20_cx415_cy1317_244_244.pickle\n",
            "NORMAL/DBT-P00889_DBT-S03339_rmlo_Normal_s51_cx1470_cy1270_244_244.pickle\n",
            "NORMAL/DBT-P00889_DBT-S03339_lcc_Normal_s41_cx371_cy1232_244_244.pickle\n",
            "NORMAL/DBT-P00889_DBT-S03339_rcc_Normal_s46_cx1487_cy1257_244_244.pickle\n",
            "NORMAL/DBT-P00889_DBT-S03339_lmlo_Normal_s5_cx400_cy1172_244_244.pickle\n",
            "NORMAL/DBT-P00860_DBT-S02000_lcc_Normal_s36_cx430_cy1243_244_244.pickle\n",
            "NORMAL/DBT-P00860_DBT-S02000_rmlo_Normal_s50_cx1514_cy1142_244_244.pickle\n",
            "NORMAL/DBT-P00860_DBT-S02000_rcc_Normal_s53_cx1539_cy1164_244_244.pickle\n",
            "NORMAL/DBT-P00860_DBT-S02000_lmlo_Normal_s25_cx470_cy1007_244_244.pickle\n",
            "NORMAL/DBT-P00853_DBT-S03835_rcc_Normal_s15_cx1625_cy1174_244_244.pickle\n",
            "NORMAL/DBT-P00853_DBT-S03835_lmlo_Normal_s31_cx399_cy1065_244_244.pickle\n",
            "NORMAL/DBT-P00853_DBT-S03835_rmlo_Normal_s21_cx1596_cy963_244_244.pickle\n",
            "NORMAL/DBT-P00853_DBT-S03835_lcc_Normal_s30_cx394_cy1259_244_244.pickle\n",
            "NORMAL/DBT-P00881_DBT-S03989_lmlo_Normal_s38_cx294_cy990_244_244.pickle\n",
            "NORMAL/DBT-P00881_DBT-S03989_rmlo_Normal_s22_cx1653_cy1026_244_244.pickle\n",
            "NORMAL/DBT-P00881_DBT-S03989_rcc_Normal_s5_cx1634_cy1229_244_244.pickle\n",
            "NORMAL/DBT-P00881_DBT-S03989_lcc_Normal_s41_cx274_cy1421_244_244.pickle\n",
            "NORMAL/DBT-P00943_DBT-S01502_rmlo_Normal_s16_cx1546_cy956_244_244.pickle\n",
            "NORMAL/DBT-P00943_DBT-S01502_rcc_Normal_s70_cx1623_cy1249_244_244.pickle\n",
            "NORMAL/DBT-P00943_DBT-S01502_lcc_Normal_s12_cx389_cy1280_244_244.pickle\n",
            "NORMAL/DBT-P00943_DBT-S01502_lmlo_Normal_s32_cx414_cy983_244_244.pickle\n",
            "NORMAL/DBT-P00930_DBT-S02022_rmlo_Normal_s25_cx1607_cy1093_244_244.pickle\n",
            "NORMAL/DBT-P00930_DBT-S02022_rcc_Normal_s47_cx1641_cy1311_244_244.pickle\n",
            "NORMAL/DBT-P00930_DBT-S02022_lcc_Normal_s58_cx388_cy1269_244_244.pickle\n",
            "NORMAL/DBT-P00930_DBT-S02022_lmlo_Normal_s27_cx439_cy1128_244_244.pickle\n",
            "NORMAL/DBT-P00962_DBT-S04375_lcc_Normal_s68_cx332_cy1189_244_244.pickle\n",
            "NORMAL/DBT-P00962_DBT-S04375_rmlo_Normal_s52_cx1442_cy1196_244_244.pickle\n",
            "NORMAL/DBT-P00962_DBT-S04375_lmlo_Normal_s45_cx481_cy1138_244_244.pickle\n",
            "NORMAL/DBT-P00962_DBT-S04375_rcc_Normal_s14_cx1535_cy1302_244_244.pickle\n",
            "NORMAL/DBT-P00949_DBT-S01343_rmlo_Normal_s59_cx1548_cy1050_244_244.pickle\n",
            "NORMAL/DBT-P00949_DBT-S01343_lcc_Normal_s51_cx400_cy1245_244_244.pickle\n",
            "NORMAL/DBT-P00949_DBT-S01343_lmlo_Normal_s29_cx436_cy1052_244_244.pickle\n",
            "NORMAL/DBT-P00949_DBT-S01343_rcc_Normal_s47_cx1610_cy1300_244_244.pickle\n",
            "NORMAL/DBT-P00964_DBT-S03903_rmlo_Normal_s67_cx1556_cy1018_244_244.pickle\n",
            "NORMAL/DBT-P00964_DBT-S03903_lmlo_Normal_s38_cx428_cy1067_244_244.pickle\n",
            "NORMAL/DBT-P00964_DBT-S03903_rcc_Normal_s48_cx1607_cy1216_244_244.pickle\n",
            "NORMAL/DBT-P00964_DBT-S03903_lcc_Normal_s56_cx366_cy1199_244_244.pickle\n",
            "NORMAL/DBT-P00993_DBT-S04812_lcc_Normal_s23_cx457_cy1170_244_244.pickle\n",
            "NORMAL/DBT-P00993_DBT-S04812_rcc_Normal_s36_cx1490_cy1212_244_244.pickle\n",
            "NORMAL/DBT-P00993_DBT-S04812_rmlo_Normal_s30_cx1463_cy1144_244_244.pickle\n",
            "NORMAL/DBT-P00993_DBT-S04812_lmlo_Normal_s43_cx472_cy1190_244_244.pickle\n",
            "NORMAL/DBT-P01007_DBT-S03475_rmlo_Normal_s74_cx1397_cy1203_244_244.pickle\n",
            "NORMAL/DBT-P03324_DBT-S03541_rcc_Normal_s16_cx1260_cy1239_244_244.pickle\n",
            "NORMAL/DBT-P03324_DBT-S03541_lcc_Normal_s62_cx706_cy1186_244_244.pickle\n",
            "NORMAL/DBT-P03392_DBT-S02032_lcc_Normal_s33_cx279_cy1251_244_244.pickle\n",
            "NORMAL/DBT-P03392_DBT-S02032_lmlo_Normal_s13_cx324_cy979_244_244.pickle\n",
            "NORMAL/DBT-P03392_DBT-S02032_rmlo_Normal_s20_cx1587_cy1065_244_244.pickle\n",
            "NORMAL/DBT-P03392_DBT-S02032_rcc_Normal_s4_cx1625_cy1389_244_244.pickle\n",
            "NORMAL/DBT-P03306_DBT-S00360_lmlo_Normal_s37_cx395_cy1078_244_244.pickle\n",
            "NORMAL/DBT-P03306_DBT-S00360_lcc_Normal_s33_cx339_cy1283_244_244.pickle\n",
            "NORMAL/DBT-P03306_DBT-S00360_rcc_Normal_s11_cx1541_cy1360_244_244.pickle\n",
            "NORMAL/DBT-P03306_DBT-S00360_rmlo_Normal_s41_cx1465_cy1081_244_244.pickle\n",
            "NORMAL/DBT-P03400_DBT-S04732_rmlo_Normal_s42_cx1514_cy1171_244_244.pickle\n",
            "NORMAL/DBT-P03400_DBT-S04732_lcc_Normal_s5_cx375_cy1251_244_244.pickle\n",
            "NORMAL/DBT-P03400_DBT-S04732_lmlo_Normal_s47_cx406_cy1107_244_244.pickle\n",
            "NORMAL/DBT-P03400_DBT-S04732_rcc_Normal_s3_cx1549_cy1224_244_244.pickle\n",
            "NORMAL/DBT-P03406_DBT-S05192_rmlo_Normal_s8_cx1396_cy1128_244_244.pickle\n",
            "NORMAL/DBT-P03406_DBT-S05192_rcc_Normal_s60_cx1423_cy1336_244_244.pickle\n",
            "NORMAL/DBT-P03406_DBT-S05192_lmlo_Normal_s76_cx610_cy1073_244_244.pickle\n",
            "NORMAL/DBT-P03406_DBT-S05192_lcc_Normal_s76_cx522_cy1165_244_244.pickle\n",
            "NORMAL/DBT-P03433_DBT-S02951_lcc_Normal_s43_cx411_cy1237_244_244.pickle\n",
            "NORMAL/DBT-P03433_DBT-S02951_rmlo_Normal_s15_cx1537_cy932_244_244.pickle\n",
            "NORMAL/DBT-P03433_DBT-S02951_rcc_Normal_s40_cx1572_cy1362_244_244.pickle\n",
            "NORMAL/DBT-P03433_DBT-S02951_lmlo_Normal_s24_cx430_cy989_244_244.pickle\n",
            "NORMAL/DBT-P03440_DBT-S01541_rcc_Normal_s4_cx1638_cy1334_244_244.pickle\n",
            "NORMAL/DBT-P03440_DBT-S01541_lcc_Normal_s11_cx259_cy1153_244_244.pickle\n",
            "NORMAL/DBT-P03440_DBT-S01541_rmlo_Normal_s18_cx1590_cy919_244_244.pickle\n",
            "NORMAL/DBT-P03440_DBT-S01541_lmlo_Normal_s12_cx316_cy897_244_244.pickle\n",
            "NORMAL/DBT-P03474_DBT-S03465_lcc_Normal_s45_cx344_cy1430_244_244.pickle\n",
            "NORMAL/DBT-P03474_DBT-S03465_rcc_Normal_s67_cx1690_cy1367_244_244.pickle\n",
            "NORMAL/DBT-P03474_DBT-S03465_lmlo_Normal_s47_cx391_cy1062_244_244.pickle\n",
            "NORMAL/DBT-P03474_DBT-S03465_rmlo_Normal_s33_cx1621_cy918_244_244.pickle\n",
            "NORMAL/DBT-P03456_DBT-S04243_lmlo_Normal_s18_cx344_cy970_244_244.pickle\n",
            "NORMAL/DBT-P03456_DBT-S04243_rcc_Normal_s48_cx1667_cy1184_244_244.pickle\n",
            "NORMAL/DBT-P03456_DBT-S04243_lcc_Normal_s32_cx276_cy1143_244_244.pickle\n",
            "NORMAL/DBT-P03456_DBT-S04243_rmlo_Normal_s33_cx1629_cy1216_244_244.pickle\n",
            "NORMAL/DBT-P03567_DBT-S03648_rcc_Normal_s5_cx1620_cy1304_244_244.pickle\n",
            "NORMAL/DBT-P03567_DBT-S03648_lcc_Normal_s29_cx296_cy1305_244_244.pickle\n",
            "NORMAL/DBT-P03567_DBT-S03648_lmlo_Normal_s17_cx315_cy1177_244_244.pickle\n",
            "NORMAL/DBT-P03567_DBT-S03648_rmlo_Normal_s20_cx1579_cy1137_244_244.pickle\n",
            "NORMAL/DBT-P03509_DBT-S00110_lmlo_Normal_s35_cx480_cy1188_244_244.pickle\n",
            "NORMAL/DBT-P03509_DBT-S00110_rmlo_Normal_s47_cx1447_cy1238_244_244.pickle\n",
            "NORMAL/DBT-P03509_DBT-S00110_lcc_Normal_s14_cx492_cy1169_244_244.pickle\n",
            "NORMAL/DBT-P03509_DBT-S00110_rcc_Normal_s30_cx1415_cy1046_244_244.pickle\n",
            "NORMAL/DBT-P03520_DBT-S01825_rmlo_Normal_s63_cx1342_cy1218_244_244.pickle\n",
            "NORMAL/DBT-P03520_DBT-S01825_lcc_Normal_s21_cx494_cy1247_244_244.pickle\n",
            "NORMAL/DBT-P03520_DBT-S01825_lmlo_Normal_s47_cx626_cy1278_244_244.pickle\n",
            "NORMAL/DBT-P03520_DBT-S01825_rcc_Normal_s11_cx1555_cy1207_244_244.pickle\n",
            "NORMAL/DBT-P03515_DBT-S01492_lmlo_Normal_s53_cx437_cy1110_244_244.pickle\n",
            "NORMAL/DBT-P03515_DBT-S01492_rmlo_Normal_s30_cx1538_cy1096_244_244.pickle\n",
            "NORMAL/DBT-P03515_DBT-S01492_lcc_Normal_s56_cx411_cy1219_244_244.pickle\n",
            "NORMAL/DBT-P03515_DBT-S01492_rcc_Normal_s15_cx1557_cy1187_244_244.pickle\n",
            "NORMAL/DBT-P03477_DBT-S03406_rmlo_Normal_s6_cx1571_cy951_244_244.pickle\n",
            "NORMAL/DBT-P03477_DBT-S03406_rcc_Normal_s7_cx1609_cy1329_244_244.pickle\n",
            "NORMAL/DBT-P03477_DBT-S03406_lmlo_Normal_s39_cx431_cy1031_244_244.pickle\n",
            "NORMAL/DBT-P03477_DBT-S03406_lcc_Normal_s13_cx409_cy1010_244_244.pickle\n",
            "NORMAL/DBT-P03555_DBT-S01123_rmlo_Normal_s9_cx1516_cy1003_244_244.pickle\n",
            "NORMAL/DBT-P03555_DBT-S01123_rcc_Normal_s18_cx1599_cy1196_244_244.pickle\n",
            "NORMAL/DBT-P03555_DBT-S01123_lmlo_Normal_s34_cx323_cy1091_244_244.pickle\n",
            "NORMAL/DBT-P03555_DBT-S01123_lcc_Normal_s24_cx313_cy1436_244_244.pickle\n",
            "NORMAL/DBT-P03615_DBT-S05048_rcc_Normal_s26_cx1638_cy1069_244_244.pickle\n",
            "NORMAL/DBT-P03615_DBT-S05048_lcc_Normal_s16_cx282_cy1067_244_244.pickle\n",
            "NORMAL/DBT-P03615_DBT-S05048_lmlo_Normal_s20_cx337_cy1115_244_244.pickle\n",
            "NORMAL/DBT-P03615_DBT-S05048_rmlo_Normal_s25_cx1562_cy1005_244_244.pickle\n",
            "NORMAL/DBT-P03596_DBT-S01314_lmlo_Normal_s23_cx541_cy1100_244_244.pickle\n",
            "NORMAL/DBT-P03596_DBT-S01314_lcc_Normal_s3_cx510_cy1265_244_244.pickle\n",
            "NORMAL/DBT-P03596_DBT-S01314_rmlo_Normal_s56_cx1317_cy1078_244_244.pickle\n",
            "NORMAL/DBT-P03596_DBT-S01314_rcc_Normal_s43_cx1391_cy1304_244_244.pickle\n",
            "NORMAL/DBT-P03586_DBT-S04539_rmlo_Normal_s38_cx1547_cy1211_244_244.pickle\n",
            "NORMAL/DBT-P03586_DBT-S04539_lcc_Normal_s49_cx302_cy1157_244_244.pickle\n",
            "NORMAL/DBT-P03586_DBT-S04539_lmlo_Normal_s44_cx331_cy1201_244_244.pickle\n",
            "NORMAL/DBT-P03586_DBT-S04539_rcc_Normal_s14_cx1581_cy1294_244_244.pickle\n",
            "NORMAL/DBT-P03597_DBT-S03889_lmlo_Normal_s53_cx530_cy1126_244_244.pickle\n",
            "NORMAL/DBT-P03597_DBT-S03889_rcc_Normal_s45_cx1475_cy1131_244_244.pickle\n",
            "NORMAL/DBT-P03597_DBT-S03889_rmlo_Normal_s36_cx1371_cy1116_244_244.pickle\n",
            "NORMAL/DBT-P03597_DBT-S03889_lcc_Normal_s60_cx415_cy1201_244_244.pickle\n",
            "NORMAL/DBT-P03609_DBT-S05174_rmlo_Normal_s63_cx1503_cy1057_244_244.pickle\n",
            "NORMAL/DBT-P03609_DBT-S05174_rcc_Normal_s11_cx1554_cy1163_244_244.pickle\n",
            "NORMAL/DBT-P03609_DBT-S05174_lcc_Normal_s50_cx442_cy1141_244_244.pickle\n",
            "NORMAL/DBT-P03609_DBT-S05174_lmlo_Normal_s18_cx454_cy1033_244_244.pickle\n",
            "NORMAL/DBT-P03595_DBT-S01222_rmlo_Normal_s7_cx1471_cy1183_244_244.pickle\n",
            "NORMAL/DBT-P03595_DBT-S01222_lcc_Normal_s3_cx426_cy1267_244_244.pickle\n",
            "NORMAL/DBT-P03595_DBT-S01222_lmlo_Normal_s33_cx451_cy1130_244_244.pickle\n",
            "NORMAL/DBT-P03595_DBT-S01222_rcc_Normal_s16_cx1494_cy1316_244_244.pickle\n",
            "NORMAL/DBT-P03622_DBT-S03385_rcc_Normal_s5_cx1614_cy1184_244_244.pickle\n",
            "NORMAL/DBT-P03622_DBT-S03385_lcc_Normal_s9_cx360_cy1341_244_244.pickle\n",
            "NORMAL/DBT-P03622_DBT-S03385_lmlo_Normal_s9_cx417_cy988_244_244.pickle\n",
            "NORMAL/DBT-P03622_DBT-S03385_rmlo_Normal_s36_cx1545_cy860_244_244.pickle\n",
            "NORMAL/DBT-P03593_DBT-S01903_lmlo_Normal_s20_cx483_cy1162_244_244.pickle\n",
            "NORMAL/DBT-P03593_DBT-S01903_lcc_Normal_s17_cx382_cy1281_244_244.pickle\n",
            "NORMAL/DBT-P03593_DBT-S01903_rcc_Normal_s27_cx1538_cy1255_244_244.pickle\n",
            "NORMAL/DBT-P03593_DBT-S01903_rmlo_Normal_s47_cx1415_cy1101_244_244.pickle\n",
            "NORMAL/DBT-P03616_DBT-S05559_lmlo_Normal_s6_cx448_cy1052_244_244.pickle\n",
            "NORMAL/DBT-P03616_DBT-S05559_rcc_Normal_s9_cx1588_cy1296_244_244.pickle\n",
            "NORMAL/DBT-P03616_DBT-S05559_rmlo_Normal_s25_cx1540_cy1152_244_244.pickle\n",
            "NORMAL/DBT-P03616_DBT-S05559_lcc_Normal_s8_cx428_cy1200_244_244.pickle\n",
            "NORMAL/DBT-P03674_DBT-S02844_lmlo_Normal_s60_cx355_cy1068_244_244.pickle\n",
            "NORMAL/DBT-P03674_DBT-S02844_lcc_Normal_s68_cx312_cy1282_244_244.pickle\n",
            "NORMAL/DBT-P03674_DBT-S02844_rmlo_Normal_s45_cx1619_cy1042_244_244.pickle\n",
            "NORMAL/DBT-P03674_DBT-S02844_rcc_Normal_s70_cx1677_cy1164_244_244.pickle\n",
            "NORMAL/DBT-P03675_DBT-S02326_lmlo_Normal_s32_cx518_cy1028_244_244.pickle\n",
            "NORMAL/DBT-P03675_DBT-S02326_rcc_Normal_s25_cx1537_cy1122_244_244.pickle\n",
            "NORMAL/DBT-P03675_DBT-S02326_rmlo_Normal_s27_cx1384_cy1165_244_244.pickle\n",
            "NORMAL/DBT-P03675_DBT-S02326_lcc_Normal_s15_cx427_cy1251_244_244.pickle\n",
            "NORMAL/DBT-P03648_DBT-S02034_lmlo_Normal_s61_cx517_cy1028_244_244.pickle\n",
            "NORMAL/DBT-P03648_DBT-S02034_lcc_Normal_s31_cx500_cy1275_244_244.pickle\n",
            "NORMAL/DBT-P03648_DBT-S02034_rcc_Normal_s62_cx1499_cy1330_244_244.pickle\n",
            "NORMAL/DBT-P03648_DBT-S02034_rmlo_Normal_s65_cx1494_cy1067_244_244.pickle\n",
            "NORMAL/DBT-P03625_DBT-S03423_lcc_Normal_s10_cx362_cy1391_244_244.pickle\n",
            "NORMAL/DBT-P03625_DBT-S03423_rcc_Normal_s25_cx1493_cy1261_244_244.pickle\n",
            "NORMAL/DBT-P03625_DBT-S03423_lmlo_Normal_s8_cx439_cy1067_244_244.pickle\n",
            "NORMAL/DBT-P03625_DBT-S03423_rmlo_Normal_s69_cx1374_cy1050_244_244.pickle\n",
            "NORMAL/DBT-P03641_DBT-S02888_rcc_Normal_s33_cx1594_cy1344_244_244.pickle\n",
            "NORMAL/DBT-P03641_DBT-S02888_lcc_Normal_s48_cx284_cy1224_244_244.pickle\n",
            "NORMAL/DBT-P03641_DBT-S02888_rmlo_Normal_s14_cx1544_cy977_244_244.pickle\n",
            "NORMAL/DBT-P03641_DBT-S02888_lmlo_Normal_s46_cx322_cy1057_244_244.pickle\n",
            "NORMAL/DBT-P03739_DBT-S04493_rcc_Normal_s37_cx1663_cy1262_244_244.pickle\n",
            "NORMAL/DBT-P03739_DBT-S04493_lcc_Normal_s18_cx334_cy1237_244_244.pickle\n",
            "NORMAL/DBT-P03739_DBT-S04493_rmlo_Normal_s63_cx1606_cy945_244_244.pickle\n",
            "NORMAL/DBT-P03739_DBT-S04493_lmlo_Normal_s24_cx409_cy998_244_244.pickle\n",
            "NORMAL/DBT-P03738_DBT-S01578_lmlo_Normal_s11_cx493_cy1304_244_244.pickle\n",
            "NORMAL/DBT-P03738_DBT-S01578_rmlo_Normal_s46_cx1423_cy1254_244_244.pickle\n",
            "NORMAL/DBT-P03738_DBT-S01578_rcc_Normal_s63_cx1470_cy1218_244_244.pickle\n",
            "NORMAL/DBT-P03738_DBT-S01578_lcc_Normal_s12_cx442_cy1270_244_244.pickle\n",
            "NORMAL/DBT-P03765_DBT-S03386_rcc_Normal_s63_cx1578_cy1106_244_244.pickle\n",
            "NORMAL/DBT-P03765_DBT-S03386_lcc_Normal_s53_cx351_cy1115_244_244.pickle\n",
            "NORMAL/DBT-P03765_DBT-S03386_rmlo_Normal_s63_cx1500_cy1023_244_244.pickle\n",
            "NORMAL/DBT-P03765_DBT-S03386_lmlo_Normal_s57_cx389_cy1213_244_244.pickle\n",
            "NORMAL/DBT-P03777_DBT-S03883_lcc_Normal_s78_cx352_cy1226_244_244.pickle\n",
            "NORMAL/DBT-P03777_DBT-S03883_lmlo_Normal_s67_cx455_cy961_244_244.pickle\n",
            "NORMAL/DBT-P03777_DBT-S03883_rcc_Normal_s61_cx1624_cy1223_244_244.pickle\n",
            "NORMAL/DBT-P03777_DBT-S03883_rmlo_Normal_s32_cx1526_cy944_244_244.pickle\n",
            "NORMAL/DBT-P03815_DBT-S01206_rmlo_Normal_s49_cx1647_cy1028_244_244.pickle\n",
            "NORMAL/DBT-P03815_DBT-S01206_rcc_Normal_s15_cx1632_cy1255_244_244.pickle\n",
            "NORMAL/DBT-P03815_DBT-S01206_lmlo_Normal_s40_cx395_cy952_244_244.pickle\n",
            "NORMAL/DBT-P03815_DBT-S01206_lcc_Normal_s41_cx403_cy1219_244_244.pickle\n",
            "NORMAL/DBT-P03799_DBT-S01754_lmlo_Normal_s30_cx362_cy1088_244_244.pickle\n",
            "NORMAL/DBT-P03799_DBT-S01754_rmlo_Normal_s18_cx1552_cy1018_244_244.pickle\n",
            "NORMAL/DBT-P03799_DBT-S01754_rcc_Normal_s45_cx1620_cy1337_244_244.pickle\n",
            "NORMAL/DBT-P03799_DBT-S01754_lcc_Normal_s26_cx327_cy1255_244_244.pickle\n",
            "NORMAL/DBT-P03718_DBT-S02358_lmlo_Normal_s8_cx358_cy1050_244_244.pickle\n",
            "NORMAL/DBT-P03718_DBT-S02358_rcc_Normal_s3_cx1588_cy1235_244_244.pickle\n",
            "NORMAL/DBT-P03718_DBT-S02358_lcc_Normal_s1_cx288_cy1206_244_244.pickle\n",
            "NORMAL/DBT-P03718_DBT-S02358_rmlo_Normal_s14_cx1461_cy1015_244_244.pickle\n",
            "NORMAL/DBT-P03837_DBT-S03384_lmlo_Normal_s62_cx295_cy1412_244_244.pickle\n",
            "NORMAL/DBT-P03837_DBT-S03384_lcc_Normal_s28_cx300_cy1322_244_244.pickle\n",
            "NORMAL/DBT-P03837_DBT-S03384_rmlo_Normal_s68_cx1412_cy1041_244_244.pickle\n",
            "NORMAL/DBT-P03837_DBT-S03384_rcc_Normal_s4_cx1560_cy1328_244_244.pickle\n",
            "NORMAL/DBT-P03872_DBT-S03194_lmlo_Normal_s22_cx462_cy887_244_244.pickle\n",
            "NORMAL/DBT-P03872_DBT-S03194_rmlo_Normal_s52_cx1605_cy932_244_244.pickle\n",
            "NORMAL/DBT-P03872_DBT-S03194_rcc_Normal_s41_cx1599_cy1129_244_244.pickle\n",
            "NORMAL/DBT-P03872_DBT-S03194_lcc_Normal_s38_cx442_cy1179_244_244.pickle\n",
            "NORMAL/DBT-P03904_DBT-S04733_rcc_Normal_s50_cx1550_cy1306_244_244.pickle\n",
            "NORMAL/DBT-P03904_DBT-S04733_rmlo_Normal_s11_cx1510_cy1107_244_244.pickle\n",
            "NORMAL/DBT-P03904_DBT-S04733_lmlo_Normal_s12_cx529_cy1074_244_244.pickle\n",
            "NORMAL/DBT-P03904_DBT-S04733_lcc_Normal_s69_cx479_cy1228_244_244.pickle\n",
            "NORMAL/DBT-P03863_DBT-S03668_lmlo_Normal_s41_cx441_cy1117_244_244.pickle\n",
            "NORMAL/DBT-P03863_DBT-S03668_rmlo_Normal_s9_cx1605_cy1067_244_244.pickle\n",
            "NORMAL/DBT-P03863_DBT-S03668_rcc_Normal_s14_cx1590_cy1317_244_244.pickle\n",
            "NORMAL/DBT-P03863_DBT-S03668_lcc_Normal_s19_cx402_cy1277_244_244.pickle\n",
            "NORMAL/DBT-P03930_DBT-S01786_lcc_Normal_s26_cx383_cy1114_244_244.pickle\n",
            "NORMAL/DBT-P03930_DBT-S01786_rmlo_Normal_s48_cx1622_cy1038_244_244.pickle\n",
            "NORMAL/DBT-P03930_DBT-S01786_lmlo_Normal_s40_cx406_cy922_244_244.pickle\n",
            "NORMAL/DBT-P03930_DBT-S01786_rcc_Normal_s8_cx1626_cy1184_244_244.pickle\n",
            "NORMAL/DBT-P03848_DBT-S02193_lmlo_Normal_s13_cx556_cy1283_244_244.pickle\n",
            "NORMAL/DBT-P03848_DBT-S02193_rmlo_Normal_s47_cx1333_cy1115_244_244.pickle\n",
            "NORMAL/DBT-P03848_DBT-S02193_rcc_Normal_s61_cx1384_cy1245_244_244.pickle\n",
            "NORMAL/DBT-P03848_DBT-S02193_lcc_Normal_s16_cx520_cy1123_244_244.pickle\n",
            "NORMAL/DBT-P04023_DBT-S02601_lmlo_Normal_s56_cx685_cy1140_244_244.pickle\n",
            "NORMAL/DBT-P04023_DBT-S02601_rcc_Normal_s63_cx1339_cy1350_244_244.pickle\n",
            "NORMAL/DBT-P04023_DBT-S02601_lcc_Normal_s66_cx651_cy1271_244_244.pickle\n",
            "NORMAL/DBT-P04023_DBT-S02601_rmlo_Normal_s61_cx1303_cy1253_244_244.pickle\n",
            "NORMAL/DBT-P03981_DBT-S00726_lcc_Normal_s25_cx243_cy963_244_244.pickle\n",
            "NORMAL/DBT-P03981_DBT-S00726_rmlo_Normal_s29_cx1611_cy1054_244_244.pickle\n",
            "NORMAL/DBT-P03981_DBT-S00726_rcc_Normal_s15_cx1659_cy1065_244_244.pickle\n",
            "NORMAL/DBT-P03981_DBT-S00726_lmlo_Normal_s39_cx284_cy1056_244_244.pickle\n",
            "NORMAL/DBT-P04015_DBT-S04238_lmlo_Normal_s26_cx341_cy980_244_244.pickle\n",
            "NORMAL/DBT-P04015_DBT-S04238_lcc_Normal_s21_cx273_cy1222_244_244.pickle\n",
            "NORMAL/DBT-P04015_DBT-S04238_rmlo_Normal_s6_cx1549_cy911_244_244.pickle\n",
            "NORMAL/DBT-P04015_DBT-S04238_rcc_Normal_s23_cx1601_cy1197_244_244.pickle\n",
            "NORMAL/DBT-P03986_DBT-S04787_rmlo_Normal_s18_cx1549_cy1033_244_244.pickle\n",
            "NORMAL/DBT-P03986_DBT-S04787_lcc_Normal_s12_cx421_cy1319_244_244.pickle\n",
            "NORMAL/DBT-P03986_DBT-S04787_lmlo_Normal_s35_cx442_cy1053_244_244.pickle\n",
            "NORMAL/DBT-P03986_DBT-S04787_rcc_Normal_s3_cx1571_cy1276_244_244.pickle\n",
            "NORMAL/DBT-P03933_DBT-S05484_rcc_Normal_s26_cx1481_cy1200_244_244.pickle\n",
            "NORMAL/DBT-P03933_DBT-S05484_lcc_Normal_s20_cx379_cy1331_244_244.pickle\n",
            "NORMAL/DBT-P03933_DBT-S05484_rmlo_Normal_s18_cx1415_cy1124_244_244.pickle\n",
            "NORMAL/DBT-P03933_DBT-S05484_lmlo_Normal_s41_cx426_cy1062_244_244.pickle\n",
            "NORMAL/DBT-P04070_DBT-S02802_lcc_Normal_s42_cx398_cy1102_244_244.pickle\n",
            "NORMAL/DBT-P04070_DBT-S02802_lmlo_Normal_s45_cx452_cy1105_244_244.pickle\n",
            "NORMAL/DBT-P04070_DBT-S02802_rcc_Normal_s66_cx1510_cy1182_244_244.pickle\n",
            "NORMAL/DBT-P04070_DBT-S02802_rmlo_Normal_s22_cx1451_cy1124_244_244.pickle\n",
            "NORMAL/DBT-P04060_DBT-S02183_lcc_Normal_s5_cx504_cy1256_244_244.pickle\n",
            "NORMAL/DBT-P04060_DBT-S02183_rcc_Normal_s50_cx1467_cy1095_244_244.pickle\n",
            "NORMAL/DBT-P04060_DBT-S02183_lmlo_Normal_s66_cx535_cy1064_244_244.pickle\n",
            "NORMAL/DBT-P04060_DBT-S02183_rmlo_Normal_s56_cx1378_cy1171_244_244.pickle\n",
            "NORMAL/DBT-P04036_DBT-S01710_lmlo_Normal_s3_cx403_cy1129_244_244.pickle\n",
            "NORMAL/DBT-P04036_DBT-S01710_lcc_Normal_s32_cx371_cy1222_244_244.pickle\n",
            "NORMAL/DBT-P04036_DBT-S01710_rcc_Normal_s21_cx1539_cy1203_244_244.pickle\n",
            "NORMAL/DBT-P04036_DBT-S01710_rmlo_Normal_s31_cx1528_cy1095_244_244.pickle\n",
            "NORMAL/DBT-P04030_DBT-S03141_lmlo_Normal_s3_cx294_cy946_244_244.pickle\n",
            "NORMAL/DBT-P04030_DBT-S03141_rmlo_Normal_s39_cx1602_cy1016_244_244.pickle\n",
            "NORMAL/DBT-P04030_DBT-S03141_lcc_Normal_s30_cx250_cy1263_244_244.pickle\n",
            "NORMAL/DBT-P04030_DBT-S03141_rcc_Normal_s39_cx1654_cy1201_244_244.pickle\n",
            "NORMAL/DBT-P04029_DBT-S03485_rcc_Normal_s11_cx1490_cy1045_244_244.pickle\n",
            "NORMAL/DBT-P04029_DBT-S03485_lcc_Normal_s45_cx374_cy1118_244_244.pickle\n",
            "NORMAL/DBT-P04029_DBT-S03485_lmlo_Normal_s18_cx406_cy1206_244_244.pickle\n",
            "NORMAL/DBT-P04029_DBT-S03485_rmlo_Normal_s32_cx1491_cy1088_244_244.pickle\n",
            "NORMAL/DBT-P04086_DBT-S01962_rcc_Normal_s26_cx1590_cy1133_244_244.pickle\n",
            "NORMAL/DBT-P04086_DBT-S01962_lmlo_Normal_s48_cx386_cy1089_244_244.pickle\n",
            "NORMAL/DBT-P04086_DBT-S01962_rmlo_Normal_s1_cx1518_cy1150_244_244.pickle\n",
            "NORMAL/DBT-P04086_DBT-S01962_lcc_Normal_s61_cx295_cy1240_244_244.pickle\n",
            "NORMAL/DBT-P04119_DBT-S02177_rcc_Normal_s45_cx1650_cy1173_244_244.pickle\n",
            "NORMAL/DBT-P04119_DBT-S02177_lmlo_Normal_s32_cx314_cy959_244_244.pickle\n",
            "NORMAL/DBT-P04119_DBT-S02177_lcc_Normal_s41_cx239_cy1133_244_244.pickle\n",
            "NORMAL/DBT-P04119_DBT-S02177_rmlo_Normal_s27_cx1577_cy1045_244_244.pickle\n",
            "NORMAL/DBT-P04122_DBT-S05098_rmlo_Normal_s18_cx1429_cy1106_244_244.pickle\n",
            "NORMAL/DBT-P04122_DBT-S05098_lmlo_Normal_s16_cx509_cy1155_244_244.pickle\n",
            "NORMAL/DBT-P04122_DBT-S05098_lcc_Normal_s62_cx508_cy1195_244_244.pickle\n",
            "NORMAL/DBT-P04122_DBT-S05098_rcc_Normal_s15_cx1485_cy1322_244_244.pickle\n",
            "NORMAL/DBT-P04127_DBT-S00418_rmlo_Normal_s28_cx1501_cy953_244_244.pickle\n",
            "NORMAL/DBT-P04127_DBT-S00418_lmlo_Normal_s42_cx390_cy1081_244_244.pickle\n",
            "NORMAL/DBT-P04127_DBT-S00418_lcc_Normal_s50_cx362_cy1205_244_244.pickle\n",
            "NORMAL/DBT-P04127_DBT-S00418_rcc_Normal_s14_cx1555_cy1126_244_244.pickle\n",
            "NORMAL/DBT-P04140_DBT-S01400_rcc_Normal_s18_cx1655_cy1146_244_244.pickle\n",
            "NORMAL/DBT-P04140_DBT-S01400_lcc_Normal_s11_cx351_cy1314_244_244.pickle\n",
            "NORMAL/DBT-P04140_DBT-S01400_rmlo_Normal_s21_cx1585_cy931_244_244.pickle\n",
            "NORMAL/DBT-P04140_DBT-S01400_lmlo_Normal_s25_cx405_cy1035_244_244.pickle\n",
            "NORMAL/DBT-P04133_DBT-S03994_lmlo_Normal_s34_cx455_cy986_244_244.pickle\n",
            "NORMAL/DBT-P04133_DBT-S03994_lcc_Normal_s60_cx372_cy1268_244_244.pickle\n",
            "NORMAL/DBT-P04133_DBT-S03994_rcc_Normal_s40_cx1651_cy1239_244_244.pickle\n",
            "NORMAL/DBT-P04133_DBT-S03994_rmlo_Normal_s9_cx1560_cy950_244_244.pickle\n",
            "NORMAL/DBT-P04155_DBT-S02439_lmlo_Normal_s45_cx478_cy1124_244_244.pickle\n",
            "NORMAL/DBT-P04155_DBT-S02439_rcc_Normal_s6_cx1536_cy1293_244_244.pickle\n",
            "NORMAL/DBT-P04155_DBT-S02439_rmlo_Normal_s46_cx1560_cy1168_244_244.pickle\n",
            "NORMAL/DBT-P04155_DBT-S02439_lcc_Normal_s69_cx497_cy1184_244_244.pickle\n",
            "NORMAL/DBT-P04132_DBT-S00774_rcc_Normal_s63_cx1567_cy1270_244_244.pickle\n",
            "NORMAL/DBT-P04132_DBT-S00774_lmlo_Normal_s54_cx372_cy1087_244_244.pickle\n",
            "NORMAL/DBT-P04132_DBT-S00774_lcc_Normal_s37_cx345_cy1208_244_244.pickle\n",
            "NORMAL/DBT-P04132_DBT-S00774_rmlo_Normal_s12_cx1522_cy1139_244_244.pickle\n",
            "NORMAL/DBT-P04177_DBT-S01432_lmlo_Normal_s15_cx504_cy1042_244_244.pickle\n",
            "NORMAL/DBT-P04177_DBT-S01432_rmlo_Normal_s61_cx1502_cy1047_244_244.pickle\n",
            "NORMAL/DBT-P04177_DBT-S01432_rcc_Normal_s40_cx1568_cy1247_244_244.pickle\n",
            "NORMAL/DBT-P04177_DBT-S01432_lcc_Normal_s38_cx444_cy1202_244_244.pickle\n",
            "NORMAL/DBT-P04189_DBT-S05166_rcc_Normal_s46_cx1588_cy1263_244_244.pickle\n",
            "NORMAL/DBT-P04189_DBT-S05166_lmlo_Normal_s24_cx464_cy966_244_244.pickle\n",
            "NORMAL/DBT-P04189_DBT-S05166_rmlo_Normal_s40_cx1552_cy1029_244_244.pickle\n",
            "NORMAL/DBT-P04189_DBT-S05166_lcc_Normal_s33_cx406_cy1401_244_244.pickle\n",
            "NORMAL/DBT-P04135_DBT-S01742_lcc_Normal_s26_cx346_cy1184_244_244.pickle\n",
            "NORMAL/DBT-P04135_DBT-S01742_rmlo_Normal_s25_cx1563_cy1095_244_244.pickle\n",
            "NORMAL/DBT-P04135_DBT-S01742_lmlo_Normal_s33_cx412_cy965_244_244.pickle\n",
            "NORMAL/DBT-P04135_DBT-S01742_rcc_Normal_s20_cx1615_cy1236_244_244.pickle\n",
            "NORMAL/DBT-P04164_DBT-S01481_rcc_Normal_s47_cx1485_cy1179_244_244.pickle\n",
            "NORMAL/DBT-P04164_DBT-S01481_lmlo_Normal_s40_cx422_cy1181_244_244.pickle\n",
            "NORMAL/DBT-P04164_DBT-S01481_lcc_Normal_s10_cx377_cy1222_244_244.pickle\n",
            "NORMAL/DBT-P04164_DBT-S01481_rmlo_Normal_s5_cx1414_cy1162_244_244.pickle\n",
            "NORMAL/DBT-P04244_DBT-S01160_rcc_Normal_s29_cx1486_cy1276_244_244.pickle\n",
            "NORMAL/DBT-P04244_DBT-S01160_lmlo_Normal_s33_cx587_cy1190_244_244.pickle\n",
            "NORMAL/DBT-P04244_DBT-S01160_rmlo_Normal_s21_cx1442_cy1240_244_244.pickle\n",
            "NORMAL/DBT-P04244_DBT-S01160_lcc_Normal_s39_cx516_cy1172_244_244.pickle\n",
            "NORMAL/DBT-P04240_DBT-S01689_lcc_Normal_s20_cx588_cy1261_244_244.pickle\n",
            "NORMAL/DBT-P04240_DBT-S01689_lmlo_Normal_s56_cx619_cy1153_244_244.pickle\n",
            "NORMAL/DBT-P04240_DBT-S01689_rcc_Normal_s53_cx1417_cy1168_244_244.pickle\n",
            "NORMAL/DBT-P04240_DBT-S01689_rmlo_Normal_s30_cx1364_cy1207_244_244.pickle\n",
            "NORMAL/DBT-P04225_DBT-S01797_lcc_Normal_s12_cx241_cy1106_244_244.pickle\n",
            "NORMAL/DBT-P04225_DBT-S01797_rmlo_Normal_s6_cx1621_cy922_244_244.pickle\n",
            "NORMAL/DBT-P04225_DBT-S01797_lmlo_Normal_s26_cx276_cy930_244_244.pickle\n",
            "NORMAL/DBT-P04225_DBT-S01797_rcc_Normal_s25_cx1688_cy1056_244_244.pickle\n",
            "NORMAL/DBT-P04238_DBT-S02663_lcc_Normal_s9_cx391_cy1154_244_244.pickle\n",
            "NORMAL/DBT-P04238_DBT-S02663_lmlo_Normal_s32_cx422_cy944_244_244.pickle\n",
            "NORMAL/DBT-P04238_DBT-S02663_rmlo_Normal_s71_cx1590_cy1031_244_244.pickle\n",
            "NORMAL/DBT-P04238_DBT-S02663_rcc_Normal_s26_cx1612_cy1126_244_244.pickle\n",
            "NORMAL/DBT-P04227_DBT-S05002_lcc_Normal_s25_cx260_cy1325_244_244.pickle\n",
            "NORMAL/DBT-P04227_DBT-S05002_lmlo_Normal_s39_cx309_cy919_244_244.pickle\n",
            "NORMAL/DBT-P04227_DBT-S05002_rmlo_Normal_s8_cx1693_cy934_244_244.pickle\n",
            "NORMAL/DBT-P04227_DBT-S05002_rcc_Normal_s4_cx1723_cy1314_244_244.pickle\n",
            "NORMAL/DBT-P04216_DBT-S02581_rcc_Normal_s31_cx1561_cy1229_244_244.pickle\n",
            "NORMAL/DBT-P04216_DBT-S02581_lcc_Normal_s45_cx456_cy1170_244_244.pickle\n",
            "NORMAL/DBT-P04216_DBT-S02581_lmlo_Normal_s41_cx487_cy1139_244_244.pickle\n",
            "NORMAL/DBT-P04216_DBT-S02581_rmlo_Normal_s58_cx1529_cy1103_244_244.pickle\n",
            "NORMAL/DBT-P04236_DBT-S05200_lmlo_Normal_s50_cx635_cy1217_244_244.pickle\n",
            "NORMAL/DBT-P04236_DBT-S05200_rcc_Normal_s12_cx1590_cy1199_244_244.pickle\n",
            "NORMAL/DBT-P04236_DBT-S05200_rmlo_Normal_s54_cx1369_cy1203_244_244.pickle\n",
            "NORMAL/DBT-P04236_DBT-S05200_lcc_Normal_s35_cx410_cy1379_244_244.pickle\n",
            "NORMAL/DBT-P04206_DBT-S01473_lcc_Normal_s16_cx327_cy1196_244_244.pickle\n",
            "NORMAL/DBT-P04206_DBT-S01473_lmlo_Normal_s8_cx421_cy1131_244_244.pickle\n",
            "NORMAL/DBT-P04206_DBT-S01473_rcc_Normal_s41_cx1542_cy1205_244_244.pickle\n",
            "NORMAL/DBT-P04206_DBT-S01473_rmlo_Normal_s43_cx1479_cy1117_244_244.pickle\n",
            "NORMAL/DBT-P04246_DBT-S03531_lmlo_Normal_s8_cx316_cy983_244_244.pickle\n",
            "NORMAL/DBT-P04246_DBT-S03531_rmlo_Normal_s44_cx1574_cy932_244_244.pickle\n",
            "NORMAL/DBT-P04246_DBT-S03531_lcc_Normal_s9_cx280_cy1179_244_244.pickle\n",
            "NORMAL/DBT-P04246_DBT-S03531_rcc_Normal_s39_cx1632_cy1161_244_244.pickle\n",
            "NORMAL/DBT-P04282_DBT-S01356_lcc_Normal_s29_cx274_cy1158_244_244.pickle\n",
            "NORMAL/DBT-P04282_DBT-S01356_lmlo_Normal_s3_cx325_cy1018_244_244.pickle\n",
            "NORMAL/DBT-P04282_DBT-S01356_rmlo_Normal_s30_cx1602_cy1036_244_244.pickle\n",
            "NORMAL/DBT-P04282_DBT-S01356_rcc_Normal_s3_cx1636_cy1250_244_244.pickle\n",
            "NORMAL/DBT-P04254_DBT-S01438_rmlo_Normal_s40_cx1551_cy1023_244_244.pickle\n",
            "NORMAL/DBT-P04254_DBT-S01438_lmlo_Normal_s35_cx370_cy1085_244_244.pickle\n",
            "NORMAL/DBT-P04254_DBT-S01438_rcc_Normal_s17_cx1553_cy1291_244_244.pickle\n",
            "NORMAL/DBT-P04254_DBT-S01438_lcc_Normal_s38_cx372_cy1340_244_244.pickle\n",
            "NORMAL/DBT-P04278_DBT-S01280_lcc_Normal_s28_cx399_cy1307_244_244.pickle\n",
            "NORMAL/DBT-P04278_DBT-S01280_lmlo_Normal_s46_cx451_cy1265_244_244.pickle\n",
            "NORMAL/DBT-P04278_DBT-S01280_rmlo_Normal_s33_cx1413_cy1175_244_244.pickle\n",
            "NORMAL/DBT-P04278_DBT-S01280_rcc_Normal_s49_cx1513_cy1260_244_244.pickle\n",
            "NORMAL/DBT-P04301_DBT-S03891_lcc_Normal_s13_cx575_cy1125_244_244.pickle\n",
            "NORMAL/DBT-P04301_DBT-S03891_lmlo_Normal_s52_cx635_cy1182_244_244.pickle\n",
            "NORMAL/DBT-P04301_DBT-S03891_rcc_Normal_s2_cx1503_cy1224_244_244.pickle\n",
            "NORMAL/DBT-P04301_DBT-S03891_rmlo_Normal_s4_cx1420_cy1109_244_244.pickle\n",
            "NORMAL/DBT-P04343_DBT-S00457_lcc_Normal_s60_cx340_cy1115_244_244.pickle\n",
            "NORMAL/DBT-P04343_DBT-S00457_rcc_Normal_s13_cx1634_cy1280_244_244.pickle\n",
            "NORMAL/DBT-P04343_DBT-S00457_rmlo_Normal_s52_cx1606_cy905_244_244.pickle\n",
            "NORMAL/DBT-P04343_DBT-S00457_lmlo_Normal_s45_cx378_cy992_244_244.pickle\n",
            "NORMAL/DBT-P04399_DBT-S02357_lcc_Normal_s31_cx486_cy1193_244_244.pickle\n",
            "NORMAL/DBT-P04399_DBT-S02357_lmlo_Normal_s56_cx516_cy1091_244_244.pickle\n",
            "NORMAL/DBT-P04399_DBT-S02357_rcc_Normal_s56_cx1612_cy1280_244_244.pickle\n",
            "NORMAL/DBT-P04399_DBT-S02357_rmlo_Normal_s1_cx1537_cy978_244_244.pickle\n",
            "NORMAL/DBT-P04353_DBT-S00354_rcc_Normal_s31_cx1539_cy1196_244_244.pickle\n",
            "NORMAL/DBT-P04353_DBT-S00354_lmlo_Normal_s37_cx417_cy1029_244_244.pickle\n",
            "NORMAL/DBT-P04353_DBT-S00354_lcc_Normal_s17_cx389_cy1197_244_244.pickle\n",
            "NORMAL/DBT-P04353_DBT-S00354_rmlo_Normal_s2_cx1475_cy1162_244_244.pickle\n",
            "NORMAL/DBT-P04306_DBT-S05531_rmlo_Normal_s57_cx1605_cy1103_244_244.pickle\n",
            "NORMAL/DBT-P04306_DBT-S05531_rcc_Normal_s3_cx1607_cy1042_244_244.pickle\n",
            "NORMAL/DBT-P04306_DBT-S05531_lmlo_Normal_s35_cx429_cy1037_244_244.pickle\n",
            "NORMAL/DBT-P04306_DBT-S05531_lcc_Normal_s36_cx423_cy1194_244_244.pickle\n",
            "NORMAL/DBT-P04341_DBT-S01146_rmlo_Normal_s23_cx1373_cy1124_244_244.pickle\n",
            "NORMAL/DBT-P04341_DBT-S01146_rcc_Normal_s65_cx1380_cy1197_244_244.pickle\n",
            "NORMAL/DBT-P04341_DBT-S01146_lmlo_Normal_s74_cx615_cy1141_244_244.pickle\n",
            "NORMAL/DBT-P04341_DBT-S01146_lcc_Normal_s26_cx611_cy1289_244_244.pickle\n",
            "NORMAL/DBT-P04322_DBT-S04522_lmlo_Normal_s18_cx290_cy968_244_244.pickle\n",
            "NORMAL/DBT-P04322_DBT-S04522_lcc_Normal_s9_cx279_cy1148_244_244.pickle\n",
            "NORMAL/DBT-P04322_DBT-S04522_rmlo_Normal_s31_cx1586_cy999_244_244.pickle\n",
            "NORMAL/DBT-P04322_DBT-S04522_rcc_Normal_s37_cx1623_cy1194_244_244.pickle\n",
            "NORMAL/DBT-P04390_DBT-S04589_rmlo_Normal_s29_cx1373_cy1219_244_244.pickle\n",
            "NORMAL/DBT-P04390_DBT-S04589_lmlo_Normal_s33_cx601_cy1193_244_244.pickle\n",
            "NORMAL/DBT-P04390_DBT-S04589_rcc_Normal_s18_cx1453_cy1319_244_244.pickle\n",
            "NORMAL/DBT-P04390_DBT-S04589_lcc_Normal_s12_cx509_cy1224_244_244.pickle\n",
            "NORMAL/DBT-P04444_DBT-S05226_rcc_Normal_s61_cx1366_cy1194_244_244.pickle\n",
            "NORMAL/DBT-P04444_DBT-S05226_lmlo_Normal_s74_cx575_cy1096_244_244.pickle\n",
            "NORMAL/DBT-P04444_DBT-S05226_rmlo_Normal_s2_cx1379_cy1133_244_244.pickle\n",
            "NORMAL/DBT-P04444_DBT-S05226_lcc_Normal_s13_cx567_cy1138_244_244.pickle\n",
            "NORMAL/DBT-P04434_DBT-S05587_lcc_Normal_s39_cx551_cy1302_244_244.pickle\n",
            "NORMAL/DBT-P04434_DBT-S05587_rmlo_Normal_s22_cx1438_cy1102_244_244.pickle\n",
            "NORMAL/DBT-P04434_DBT-S05587_lmlo_Normal_s49_cx621_cy1175_244_244.pickle\n",
            "NORMAL/DBT-P04434_DBT-S05587_rcc_Normal_s45_cx1504_cy1179_244_244.pickle\n",
            "NORMAL/DBT-P04443_DBT-S01577_lmlo_Normal_s48_cx465_cy1143_244_244.pickle\n",
            "NORMAL/DBT-P04443_DBT-S01577_lcc_Normal_s12_cx483_cy1105_244_244.pickle\n",
            "NORMAL/DBT-P04443_DBT-S01577_rmlo_Normal_s61_cx1538_cy1120_244_244.pickle\n",
            "NORMAL/DBT-P04443_DBT-S01577_rcc_Normal_s13_cx1558_cy1123_244_244.pickle\n",
            "NORMAL/DBT-P04405_DBT-S02037_lmlo_Normal_s41_cx530_cy1153_244_244.pickle\n",
            "NORMAL/DBT-P04405_DBT-S02037_lcc_Normal_s11_cx509_cy1275_244_244.pickle\n",
            "NORMAL/DBT-P04405_DBT-S02037_rmlo_Normal_s16_cx1460_cy1166_244_244.pickle\n",
            "NORMAL/DBT-P04405_DBT-S02037_rcc_Normal_s33_cx1440_cy1348_244_244.pickle\n",
            "NORMAL/DBT-P04407_DBT-S02633_rmlo_Normal_s23_cx1632_cy970_244_244.pickle\n",
            "NORMAL/DBT-P04407_DBT-S02633_lmlo_Normal_s5_cx342_cy978_244_244.pickle\n",
            "NORMAL/DBT-P04407_DBT-S02633_lcc_Normal_s28_cx304_cy1335_244_244.pickle\n",
            "NORMAL/DBT-P04407_DBT-S02633_rcc_Normal_s55_cx1683_cy1305_244_244.pickle\n",
            "NORMAL/DBT-P04419_DBT-S04678_rcc_Normal_s45_cx1700_cy1225_244_244.pickle\n",
            "NORMAL/DBT-P04419_DBT-S04678_lmlo_Normal_s44_cx362_cy997_244_244.pickle\n",
            "NORMAL/DBT-P04419_DBT-S04678_rmlo_Normal_s1_cx1676_cy913_244_244.pickle\n",
            "NORMAL/DBT-P04419_DBT-S04678_lcc_Normal_s2_cx339_cy1212_244_244.pickle\n",
            "NORMAL/DBT-P04441_DBT-S02702_lcc_Normal_s53_cx540_cy1260_244_244.pickle\n",
            "NORMAL/DBT-P04441_DBT-S02702_lmlo_Normal_s57_cx559_cy1190_244_244.pickle\n",
            "NORMAL/DBT-P04441_DBT-S02702_rmlo_Normal_s39_cx1441_cy1063_244_244.pickle\n",
            "NORMAL/DBT-P04441_DBT-S02702_rcc_Normal_s29_cx1467_cy1267_244_244.pickle\n",
            "NORMAL/DBT-P04445_DBT-S00035_rcc_Normal_s26_cx1427_cy1307_244_244.pickle\n",
            "NORMAL/DBT-P04445_DBT-S00035_lmlo_Normal_s28_cx439_cy1086_244_244.pickle\n",
            "NORMAL/DBT-P04445_DBT-S00035_lcc_Normal_s17_cx463_cy1479_244_244.pickle\n",
            "NORMAL/DBT-P04445_DBT-S00035_rmlo_Normal_s7_cx1401_cy1103_244_244.pickle\n",
            "NORMAL/DBT-P04464_DBT-S01428_rmlo_Normal_s6_cx1475_cy1058_244_244.pickle\n",
            "NORMAL/DBT-P04464_DBT-S01428_rcc_Normal_s69_cx1536_cy1197_244_244.pickle\n",
            "NORMAL/DBT-P04464_DBT-S01428_lcc_Normal_s41_cx460_cy1357_244_244.pickle\n",
            "NORMAL/DBT-P04464_DBT-S01428_lmlo_Normal_s81_cx507_cy1134_244_244.pickle\n",
            "NORMAL/DBT-P04462_DBT-S00715_rmlo_Normal_s57_cx1474_cy1132_244_244.pickle\n",
            "NORMAL/DBT-P04462_DBT-S00715_rcc_Normal_s6_cx1486_cy1155_244_244.pickle\n",
            "NORMAL/DBT-P04462_DBT-S00715_lcc_Normal_s52_cx419_cy1249_244_244.pickle\n",
            "NORMAL/DBT-P04462_DBT-S00715_lmlo_Normal_s57_cx424_cy1149_244_244.pickle\n",
            "NORMAL/DBT-P04547_DBT-S02671_rmlo_Normal_s26_cx1533_cy992_244_244.pickle\n",
            "NORMAL/DBT-P04547_DBT-S02671_lmlo_Normal_s52_cx380_cy1045_244_244.pickle\n",
            "NORMAL/DBT-P04547_DBT-S02671_lcc_Normal_s25_cx326_cy1406_244_244.pickle\n",
            "NORMAL/DBT-P04547_DBT-S02671_rcc_Normal_s57_cx1620_cy1333_244_244.pickle\n",
            "NORMAL/DBT-P04551_DBT-S04227_lmlo_Normal_s65_cx361_cy897_244_244.pickle\n",
            "NORMAL/DBT-P04551_DBT-S04227_lcc_Normal_s49_cx280_cy1123_244_244.pickle\n",
            "NORMAL/DBT-P04551_DBT-S04227_rmlo_Normal_s7_cx1647_cy1014_244_244.pickle\n",
            "NORMAL/DBT-P04551_DBT-S04227_rcc_Normal_s57_cx1688_cy1236_244_244.pickle\n",
            "NORMAL/DBT-P04522_DBT-S03163_lmlo_Normal_s18_cx312_cy1125_244_244.pickle\n",
            "NORMAL/DBT-P04522_DBT-S03163_rcc_Normal_s14_cx1629_cy1153_244_244.pickle\n",
            "NORMAL/DBT-P04522_DBT-S03163_rmlo_Normal_s20_cx1552_cy1021_244_244.pickle\n",
            "NORMAL/DBT-P04522_DBT-S03163_lcc_Normal_s40_cx243_cy1124_244_244.pickle\n",
            "NORMAL/DBT-P04447_DBT-S01716_rmlo_Normal_s22_cx1520_cy1091_244_244.pickle\n",
            "NORMAL/DBT-P04447_DBT-S01716_lmlo_Normal_s1_cx353_cy1130_244_244.pickle\n",
            "NORMAL/DBT-P04447_DBT-S01716_lcc_Normal_s3_cx302_cy1185_244_244.pickle\n",
            "NORMAL/DBT-P04447_DBT-S01716_rcc_Normal_s43_cx1580_cy1384_244_244.pickle\n",
            "NORMAL/DBT-P04621_DBT-S01950_rmlo_Normal_s36_cx1618_cy1102_244_244.pickle\n",
            "NORMAL/DBT-P04621_DBT-S01950_rcc_Normal_s13_cx1666_cy1373_244_244.pickle\n",
            "NORMAL/DBT-P04621_DBT-S01950_lcc_Normal_s9_cx239_cy1381_244_244.pickle\n",
            "NORMAL/DBT-P04621_DBT-S01950_lmlo_Normal_s43_cx316_cy1100_244_244.pickle\n",
            "NORMAL/DBT-P04611_DBT-S01417_rcc_Normal_s25_cx1625_cy1138_244_244.pickle\n",
            "NORMAL/DBT-P04611_DBT-S01417_lmlo_Normal_s35_cx405_cy999_244_244.pickle\n",
            "NORMAL/DBT-P04611_DBT-S01417_rmlo_Normal_s62_cx1593_cy959_244_244.pickle\n",
            "NORMAL/DBT-P04611_DBT-S01417_lcc_Normal_s64_cx358_cy1225_244_244.pickle\n",
            "NORMAL/DBT-P04593_DBT-S02723_lcc_Normal_s7_cx337_cy1185_244_244.pickle\n",
            "NORMAL/DBT-P04593_DBT-S02723_rcc_Normal_s1_cx1556_cy1117_244_244.pickle\n",
            "NORMAL/DBT-P04593_DBT-S02723_rmlo_Normal_s11_cx1488_cy1079_244_244.pickle\n",
            "NORMAL/DBT-P04593_DBT-S02723_lmlo_Normal_s36_cx405_cy1042_244_244.pickle\n",
            "NORMAL/DBT-P04566_DBT-S04616_lcc_Normal_s26_cx324_cy1231_244_244.pickle\n",
            "NORMAL/DBT-P04566_DBT-S04616_rmlo_Normal_s12_cx1529_cy1062_244_244.pickle\n",
            "NORMAL/DBT-P04566_DBT-S04616_rcc_Normal_s33_cx1559_cy1165_244_244.pickle\n",
            "NORMAL/DBT-P04566_DBT-S04616_lmlo_Normal_s3_cx360_cy1044_244_244.pickle\n",
            "NORMAL/DBT-P04610_DBT-S05438_lmlo_Normal_s24_cx472_cy1147_244_244.pickle\n",
            "NORMAL/DBT-P04610_DBT-S05438_rmlo_Normal_s20_cx1466_cy1095_244_244.pickle\n",
            "NORMAL/DBT-P04610_DBT-S05438_lcc_Normal_s35_cx433_cy1193_244_244.pickle\n",
            "NORMAL/DBT-P04610_DBT-S05438_rcc_Normal_s7_cx1555_cy1270_244_244.pickle\n",
            "NORMAL/DBT-P04617_DBT-S00526_lcc_Normal_s5_cx427_cy1309_244_244.pickle\n",
            "NORMAL/DBT-P04617_DBT-S00526_lmlo_Normal_s72_cx478_cy1113_244_244.pickle\n",
            "NORMAL/DBT-P04617_DBT-S00526_rcc_Normal_s68_cx1540_cy1219_244_244.pickle\n",
            "NORMAL/DBT-P04617_DBT-S00526_rmlo_Normal_s64_cx1486_cy1073_244_244.pickle\n",
            "NORMAL/DBT-P04561_DBT-S02522_rcc_Normal_s41_cx1449_cy1220_244_244.pickle\n",
            "NORMAL/DBT-P04561_DBT-S02522_lcc_Normal_s22_cx378_cy1276_244_244.pickle\n",
            "NORMAL/DBT-P04561_DBT-S02522_rmlo_Normal_s42_cx1396_cy1110_244_244.pickle\n",
            "NORMAL/DBT-P04561_DBT-S02522_lmlo_Normal_s15_cx400_cy1076_244_244.pickle\n",
            "NORMAL/DBT-P04579_DBT-S00461_rmlo_Normal_s25_cx1555_cy1073_244_244.pickle\n",
            "NORMAL/DBT-P04579_DBT-S00461_rcc_Normal_s34_cx1530_cy1221_244_244.pickle\n",
            "NORMAL/DBT-P04579_DBT-S00461_lcc_Normal_s4_cx509_cy1242_244_244.pickle\n",
            "NORMAL/DBT-P04579_DBT-S00461_lmlo_Normal_s4_cx472_cy1047_244_244.pickle\n",
            "NORMAL/DBT-P04664_DBT-S01645_rmlo_Normal_s42_cx1557_cy1029_244_244.pickle\n",
            "NORMAL/DBT-P04664_DBT-S01645_rcc_Normal_s10_cx1557_cy995_244_244.pickle\n",
            "NORMAL/DBT-P04664_DBT-S01645_lmlo_Normal_s19_cx330_cy1152_244_244.pickle\n",
            "NORMAL/DBT-P04664_DBT-S01645_lcc_Normal_s4_cx303_cy1234_244_244.pickle\n",
            "NORMAL/DBT-P04640_DBT-S03087_rmlo_Normal_s46_cx1577_cy968_244_244.pickle\n",
            "NORMAL/DBT-P04640_DBT-S03087_rcc_Normal_s74_cx1682_cy1210_244_244.pickle\n",
            "NORMAL/DBT-P04640_DBT-S03087_lcc_Normal_s79_cx316_cy1259_244_244.pickle\n",
            "NORMAL/DBT-P04640_DBT-S03087_lmlo_Normal_s18_cx400_cy1054_244_244.pickle\n",
            "NORMAL/DBT-P04683_DBT-S03436_rcc_Normal_s66_cx1457_cy1291_244_244.pickle\n",
            "NORMAL/DBT-P04683_DBT-S03436_rmlo_Normal_s56_cx1400_cy1129_244_244.pickle\n",
            "NORMAL/DBT-P04683_DBT-S03436_lmlo_Normal_s26_cx591_cy1227_244_244.pickle\n",
            "NORMAL/DBT-P04683_DBT-S03436_lcc_Normal_s48_cx550_cy1242_244_244.pickle\n",
            "NORMAL/DBT-P04636_DBT-S02954_lcc_Normal_s3_cx279_cy1294_244_244.pickle\n",
            "NORMAL/DBT-P04636_DBT-S02954_lmlo_Normal_s12_cx345_cy1133_244_244.pickle\n",
            "NORMAL/DBT-P04636_DBT-S02954_rmlo_Normal_s47_cx1471_cy1080_244_244.pickle\n",
            "NORMAL/DBT-P04636_DBT-S02954_rcc_Normal_s28_cx1556_cy1140_244_244.pickle\n",
            "NORMAL/DBT-P04668_DBT-S05401_rcc_Normal_s28_cx1543_cy1209_244_244.pickle\n",
            "NORMAL/DBT-P04668_DBT-S05401_lcc_Normal_s11_cx333_cy1328_244_244.pickle\n",
            "NORMAL/DBT-P04668_DBT-S05401_rmlo_Normal_s55_cx1520_cy1022_244_244.pickle\n",
            "NORMAL/DBT-P04668_DBT-S05401_lmlo_Normal_s48_cx383_cy1027_244_244.pickle\n",
            "NORMAL/DBT-P04637_DBT-S00370_rcc_Normal_s10_cx1344_cy1235_244_244.pickle\n",
            "NORMAL/DBT-P04637_DBT-S00370_lcc_Normal_s9_cx689_cy1184_244_244.pickle\n",
            "NORMAL/DBT-P04637_DBT-S00370_lmlo_Normal_s61_cx617_cy1308_244_244.pickle\n",
            "NORMAL/DBT-P04637_DBT-S00370_rmlo_Normal_s44_cx1298_cy1319_244_244.pickle\n",
            "NORMAL/DBT-P04652_DBT-S01820_lmlo_Normal_s18_cx670_cy1083_244_244.pickle\n",
            "NORMAL/DBT-P04652_DBT-S01820_rmlo_Normal_s40_cx1414_cy1043_244_244.pickle\n",
            "NORMAL/DBT-P04652_DBT-S01820_lcc_Normal_s18_cx642_cy1268_244_244.pickle\n",
            "NORMAL/DBT-P04652_DBT-S01820_rcc_Normal_s41_cx1457_cy1240_244_244.pickle\n",
            "NORMAL/DBT-P04752_DBT-S04976_rcc_Normal_s16_cx1660_cy1157_244_244.pickle\n",
            "NORMAL/DBT-P04752_DBT-S04976_rmlo_Normal_s22_cx1607_cy933_244_244.pickle\n",
            "NORMAL/DBT-P04752_DBT-S04976_lcc_Normal_s28_cx416_cy1229_244_244.pickle\n",
            "NORMAL/DBT-P04752_DBT-S04976_lmlo_Normal_s2_cx426_cy970_244_244.pickle\n",
            "NORMAL/DBT-P04685_DBT-S00472_rcc_Normal_s28_cx1440_cy1166_244_244.pickle\n",
            "NORMAL/DBT-P04685_DBT-S00472_lmlo_Normal_s50_cx492_cy1230_244_244.pickle\n",
            "NORMAL/DBT-P04685_DBT-S00472_rmlo_Normal_s2_cx1392_cy1237_244_244.pickle\n",
            "NORMAL/DBT-P04685_DBT-S00472_lcc_Normal_s21_cx442_cy1302_244_244.pickle\n",
            "NORMAL/DBT-P04746_DBT-S05244_lmlo_Normal_s4_cx464_cy1074_244_244.pickle\n",
            "NORMAL/DBT-P04746_DBT-S05244_rcc_Normal_s27_cx1478_cy1232_244_244.pickle\n",
            "NORMAL/DBT-P04746_DBT-S05244_lcc_Normal_s36_cx398_cy1036_244_244.pickle\n",
            "NORMAL/DBT-P04746_DBT-S05244_rmlo_Normal_s10_cx1469_cy1041_244_244.pickle\n",
            "NORMAL/DBT-P04789_DBT-S03739_lmlo_Normal_s15_cx595_cy1162_244_244.pickle\n",
            "NORMAL/DBT-P04789_DBT-S03739_rmlo_Normal_s68_cx1416_cy1241_244_244.pickle\n",
            "NORMAL/DBT-P04789_DBT-S03739_rcc_Normal_s57_cx1447_cy1260_244_244.pickle\n",
            "NORMAL/DBT-P04789_DBT-S03739_lcc_Normal_s41_cx558_cy1254_244_244.pickle\n",
            "NORMAL/DBT-P04764_DBT-S01998_rcc_Normal_s3_cx1489_cy1268_244_244.pickle\n",
            "NORMAL/DBT-P04764_DBT-S01998_lmlo_Normal_s15_cx435_cy1260_244_244.pickle\n",
            "NORMAL/DBT-P04764_DBT-S01998_lcc_Normal_s55_cx413_cy1407_244_244.pickle\n",
            "NORMAL/DBT-P04764_DBT-S01998_rmlo_Normal_s5_cx1521_cy1231_244_244.pickle\n",
            "NORMAL/DBT-P04795_DBT-S04036_lmlo_Normal_s59_cx487_cy1077_244_244.pickle\n",
            "NORMAL/DBT-P04795_DBT-S04036_rcc_Normal_s6_cx1534_cy1186_244_244.pickle\n",
            "NORMAL/DBT-P04795_DBT-S04036_rmlo_Normal_s45_cx1510_cy1073_244_244.pickle\n",
            "NORMAL/DBT-P04795_DBT-S04036_lcc_Normal_s46_cx470_cy1264_244_244.pickle\n",
            "NORMAL/DBT-P04779_DBT-S01877_lcc_Normal_s21_cx301_cy1229_244_244.pickle\n",
            "NORMAL/DBT-P04779_DBT-S01877_rmlo_Normal_s11_cx1541_cy1069_244_244.pickle\n",
            "NORMAL/DBT-P04779_DBT-S01877_rcc_Normal_s10_cx1645_cy1220_244_244.pickle\n",
            "NORMAL/DBT-P04779_DBT-S01877_lmlo_Normal_s40_cx386_cy1096_244_244.pickle\n",
            "NORMAL/DBT-P04773_DBT-S04859_lmlo_Normal_s62_cx358_cy861_244_244.pickle\n",
            "NORMAL/DBT-P04773_DBT-S04859_rmlo_Normal_s13_cx1664_cy876_244_244.pickle\n",
            "NORMAL/DBT-P04773_DBT-S04859_rcc_Normal_s28_cx1686_cy1382_244_244.pickle\n",
            "NORMAL/DBT-P04773_DBT-S04859_lcc_Normal_s39_cx289_cy1214_244_244.pickle\n",
            "NORMAL/DBT-P04786_DBT-S03634_rmlo_Normal_s86_cx1336_cy1310_244_244.pickle\n",
            "NORMAL/DBT-P04786_DBT-S03634_lcc_Normal_s82_cx577_cy1296_244_244.pickle\n",
            "NORMAL/DBT-P04786_DBT-S03634_lmlo_Normal_s78_cx674_cy1251_244_244.pickle\n",
            "NORMAL/DBT-P04786_DBT-S03634_rcc_Normal_s28_cx1419_cy1330_244_244.pickle\n",
            "NORMAL/DBT-P04772_DBT-S00483_rmlo_Normal_s35_cx1430_cy1049_244_244.pickle\n",
            "NORMAL/DBT-P04772_DBT-S00483_lmlo_Normal_s49_cx448_cy1041_244_244.pickle\n",
            "NORMAL/DBT-P04772_DBT-S00483_lcc_Normal_s1_cx415_cy1187_244_244.pickle\n",
            "NORMAL/DBT-P04772_DBT-S00483_rcc_Normal_s33_cx1475_cy1301_244_244.pickle\n",
            "NORMAL/DBT-P04768_DBT-S04230_lcc_Normal_s33_cx207_cy1133_244_244.pickle\n",
            "NORMAL/DBT-P04768_DBT-S04230_rcc_Normal_s28_cx1680_cy1289_244_244.pickle\n",
            "NORMAL/DBT-P04768_DBT-S04230_rmlo_Normal_s3_cx1620_cy847_244_244.pickle\n",
            "NORMAL/DBT-P04768_DBT-S04230_lmlo_Normal_s34_cx272_cy939_244_244.pickle\n",
            "NORMAL/DBT-P04894_DBT-S03030_lcc_Normal_s48_cx446_cy1298_244_244.pickle\n",
            "NORMAL/DBT-P04894_DBT-S03030_rcc_Normal_s65_cx1602_cy1281_244_244.pickle\n",
            "NORMAL/DBT-P04894_DBT-S03030_lmlo_Normal_s31_cx519_cy965_244_244.pickle\n",
            "NORMAL/DBT-P04894_DBT-S03030_rmlo_Normal_s40_cx1571_cy1048_244_244.pickle\n",
            "NORMAL/DBT-P04896_DBT-S01312_rcc_Normal_s23_cx1621_cy1182_244_244.pickle\n",
            "NORMAL/DBT-P04896_DBT-S01312_lcc_Normal_s33_cx406_cy1238_244_244.pickle\n",
            "NORMAL/DBT-P04896_DBT-S01312_rmlo_Normal_s48_cx1513_cy1086_244_244.pickle\n",
            "NORMAL/DBT-P04896_DBT-S01312_lmlo_Normal_s74_cx496_cy1052_244_244.pickle\n",
            "NORMAL/DBT-P04853_DBT-S02196_rmlo_Normal_s30_cx1401_cy1158_244_244.pickle\n",
            "NORMAL/DBT-P04853_DBT-S02196_lcc_Normal_s2_cx439_cy1224_244_244.pickle\n",
            "NORMAL/DBT-P04853_DBT-S02196_rcc_Normal_s4_cx1459_cy1055_244_244.pickle\n",
            "NORMAL/DBT-P04853_DBT-S02196_lmlo_Normal_s8_cx472_cy1093_244_244.pickle\n",
            "NORMAL/DBT-P04927_DBT-S02398_lcc_Normal_s17_cx576_cy1308_244_244.pickle\n",
            "NORMAL/DBT-P04927_DBT-S02398_rmlo_Normal_s57_cx1448_cy1165_244_244.pickle\n",
            "NORMAL/DBT-P04927_DBT-S02398_lmlo_Normal_s18_cx601_cy1194_244_244.pickle\n",
            "NORMAL/DBT-P04927_DBT-S02398_rcc_Normal_s15_cx1439_cy1307_244_244.pickle\n",
            "NORMAL/DBT-P04951_DBT-S01031_rcc_Normal_s27_cx1632_cy1231_244_244.pickle\n",
            "NORMAL/DBT-P04951_DBT-S01031_rmlo_Normal_s39_cx1555_cy1012_244_244.pickle\n",
            "NORMAL/DBT-P04951_DBT-S01031_lcc_Normal_s16_cx369_cy1172_244_244.pickle\n",
            "NORMAL/DBT-P04951_DBT-S01031_lmlo_Normal_s54_cx467_cy974_244_244.pickle\n",
            "NORMAL/DBT-P04914_DBT-S03547_lmlo_Normal_s32_cx521_cy1047_244_244.pickle\n",
            "NORMAL/DBT-P04914_DBT-S03547_lcc_Normal_s23_cx473_cy1410_244_244.pickle\n",
            "NORMAL/DBT-P04914_DBT-S03547_rmlo_Normal_s55_cx1473_cy1071_244_244.pickle\n",
            "NORMAL/DBT-P04914_DBT-S03547_rcc_Normal_s11_cx1480_cy1288_244_244.pickle\n",
            "NORMAL/DBT-P04941_DBT-S00275_lcc_Normal_s17_cx326_cy1312_244_244.pickle\n",
            "NORMAL/DBT-P04941_DBT-S00275_rmlo_Normal_s26_cx1677_cy993_244_244.pickle\n",
            "NORMAL/DBT-P04941_DBT-S00275_rcc_Normal_s45_cx1703_cy1305_244_244.pickle\n",
            "NORMAL/DBT-P04941_DBT-S00275_lmlo_Normal_s2_cx377_cy917_244_244.pickle\n",
            "NORMAL/DBT-P04960_DBT-S01881_rcc_Normal_s52_cx1548_cy1184_244_244.pickle\n",
            "NORMAL/DBT-P04960_DBT-S01881_lmlo_Normal_s29_cx473_cy992_244_244.pickle\n",
            "NORMAL/DBT-P04960_DBT-S01881_lcc_Normal_s31_cx460_cy1218_244_244.pickle\n",
            "NORMAL/DBT-P04960_DBT-S01881_rmlo_Normal_s61_cx1564_cy1017_244_244.pickle\n",
            "NORMAL/DBT-P04959_DBT-S05045_rcc_Normal_s38_cx1681_cy1283_244_244.pickle\n",
            "NORMAL/DBT-P04959_DBT-S05045_rmlo_Normal_s1_cx1682_cy1071_244_244.pickle\n",
            "NORMAL/DBT-P04959_DBT-S05045_lmlo_Normal_s30_cx302_cy1096_244_244.pickle\n",
            "NORMAL/DBT-P04959_DBT-S05045_lcc_Normal_s53_cx306_cy1273_244_244.pickle\n",
            "NORMAL/DBT-P04918_DBT-S00595_lcc_Normal_s64_cx503_cy1249_244_244.pickle\n",
            "NORMAL/DBT-P04918_DBT-S00595_rmlo_Normal_s30_cx1367_cy1134_244_244.pickle\n",
            "NORMAL/DBT-P04918_DBT-S00595_rcc_Normal_s39_cx1397_cy1150_244_244.pickle\n",
            "NORMAL/DBT-P04918_DBT-S00595_lmlo_Normal_s55_cx512_cy1251_244_244.pickle\n",
            "NORMAL/DBT-P04994_DBT-S04187_rcc_Normal_s10_cx1657_cy1129_244_244.pickle\n",
            "NORMAL/DBT-P04994_DBT-S04187_lmlo_Normal_s85_cx379_cy1091_244_244.pickle\n",
            "NORMAL/DBT-P04994_DBT-S04187_lcc_Normal_s17_cx323_cy1121_244_244.pickle\n",
            "NORMAL/DBT-P04994_DBT-S04187_rmlo_Normal_s5_cx1601_cy1052_244_244.pickle\n",
            "NORMAL/DBT-P05009_DBT-S02896_lmlo_Normal_s25_cx370_cy1047_244_244.pickle\n",
            "NORMAL/DBT-P05009_DBT-S02896_lcc_Normal_s11_cx320_cy1330_244_244.pickle\n",
            "NORMAL/DBT-P05009_DBT-S02896_rmlo_Normal_s28_cx1563_cy1078_244_244.pickle\n",
            "NORMAL/DBT-P05009_DBT-S02896_rcc_Normal_s13_cx1563_cy1185_244_244.pickle\n",
            "NORMAL/DBT-P04997_DBT-S00788_lcc_Normal_s65_cx434_cy1173_244_244.pickle\n",
            "NORMAL/DBT-P04997_DBT-S00788_rcc_Normal_s45_cx1448_cy1305_244_244.pickle\n",
            "NORMAL/DBT-P04997_DBT-S00788_lmlo_Normal_s29_cx427_cy1007_244_244.pickle\n",
            "NORMAL/DBT-P04997_DBT-S00788_rmlo_Normal_s28_cx1458_cy1115_244_244.pickle\n",
            "NORMAL/DBT-P05027_DBT-S01964_rcc_Normal_s30_cx1568_cy1200_244_244.pickle\n",
            "NORMAL/DBT-P05027_DBT-S01964_rmlo_Normal_s11_cx1479_cy953_244_244.pickle\n",
            "NORMAL/DBT-P05027_DBT-S01964_lmlo_Normal_s10_cx382_cy1018_244_244.pickle\n",
            "NORMAL/DBT-P05027_DBT-S01964_lcc_Normal_s67_cx337_cy1091_244_244.pickle\n",
            "NORMAL/DBT-P05024_DBT-S00990_lcc_Normal_s36_cx452_cy1334_244_244.pickle\n",
            "NORMAL/DBT-P05024_DBT-S00990_lmlo_Normal_s30_cx436_cy1239_244_244.pickle\n",
            "NORMAL/DBT-P05024_DBT-S00990_rmlo_Normal_s4_cx1434_cy1245_244_244.pickle\n",
            "NORMAL/DBT-P05024_DBT-S00990_rcc_Normal_s19_cx1482_cy1447_244_244.pickle\n",
            "NORMAL/DBT-P03032_DBT-S01330_lmlo_Normal_s16_cx368_cy950_244_244.pickle\n",
            "NORMAL/DBT-P03032_DBT-S01330_lcc_Normal_s41_cx320_cy1263_244_244.pickle\n",
            "NORMAL/DBT-P03032_DBT-S01330_rcc_Normal_s26_cx1654_cy1309_244_244.pickle\n",
            "NORMAL/DBT-P03032_DBT-S01330_rmlo_Normal_s12_cx1627_cy1005_244_244.pickle\n",
            "NORMAL/DBT-P05048_DBT-S02631_rmlo_Normal_s15_cx1434_cy1225_244_244.pickle\n",
            "NORMAL/DBT-P05048_DBT-S02631_rcc_Normal_s13_cx1498_cy1214_244_244.pickle\n",
            "NORMAL/DBT-P05048_DBT-S02631_lcc_Normal_s10_cx414_cy1119_244_244.pickle\n",
            "NORMAL/DBT-P03066_DBT-S00404_lmlo_Normal_s39_cx449_cy1027_244_244.pickle\n",
            "NORMAL/DBT-P05048_DBT-S02631_lmlo_Normal_s4_cx449_cy1236_244_244.pickle\n",
            "NORMAL/DBT-P03066_DBT-S00404_lcc_Normal_s42_cx438_cy1439_244_244.pickle\n",
            "NORMAL/DBT-P03066_DBT-S00404_rcc_Normal_s50_cx1544_cy1398_244_244.pickle\n",
            "NORMAL/DBT-P03066_DBT-S00404_rmlo_Normal_s24_cx1542_cy1058_244_244.pickle\n",
            "NORMAL/DBT-P03098_DBT-S00509_lcc_Normal_s4_cx371_cy1274_244_244.pickle\n",
            "NORMAL/DBT-P03098_DBT-S00509_rmlo_Normal_s51_cx1637_cy933_244_244.pickle\n",
            "NORMAL/DBT-P03098_DBT-S00509_lmlo_Normal_s26_cx354_cy982_244_244.pickle\n",
            "NORMAL/DBT-P03098_DBT-S00509_rcc_Normal_s12_cx1434_cy1145_244_244.pickle\n",
            "NORMAL/DBT-P03062_DBT-S02786_rcc_Normal_s67_cx1534_cy1391_244_244.pickle\n",
            "NORMAL/DBT-P03062_DBT-S02786_lcc_Normal_s53_cx345_cy1331_244_244.pickle\n",
            "NORMAL/DBT-P03062_DBT-S02786_lmlo_Normal_s39_cx415_cy1079_244_244.pickle\n",
            "NORMAL/DBT-P03062_DBT-S02786_rmlo_Normal_s13_cx1439_cy1086_244_244.pickle\n",
            "NORMAL/DBT-P03081_DBT-S05220_lmlo_Normal_s45_cx389_cy998_244_244.pickle\n",
            "NORMAL/DBT-P03081_DBT-S05220_lcc_Normal_s42_cx347_cy1319_244_244.pickle\n",
            "NORMAL/DBT-P03081_DBT-S05220_rmlo_Normal_s26_cx1496_cy1032_244_244.pickle\n",
            "NORMAL/DBT-P03081_DBT-S05220_rcc_Normal_s24_cx1532_cy1240_244_244.pickle\n",
            "NORMAL/DBT-P03079_DBT-S00277_lmlo_Normal_s70_cx507_cy1204_244_244.pickle\n",
            "NORMAL/DBT-P03079_DBT-S00277_rcc_Normal_s39_cx1460_cy1158_244_244.pickle\n",
            "NORMAL/DBT-P03079_DBT-S00277_rmlo_Normal_s47_cx1386_cy1146_244_244.pickle\n",
            "NORMAL/DBT-P03079_DBT-S00277_lcc_Normal_s11_cx478_cy1344_244_244.pickle\n",
            "NORMAL/DBT-P03088_DBT-S02585_rcc_Normal_s41_cx1432_cy1282_244_244.pickle\n",
            "NORMAL/DBT-P03088_DBT-S02585_lmlo_Normal_s19_cx524_cy1207_244_244.pickle\n",
            "NORMAL/DBT-P03088_DBT-S02585_lcc_Normal_s3_cx491_cy1372_244_244.pickle\n",
            "NORMAL/DBT-P03088_DBT-S02585_rmlo_Normal_s16_cx1354_cy1071_244_244.pickle\n",
            "NORMAL/DBT-P03063_DBT-S01991_lcc_Normal_s42_cx348_cy1286_244_244.pickle\n",
            "NORMAL/DBT-P03063_DBT-S01991_rcc_Normal_s36_cx1657_cy1245_244_244.pickle\n",
            "NORMAL/DBT-P03063_DBT-S01991_lmlo_Normal_s2_cx393_cy1157_244_244.pickle\n",
            "NORMAL/DBT-P03063_DBT-S01991_rmlo_Normal_s26_cx1651_cy1145_244_244.pickle\n",
            "NORMAL/DBT-P03139_DBT-S02312_rcc_Normal_s65_cx1603_cy1270_244_244.pickle\n",
            "NORMAL/DBT-P03139_DBT-S02312_lmlo_Normal_s62_cx457_cy1132_244_244.pickle\n",
            "NORMAL/DBT-P03139_DBT-S02312_rmlo_Normal_s3_cx1519_cy1106_244_244.pickle\n",
            "NORMAL/DBT-P03139_DBT-S02312_lcc_Normal_s56_cx450_cy1327_244_244.pickle\n",
            "NORMAL/DBT-P03140_DBT-S04232_rcc_Normal_s65_cx1353_cy1217_244_244.pickle\n",
            "NORMAL/DBT-P03140_DBT-S04232_rmlo_Normal_s31_cx1288_cy1218_244_244.pickle\n",
            "NORMAL/DBT-P03140_DBT-S04232_lcc_Normal_s2_cx645_cy1205_244_244.pickle\n",
            "NORMAL/DBT-P03140_DBT-S04232_lmlo_Normal_s30_cx715_cy1191_244_244.pickle\n",
            "NORMAL/DBT-P03164_DBT-S04545_lcc_Normal_s13_cx187_cy1151_244_244.pickle\n",
            "NORMAL/DBT-P03164_DBT-S04545_lmlo_Normal_s9_cx264_cy1105_244_244.pickle\n",
            "NORMAL/DBT-P03164_DBT-S04545_rmlo_Normal_s17_cx1648_cy1127_244_244.pickle\n",
            "NORMAL/DBT-P03164_DBT-S04545_rcc_Normal_s16_cx1692_cy1258_244_244.pickle\n",
            "NORMAL/DBT-P03101_DBT-S00624_rmlo_Normal_s16_cx1371_cy1400_244_244.pickle\n",
            "NORMAL/DBT-P03101_DBT-S00624_lcc_Normal_s1_cx591_cy1265_244_244.pickle\n",
            "NORMAL/DBT-P03101_DBT-S00624_lmlo_Normal_s2_cx589_cy1382_244_244.pickle\n",
            "NORMAL/DBT-P03101_DBT-S00624_rcc_Normal_s68_cx1383_cy1189_244_244.pickle\n",
            "NORMAL/DBT-P03142_DBT-S02541_lcc_Normal_s18_cx503_cy1318_244_244.pickle\n",
            "NORMAL/DBT-P03142_DBT-S02541_rmlo_Normal_s30_cx1433_cy1083_244_244.pickle\n",
            "NORMAL/DBT-P03142_DBT-S02541_rcc_Normal_s32_cx1480_cy1292_244_244.pickle\n",
            "NORMAL/DBT-P03142_DBT-S02541_lmlo_Normal_s63_cx500_cy1171_244_244.pickle\n",
            "NORMAL/DBT-P03163_DBT-S00586_lmlo_Normal_s46_cx556_cy1075_244_244.pickle\n",
            "NORMAL/DBT-P03163_DBT-S00586_rmlo_Normal_s60_cx1451_cy1084_244_244.pickle\n",
            "NORMAL/DBT-P03163_DBT-S00586_rcc_Normal_s70_cx1558_cy1218_244_244.pickle\n",
            "NORMAL/DBT-P03163_DBT-S00586_lcc_Normal_s81_cx466_cy1271_244_244.pickle\n",
            "NORMAL/DBT-P03102_DBT-S03300_lcc_Normal_s61_cx362_cy1310_244_244.pickle\n",
            "NORMAL/DBT-P03102_DBT-S03300_rcc_Normal_s53_cx1594_cy1313_244_244.pickle\n",
            "NORMAL/DBT-P03102_DBT-S03300_rmlo_Normal_s20_cx1510_cy900_244_244.pickle\n",
            "NORMAL/DBT-P03102_DBT-S03300_lmlo_Normal_s51_cx404_cy1026_244_244.pickle\n",
            "NORMAL/DBT-P03153_DBT-S04937_rmlo_Normal_s64_cx1564_cy1058_244_244.pickle\n",
            "NORMAL/DBT-P03153_DBT-S04937_lcc_Normal_s59_cx363_cy1412_244_244.pickle\n",
            "NORMAL/DBT-P03153_DBT-S04937_rcc_Normal_s46_cx1647_cy1266_244_244.pickle\n",
            "NORMAL/DBT-P03153_DBT-S04937_lmlo_Normal_s58_cx413_cy1084_244_244.pickle\n",
            "NORMAL/DBT-P03217_DBT-S03671_rmlo_Normal_s45_cx1558_cy1104_244_244.pickle\n",
            "NORMAL/DBT-P03217_DBT-S03671_lmlo_Normal_s55_cx437_cy1105_244_244.pickle\n",
            "NORMAL/DBT-P03217_DBT-S03671_lcc_Normal_s3_cx438_cy1293_244_244.pickle\n",
            "NORMAL/DBT-P03217_DBT-S03671_rcc_Normal_s52_cx1561_cy1226_244_244.pickle\n",
            "NORMAL/DBT-P03192_DBT-S00409_lcc_Normal_s2_cx487_cy1376_244_244.pickle\n",
            "NORMAL/DBT-P03192_DBT-S00409_rmlo_Normal_s39_cx1314_cy1077_244_244.pickle\n",
            "NORMAL/DBT-P03192_DBT-S00409_lmlo_Normal_s37_cx586_cy1180_244_244.pickle\n",
            "NORMAL/DBT-P03192_DBT-S00409_rcc_Normal_s2_cx1423_cy1457_244_244.pickle\n",
            "NORMAL/DBT-P03182_DBT-S00363_rcc_Normal_s32_cx1630_cy1029_244_244.pickle\n",
            "NORMAL/DBT-P03182_DBT-S00363_rmlo_Normal_s1_cx1607_cy1016_244_244.pickle\n",
            "NORMAL/DBT-P03182_DBT-S00363_lcc_Normal_s5_cx261_cy1076_244_244.pickle\n",
            "NORMAL/DBT-P03182_DBT-S00363_lmlo_Normal_s25_cx300_cy960_244_244.pickle\n",
            "NORMAL/DBT-P03216_DBT-S00399_rcc_Normal_s46_cx1665_cy1301_244_244.pickle\n",
            "NORMAL/DBT-P03216_DBT-S00399_rmlo_Normal_s49_cx1677_cy997_244_244.pickle\n",
            "NORMAL/DBT-P03216_DBT-S00399_lmlo_Normal_s10_cx229_cy903_244_244.pickle\n",
            "NORMAL/DBT-P03216_DBT-S00399_lcc_Normal_s3_cx240_cy1270_244_244.pickle\n",
            "NORMAL/DBT-P03296_DBT-S04679_lmlo_Normal_s28_cx277_cy1027_244_244.pickle\n",
            "NORMAL/DBT-P03296_DBT-S04679_lcc_Normal_s3_cx224_cy1162_244_244.pickle\n",
            "NORMAL/DBT-P03296_DBT-S04679_rmlo_Normal_s31_cx1662_cy1081_244_244.pickle\n",
            "NORMAL/DBT-P03296_DBT-S04679_rcc_Normal_s38_cx1680_cy1299_244_244.pickle\n",
            "NORMAL/DBT-P03298_DBT-S01519_rcc_Normal_s29_cx1529_cy1227_244_244.pickle\n",
            "NORMAL/DBT-P03298_DBT-S01519_lmlo_Normal_s62_cx419_cy983_244_244.pickle\n",
            "NORMAL/DBT-P03298_DBT-S01519_lcc_Normal_s34_cx362_cy1309_244_244.pickle\n",
            "NORMAL/DBT-P03298_DBT-S01519_rmlo_Normal_s52_cx1459_cy972_244_244.pickle\n",
            "NORMAL/DBT-P03290_DBT-S04684_rmlo_Normal_s51_cx1389_cy1173_244_244.pickle\n",
            "NORMAL/DBT-P03290_DBT-S04684_rcc_Normal_s13_cx1391_cy1211_244_244.pickle\n",
            "NORMAL/DBT-P03290_DBT-S04684_lmlo_Normal_s40_cx568_cy1177_244_244.pickle\n",
            "NORMAL/DBT-P03290_DBT-S04684_lcc_Normal_s57_cx556_cy1206_244_244.pickle\n",
            "NORMAL/DBT-P03279_DBT-S03685_rcc_Normal_s20_cx1683_cy1310_244_244.pickle\n",
            "NORMAL/DBT-P03279_DBT-S03685_lmlo_Normal_s15_cx331_cy1025_244_244.pickle\n",
            "NORMAL/DBT-P03279_DBT-S03685_lcc_Normal_s55_cx327_cy1327_244_244.pickle\n",
            "NORMAL/DBT-P03279_DBT-S03685_rmlo_Normal_s8_cx1656_cy867_244_244.pickle\n",
            "NORMAL/DBT-P03258_DBT-S05584_lmlo_Normal_s4_cx398_cy1156_244_244.pickle\n",
            "NORMAL/DBT-P03258_DBT-S05584_rmlo_Normal_s25_cx1482_cy1098_244_244.pickle\n",
            "NORMAL/DBT-P03258_DBT-S05584_lcc_Normal_s7_cx341_cy1293_244_244.pickle\n",
            "NORMAL/DBT-P03258_DBT-S05584_rcc_Normal_s8_cx1539_cy1123_244_244.pickle\n",
            "NORMAL/DBT-P03261_DBT-S05042_rcc_Normal_s72_cx1585_cy1146_244_244.pickle\n",
            "1859\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'DBT-P03915_DBT-S05004_lcc_Cancer_s25_cx417_cy1039_244_244.pickle'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp7kwrfSlK2r"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms  #get normalization functions\n",
        "\n",
        "class CustomImageDataset(): #Dataset):\n",
        "    def __init__(self, img_dir,category=[],file_count=1,file_list =[],transform=None, target_transform=None):\n",
        "        #self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.category = category\n",
        "        self.file_count = file_count\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.category_name =''\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    def image_normalize(image):\n",
        "        #replace with the more tensor friendly normalize once tensor shapes confirmed\n",
        "        image = image/65535.0\n",
        "        return image\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.file_count #len(self.file_list) #99 #len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        fname = self.file_list[index]\n",
        "\n",
        "        #get label and pull category \n",
        "        text_tokens = fname.split(sep='_')\n",
        "        label_class = text_tokens[3] #get the label token in 4th position\n",
        "        self.category_name =  label_class.upper() \n",
        "\n",
        "\n",
        "        full_file_name = os.path.join(self.img_dir,self.category_name,fname)\n",
        "        image = pickle.load( open( full_file_name, \"rb\" ) )\n",
        "        image = image.astype(float) #using patch images\n",
        "        \n",
        "\n",
        "        #VERIFY THE IMAGES ARE ALL THE SAME 3x244x244\n",
        "        shapes = image.shape\n",
        "        assert (shapes[0] == 3),\"Image slice error: {0}\".format(fname)\n",
        "        assert (shapes[1] == 244), print('Image row error: ',fname)\n",
        "        assert (shapes[2] == 244), print('Image column error: ',fname)\n",
        "        #if (shapes[0] != 3 and shapes[1] != 244 and shapes[2]!= 244):\n",
        "\n",
        "\n",
        "        #Normalize the data to 0,1 from 2^16\n",
        "        image = image/65535.0 #image_normalize(image)\n",
        "\n",
        "\n",
        "\n",
        "        #test out numeric label\n",
        "        if (label_class in 'Normal'):\n",
        "            label = 0\n",
        "        elif (label_class in 'Actionable'):\n",
        "            print('!!!!! ACTIONABLE PASSED THROUGH')\n",
        "            stop()\n",
        "            label = 1\n",
        "        elif (label_class in 'Benign'):\n",
        "            label = 1\n",
        "        else: # (label_class in 'Cancer'):\n",
        "            label = 2\n",
        "\n",
        "\n",
        "        #print(full_file_name)\n",
        "\n",
        "\n",
        "        #read_image(img_path)\n",
        "        #label = self.img_labels.iloc[idx, 1]\n",
        "        #if self.transform:\n",
        "        #    image = self.transform(image)\n",
        "        #if self.target_transform:\n",
        "        #    label = self.target_transform(label)\n",
        "        sample = {\"image\": image, \"label\": label}\n",
        "            #sample = file_name\n",
        "        return sample\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vH5Cqast3JR",
        "outputId": "8a744b01-4dd8-4556-a130-fca4e10cd7b2"
      },
      "source": [
        "#\n",
        "# Adjust the weighting and sampling of the training data to correct for any\n",
        "#imbalances. This is used in loss and dataloading\n",
        "#\n",
        "\n",
        "#GENERATE THE WEIGHT TENSOR BASED UPON THE LABEL APPLIED\n",
        "weight_list =[]\n",
        "for ii in  full_file_list:\n",
        "        \n",
        "    for jj in ii:\n",
        "        if ('Normal' in jj):\n",
        "            wval = 1\n",
        "        elif ('Benign' in jj):\n",
        "            wval = 3\n",
        "        else:\n",
        "            wval = 5\n",
        "\n",
        "        weight_list.append(wval)\n",
        "\n",
        "    #count = count + len(nfiles)\n",
        "number_weight_values = len(weight_list)\n",
        "number_weight_values = torch.tensor(number_weight_values)\n",
        "print('Number of weight values = ',number_weight_values)\n",
        "\n",
        "#target_list = full_file_list[torch.randperm(len(full_file_list))]\n",
        "weight_numbers = torch.tensor([1,15,27], dtype=torch.float32)\n",
        "\n",
        "#class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "#weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
        "sampler = torch.utils.data.WeightedRandomSampler(torch.DoubleTensor(weight_list), int(4))\n",
        "\n",
        "\n",
        "#sampler = torch.utils.data.WeightedRandomSampler(weights=weight_numbers, \n",
        "#                                                 num_samples=number_weight_values, replacement=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of weight values =  tensor(120306)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1p45uP4oi6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4fff47c-e8bd-498c-997d-17b402820c3a"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#\n",
        "count =0\n",
        "# Setup the custom dataset\n",
        "for ii in category_folders:\n",
        "        nfiles = os.listdir(os.path.join(patch_dir,ii))\n",
        "        count = count + len(nfiles)\n",
        "print('Number of patch files found = ',count)\n",
        "\n",
        "#load up with the pre-sized patch images\n",
        "training_data = CustomImageDataset(img_dir=patch_dir,\n",
        "                                   category = full_category_name, \n",
        "                                   file_count=full_file_count,\n",
        "                                   file_list = full_file_list, \n",
        "                                   transform=None, \n",
        "                                   target_transform=None)\n",
        "\n",
        "#\n",
        "# TEST OUT SPLITTING DATASETS INTO TRAIN/TEST\n",
        "#\n",
        "train_size = int(0.8 * len(training_data))\n",
        "val_size = int(0.1 * len(training_data))\n",
        "test_size = len(training_data) - train_size - val_size\n",
        "train_subset, val_subset, test_subset = torch.utils.data.random_split(training_data, [train_size, val_size,test_size])\n",
        "\n",
        "\n",
        "bsize = 50\n",
        "#training_data = CustomImageDataset( annotations_file='', img_dir=data_dir, file_list=raw_files,transform=None, target_transform=None)\n",
        "if (train_on_gpu==1):\n",
        "    print('setting GPU data loads')\n",
        "    dataloader_training = DataLoader(train_subset, batch_size=bsize,shuffle=True, num_workers=2) #only 2 workers for Colab CPU\n",
        "    dataloader_validation = DataLoader(val_subset, batch_size=bsize,shuffle=True, num_workers=2) #only 2 workers for Colab CPU\n",
        "    dataloader_test = DataLoader(test_subset, batch_size=bsize,shuffle=True, num_workers=2) #only 2 workers for Colab CPU\n",
        "elif ((train_on_tpu == 1) and (train_on_gpu == 0)):\n",
        "    print('setting TPU data loads')\n",
        "    dataloader_training = DataLoader(train_subset, batch_size=bsize,shuffle=True, num_workers=2) #only 2 workers for Colab CPU\n",
        "    dataloader_validation = DataLoader(val_subset, batch_size=bsize,shuffle=True, num_workers=2) #only 2 workers for Colab CPU\n",
        "    dataloader_test = DataLoader(test_subset, batch_size=bsize,shuffle=True, num_workers=2) #only 2 workers for Colab CPU\n",
        "\n",
        "else:\n",
        "    dataloader_training = DataLoader(train_subset, batch_size=bsize,shuffle=True, num_workers=2)#, sampler=sampler) #only 2 workers for Colab CPU\n",
        "    dataloader_validation = DataLoader(val_subset, batch_size=bsize,shuffle=True, num_workers=2,drop_last=True)#, sampler=sampler) #only 2 workers for Colab CPU\n",
        "    dataloader_test = DataLoader(test_subset, batch_size=bsize,shuffle=True, num_workers=2) #only 2 workers for Colab CPU\n",
        "\n",
        "\n",
        "############################################\n",
        "if (0): #disable while testing above, but this works\n",
        "    bsize = 8\n",
        "    #training_data = CustomImageDataset( annotations_file='', img_dir=data_dir, file_list=raw_files,transform=None, target_transform=None)\n",
        "    if (train_on_gpu):\n",
        "        dataloader = DataLoader(training_data, batch_size=bsize,shuffle=True, num_workers=4) #only 2 workers for Colab CPU\n",
        "    else:\n",
        "        dataloader = DataLoader(training_data, batch_size=bsize,shuffle=True, num_workers=2) #only 2 workers for Colab CPU\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of patch files found =  1859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-x-rQSkAbxQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNyc0Qzc3naC"
      },
      "source": [
        "### DEBUG\n",
        "#for ii in training_data:\n",
        "#    print(ii)\n",
        "#print(full_file_list[401])\n",
        "#d = next(iter(dataloader_training))\n",
        "#print(len(d['image']), len(d['label']))\n",
        "#print(d['label'])\n",
        "if (0):\n",
        "    for i, _ in enumerate(dataloader_training, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        #inputs, labels = data\n",
        "        pass\n",
        "        #print(type(data['image']))\n",
        "\n",
        "\n",
        "        if (i > 0):\n",
        "            break\n",
        "\n",
        "if (0):\n",
        "    #Mixing GPU and CPU model saves doesn't seem to map well yet\n",
        "    final_file = os.path.join(model_dir,'vgg16_best_accuracy_97_gpu')\n",
        "    checkpoint = torch.load(final_file, map_location=torch.device('cpu'))\n",
        "    #model_vgg16 = VGG16(*args, **kwargs)\n",
        "    #model_vgg16.load_state_dict(torch.load(PATH))\n",
        "    \n",
        "    #model_vgg16.load_state_dict(checkpoint)\n",
        "    #best_acc = checkpoint['acc']\n",
        "    #start_epoch = checkpoint['epoch']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea5d1nDDVt1t",
        "outputId": "5533b62e-abe7-4cf6-af04-7fa1944d8c78"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.vgg16_stack = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),  \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),         \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),        \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),            \n",
        "            nn.ReLU(inplace=True),         \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),         \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            #flattening layer before the linear??\n",
        "            nn.Flatten(), #testing this out before FC layers\n",
        "            nn.Linear(25088, 4096), #in should match 512x512 above\n",
        "            nn.ReLU(inplace=True), #testing this layer out instead of softmax\n",
        "            nn.Linear(4096,3))#, #3 classes instead of 4, removed Actionable\n",
        "            #nn.Softmax(dim=1))\n",
        "        #nn.Sequential(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        #self.linear_layers = Sequential(\n",
        "        #    Linear(4 * 7 * 7, )\n",
        "        #)\n",
        "\n",
        "#transform_test = transforms.Compose([\n",
        "#    transforms.ToTensor(),\n",
        "#    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "#])\n",
        "\n",
        "    def forward(self, x):\n",
        "        #self.flatten = nn.Flatten()\n",
        "        #x = self.flatten(x)\n",
        "        #print('fwd shape x = ',x.shape)\n",
        "        logits = self.vgg16_stack(x)\n",
        "        #print('logits out = ', logits.shape)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "model_vgg16 = VGG16() #.to(device)\n",
        "model_vgg16 = model_vgg16.float()\n",
        "print(model_vgg16)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG16(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (vgg16_stack): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (13): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): ReLU(inplace=True)\n",
            "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (23): Flatten(start_dim=1, end_dim=-1)\n",
            "    (24): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Linear(in_features=4096, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCYkVCHmm3Ob"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmD0A9SkmDrC"
      },
      "source": [
        "#\n",
        "# GET TPU Information  --- use only when GPU has timed out\n",
        "# EXPERIMENT!!!!\n",
        "if (train_on_tpu == 1):\n",
        "    import tensorflow as tf\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    # This is the TPU initialization code that has to be at the beginning.\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URt7A8iTUka2",
        "outputId": "b6ecd4bd-4af7-4474-c0a9-b2ef359d32e1"
      },
      "source": [
        "#clear GPU cache if needed\n",
        "\n",
        "try_to_clear = 0\n",
        "if (try_to_clear == 1):\n",
        "    del model_vgg16\n",
        "    mem_alloc = torch.cuda.memory_allocated(dev)\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache() #   clear_cache\n",
        "\n",
        "    print('memory allocated is ', mem_alloc)\n",
        "else:\n",
        "    print('SKIP GPU MEM CLEAR---')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SKIP GPU MEM CLEAR---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDWdC1VVgkXg",
        "outputId": "aedc464c-80d7-47b7-d4d9-055467ded6ce"
      },
      "source": [
        "#Show summary of model setup and move model to the GPU\n",
        " #train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if (train_on_gpu == 1):\n",
        "    #dev=torch.device(\"cuda\") \n",
        "    model_vgg16.to(dev)\n",
        "    summary(model_vgg16,(3,244,244), batch_size = bsize, device='cuda')\n",
        "elif ( (train_on_tpu == 1) and (train_on_gpu == 0)):\n",
        "    ### TPU with pytorch has some issues\n",
        "    model_vgg16.to(dev)\n",
        "    summary(model_vgg16,(3,244,244), batch_size = bsize, device=dev)\n",
        "else:\n",
        "    summary(model_vgg16,(3,244,244), batch_size = bsize)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [50, 64, 244, 244]           1,792\n",
            "            Conv2d-2         [50, 64, 244, 244]          36,928\n",
            "              ReLU-3         [50, 64, 244, 244]               0\n",
            "         MaxPool2d-4         [50, 64, 122, 122]               0\n",
            "            Conv2d-5        [50, 128, 122, 122]          73,856\n",
            "            Conv2d-6        [50, 128, 122, 122]         147,584\n",
            "              ReLU-7        [50, 128, 122, 122]               0\n",
            "         MaxPool2d-8          [50, 128, 61, 61]               0\n",
            "            Conv2d-9          [50, 256, 61, 61]         295,168\n",
            "           Conv2d-10          [50, 256, 61, 61]         590,080\n",
            "           Conv2d-11          [50, 256, 61, 61]         590,080\n",
            "             ReLU-12          [50, 256, 61, 61]               0\n",
            "        MaxPool2d-13          [50, 256, 30, 30]               0\n",
            "           Conv2d-14          [50, 512, 30, 30]       1,180,160\n",
            "           Conv2d-15          [50, 512, 30, 30]       2,359,808\n",
            "           Conv2d-16          [50, 512, 30, 30]       2,359,808\n",
            "             ReLU-17          [50, 512, 30, 30]               0\n",
            "        MaxPool2d-18          [50, 512, 15, 15]               0\n",
            "           Conv2d-19          [50, 512, 15, 15]       2,359,808\n",
            "           Conv2d-20          [50, 512, 15, 15]       2,359,808\n",
            "           Conv2d-21          [50, 512, 15, 15]       2,359,808\n",
            "             ReLU-22          [50, 512, 15, 15]               0\n",
            "        MaxPool2d-23            [50, 512, 7, 7]               0\n",
            "          Flatten-24                [50, 25088]               0\n",
            "           Linear-25                 [50, 4096]     102,764,544\n",
            "             ReLU-26                 [50, 4096]               0\n",
            "           Linear-27                    [50, 3]          12,291\n",
            "================================================================\n",
            "Total params: 117,491,523\n",
            "Trainable params: 117,491,523\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 34.07\n",
            "Forward/backward pass size (MB): 9572.41\n",
            "Params size (MB): 448.19\n",
            "Estimated Total Size (MB): 10054.67\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY_ek5CWuvLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad9a287-c04b-4fda-b24a-930ed438b0fc"
      },
      "source": [
        "#\n",
        "# LOSS FUNCTION SETUP\n",
        "#\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "\n",
        "import torch\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight = weight_numbers)\n",
        "if (train_on_gpu ==1):\n",
        "    criterion.cuda(dev)\n",
        "\n",
        "#OPTIMIZERS\n",
        "#optimizer = optim.SGD(model_vgg16.parameters(), lr=0.1, momentum = 0.99) #lr=0.001, momentum=0.9)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_vgg16.parameters(), lr=0.000007, weight_decay=0.00000)#lr=0.00005, weight_decay=0.0000)\n",
        "\n",
        "#add scheduler to adjust learning rates when val gets stuck\n",
        "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.1)\n",
        "\n",
        "if (train_on_gpu == 1):\n",
        "    m = nn.LogSoftmax(dim=1).cuda(dev)\n",
        "    nll_loss = nn.NLLLoss().cuda(dev)\n",
        "else:\n",
        "    m = nn.LogSoftmax(dim=1)\n",
        "    nll_loss = nn.NLLLoss()\n",
        "\n",
        "L1loss = nn.L1Loss()\n",
        "\n",
        "model_vgg16.parameters\n",
        "#summary(model_vgg16, (3, 224, 224))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of VGG16(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (vgg16_stack): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (13): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (16): ReLU(inplace=True)\n",
              "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (21): ReLU(inplace=True)\n",
              "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (23): Flatten(start_dim=1, end_dim=-1)\n",
              "    (24): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Linear(in_features=4096, out_features=3, bias=True)\n",
              "  )\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvu7z1XuILhw"
      },
      "source": [
        "#tensorboard for debugging views\n",
        "use_tensor_board = 1\n",
        "if (use_tensor_board == 1):\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "    writer = SummaryWriter(tensorboard_dir)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF7jbtrd8Gpd"
      },
      "source": [
        "#Look at Tensorboard info\n",
        "if (0):\n",
        "    !pip3 install -q tensorboard\n",
        "    %tensorboard --logdir=tensorboard_dir"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ9GMcB0-aDE"
      },
      "source": [
        "###Generate Data weight values\n",
        "#weight_numbers = torch.tensor([1,40,80], dtype=torch.float32)\n",
        "#[1600,124,76] [1,13,21]\n",
        "#weight_sum = torch.sum(weight_numbers)\n",
        "##weight_balance=[]\n",
        "#for ii in weight_numbers:\n",
        "#    weight_balance.append(ii/weight_sum)\n",
        "#torch.tensor(weight_balance)\n",
        "\n",
        "#newList = [x / sum(weight_sum) for x in weight_balance]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bA94lc_wsYV",
        "outputId": "ccb369b0-98a0-4e82-e814-782f63c21aa5"
      },
      "source": [
        "##########################################################\n",
        "# Test against the test dataset to get accuracy results\n",
        "##########################################################\n",
        "run_test= 1\n",
        "if (run_test == 1):\n",
        "    #Mixing GPU and CPU model saves doesn't seem to map well yet\n",
        "    final_file = os.path.join(model_dir,'vgg16_best_accuracy_97_gpu_final')\n",
        "    checkpoint = torch.load(final_file, map_location=torch.device('cpu'))\n",
        "    #model_vgg16 = VGG16(*args, **kwargs)\n",
        "    #model_vgg16.load_state_dict(torch.load(PATH))\n",
        "    \n",
        "    #model_vgg16.load_state_dict(checkpoint)\n",
        "    #best_acc = checkpoint['acc']\n",
        "    #start_epoch = checkpoint['epoch']\n",
        "\n",
        "\n",
        "\n",
        "    model_vgg16.load_state_dict(checkpoint)\n",
        "    model_vgg16.eval()\n",
        "\n",
        "\n",
        "    #load up with the pre-sized patch images\n",
        "    all_data = CustomImageDataset(img_dir=patch_dir,\n",
        "                                    category = full_category_name, \n",
        "                                    file_count=full_file_count,\n",
        "                                    file_list = full_file_list, \n",
        "                                    transform=None, \n",
        "                                    target_transform=None)\n",
        "\n",
        "    dataloader_all = DataLoader(all_data, batch_size=50,shuffle=True, num_workers=2)#, \n",
        "\n",
        "    total_accuracy = []\n",
        "    total_precision = []\n",
        "    for epoch in range(0,35):\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(dataloader_all, 0):\n",
        "                print(i)\n",
        "                # get the inputs; data is a list of [inputs, labels]\n",
        "                #inputs, labels = data\n",
        "                inputs = data['image'].type(FloatTensor)\n",
        "                labels = data['label'] #.type(FloatTensor)\n",
        "\n",
        "                if (train_on_gpu):\n",
        "                    inputs, labels = inputs.to(dev), labels.to(dev)\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs = model_vgg16(inputs) #.permute(0, 1, 2, 3))\n",
        "\n",
        "                outputs=torch.flatten(outputs, start_dim=1)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "\n",
        "\n",
        "                #print(outputs)\n",
        "                y_pred_softmax = torch.log_softmax(outputs, dim = 1)\n",
        "                _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
        "                #print(y_pred_tags)\n",
        "                #print(labels)\n",
        "                correct_pred = (y_pred_tags == labels).float()\n",
        "                accuracy = correct_pred.sum() / len(correct_pred)\n",
        "                #accuracy = torch.round(accuracy)\n",
        "\n",
        "                #calculate PRECISION\n",
        "                #get # of true cancer cases, find # of cancer cases falsely predicted\n",
        "                from sklearn.metrics import average_precision_score\n",
        "                labels \n",
        "                temp_labels = []\n",
        "                temp_prediction = []\n",
        "                for ii in labels:\n",
        "                    if (ii == 2):\n",
        "                        temp_labels.append(1)\n",
        "                    else:\n",
        "                        temp_labels.append(0)\n",
        "\n",
        "                for ii in y_pred_tags:\n",
        "                    if (ii == 2):\n",
        "                        temp_prediction.append(1)\n",
        "                    else:\n",
        "                        temp_prediction.append(0)\n",
        "                print(temp_labels)\n",
        "                print(temp_prediction)                   \n",
        "                average_precision = average_precision_score(temp_labels, temp_prediction)\n",
        "                total_precision.append(average_precision)\n",
        "                print('Average precision-recall score: {0:0.2f}'.format(\n",
        "                    average_precision))\n",
        "\n",
        "\n",
        "                total_accuracy.append(accuracy)\n",
        "                if (i%100 == 0):\n",
        "                    print('@ interim accuracy = ',i,  sum(total_accuracy)/len(total_accuracy))                \n",
        "                #print('-----#correct, training accuracy = ',correct_pred,accuracy)\n",
        "    print('Finished testing all data')\n",
        "    print('total accuracy = ', sum(total_accuracy)/len(total_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: nan\n",
            "@ interim accuracy =  0 tensor(1.)\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recall = tps / tps[-1]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 0.67\n",
            "2\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "3\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "4\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "5\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "6\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 0.50\n",
            "7\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "8\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 0.77\n",
            "9\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "Average precision-recall score: 0.85\n",
            "10\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 0.77\n",
            "11\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "12\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 0.02\n",
            "13\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "14\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "Average precision-recall score: 1.00\n",
            "15\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 0.67\n",
            "16\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "17\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 0.52\n",
            "18\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 0.02\n",
            "19\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "Average precision-recall score: 1.00\n",
            "20\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
            "Average precision-recall score: 0.67\n",
            "21\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "Average precision-recall score: 1.00\n",
            "22\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "23\n",
            "[0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "24\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 0.75\n",
            "25\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: nan\n",
            "26\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: nan\n",
            "27\n",
            "[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 0.82\n",
            "28\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "29\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 0.50\n",
            "30\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "31\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 0.52\n",
            "32\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "33\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "Average precision-recall score: 1.00\n",
            "34\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "Average precision-recall score: 1.00\n",
            "35\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: 1.00\n",
            "36\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "Average precision-recall score: 1.00\n",
            "37\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Average precision-recall score: nan\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ogj4Oq-MMJf",
        "outputId": "d13c0d3e-6359-49d8-b9b8-b97e84f982f0"
      },
      "source": [
        "temp_labels = []\n",
        "for ii in labels:\n",
        "    if (ii == 2):\n",
        "        temp_labels.append(1)\n",
        "    else:\n",
        "        temp_labels.append(0)\n",
        "print(labels)\n",
        "print(temp_labels)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0])\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp_zI8hfwFRL",
        "outputId": "77c793c1-d385-4ec1-fd73-74f135e76766"
      },
      "source": [
        "### TRAIN THE NETWORK\n",
        "from torch import FloatTensor\n",
        "from torch import tensor\n",
        "\n",
        "\n",
        "start_epoch = 0 #used for reloading model\n",
        "stop_epoch = 100\n",
        "training_loss = {}\n",
        "validation_loss = {}\n",
        "training_accuracy = {}\n",
        "tloss = []\n",
        "vloss = []\n",
        "best_accuracy = 0.0 #store highest accuracy for current model\n",
        "\n",
        "##################################################################\n",
        "############# RESUME FROM EARLIER CHECKPOINT\n",
        "###################################################################\n",
        "resume_checkpoint = 0\n",
        "if (resume_checkpoint == 1):\n",
        "    #Mixing GPU and CPU model saves doesn't seem to map well yet\n",
        "    check_file = os.path.join(model_dir,'model_final_1')\n",
        "    checkpoint = torch.load(check_file, map_location=torch.device('cpu'))\n",
        "    model_vgg16.load_state_dict(checkpoint)\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    \n",
        "\n",
        "for epoch in range(start_epoch,stop_epoch): #(2):  # loop over the dataset multiple times\n",
        "\n",
        "    training_loss[epoch] = []\n",
        "    validation_loss[epoch] = []\n",
        "    training_accuracy[epoch] = []\n",
        "\n",
        "#\n",
        "# TRAINING SECTION\n",
        "#\n",
        "    model_vgg16.train()     # Optional when not using Model Specific layer\n",
        "    running_loss = 0.0\n",
        "    iteration_counter = 0 #keep track of total runs\n",
        "    #for i, data in enumerate(dataloader, 0):  #works for general setup!!\n",
        "    for i, data in enumerate(dataloader_training, 0):\n",
        "        model_vgg16.train() #reset if validation step kicked off\n",
        "        iteration_counter = iteration_counter + 1\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        #inputs, labels = data\n",
        "        inputs = data['image'].type(FloatTensor)\n",
        "        labels = data['label'] #.type(FloatTensor)\n",
        "\n",
        "        if (train_on_gpu):\n",
        "            inputs, labels = inputs.to(dev), labels.to(dev)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model_vgg16(inputs) #.permute(0, 1, 2, 3))\n",
        "        if (i == 0):\n",
        "            #show the initial sizes as a check\n",
        "            print(outputs.size())\n",
        "            print(labels.size())\n",
        "\n",
        "        ### L1 loss complains about outputs columns being out of order, permute\n",
        "        ### outputs to match up\n",
        "        use_L1 = 0\n",
        "        if (use_L1==1):\n",
        "            outputs= outputs.permute(3,1,2,0)\n",
        "            loss = L1loss(outputs,labels)\n",
        "        else:\n",
        "            #outputs = nn.Softmax2d(outputs)\n",
        "            \n",
        "            outputs=torch.flatten(outputs, start_dim=1)\n",
        "            #weight_balance = torch.tensor([0,0])\n",
        "            #loss = criterion(outputs, labels.long())\n",
        "            loss = criterion(outputs, labels.long())\n",
        "        training_loss[epoch].append(loss.item())\n",
        "        print('Loss = ', loss.item())\n",
        "\n",
        "        #\n",
        "        # ACCURACY\n",
        "        #\n",
        "        if (i%2 == 0):\n",
        "            #print(outputs)\n",
        "            y_pred_softmax = torch.log_softmax(outputs, dim = 1)\n",
        "            _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
        "            print(y_pred_tags)\n",
        "            print(labels)\n",
        "            correct_pred = (y_pred_tags == labels).float()\n",
        "            accuracy = correct_pred.sum() / len(correct_pred)\n",
        "            #accuracy = torch.round(accuracy)\n",
        "            training_accuracy[epoch].append(accuracy)\n",
        "            print('-----#correct, training accuracy = ',correct_pred,accuracy)\n",
        "\n",
        "        #\n",
        "        # TENSOR BOARD ADD\n",
        "        #\n",
        "        if (use_tensor_board == 1):\n",
        "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        if (i %50 == 0):\n",
        "            if(train_on_gpu == 1):\n",
        "                !nvidia-smi\n",
        "        if (i % 100 == 0):\n",
        "            running_loss += loss.item()\n",
        "            print('loop,epoch, loss, total running loss = ',i,epoch, loss.item(), running_loss/100)\n",
        "            running_loss = 0.0\n",
        "\n",
        "\n",
        "###############################################\n",
        "# VALIDATION STEP\n",
        "###############################################\n",
        "    if (iteration_counter %1 == 0):\n",
        "        model_vgg16.eval()     # Optional when not using Model Specific layer\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for ii, vdata in enumerate(dataloader_validation, 0):\n",
        "                # get the inputs; data is a list of [inputs, labels]\n",
        "                #inputs, labels = data\n",
        "                vinputs = vdata['image'].type(FloatTensor)\n",
        "                vlabels = vdata['label'] #.type(FloatTensor)\n",
        "\n",
        "                #assert(len(vlabels) == bsize), \"Wrong number of val labels\"\n",
        "\n",
        "                if (train_on_gpu):\n",
        "                    vinputs, vlabels = vinputs.to(dev), vlabels.to(dev)\n",
        "\n",
        "                target = model_vgg16(vinputs)\n",
        "\n",
        "                #flatten for cross entropy loss\n",
        "                target=torch.flatten(target, start_dim=1)\n",
        "                loss = criterion(target,vlabels)\n",
        "                valid_loss = loss.item()# * data.size(0)\n",
        "                validation_loss[epoch].append(valid_loss)\n",
        "\n",
        "\n",
        "                y_val_softmax = torch.log_softmax(target, dim = 1)\n",
        "                _, y_val_tags = torch.max(y_val_softmax, dim = 1)\n",
        "                print(y_val_tags)\n",
        "                print(vlabels)\n",
        "                correct_pred = (y_val_tags == vlabels).float()\n",
        "                val_accuracy = correct_pred.sum() / len(correct_pred)\n",
        "                #accuracy = torch.round(accuracy)\n",
        "                #training_accuracy[epoch].append(accuracy)\n",
        "                print('....#correct, validation accuracy = ',correct_pred,val_accuracy)\n",
        "                if ((val_accuracy > best_accuracy) and (valid_loss < 0.5) and (valid_loss > 0.05)):\n",
        "                    print('!!!!!Accuracy < previous. Saving model. acc= ',val_accuracy)\n",
        "                    best_accuracy = val_accuracy\n",
        "                    #save this model off\n",
        "                    best_model_name = 'vgg16_best_accuracy_' + str(np.int(best_accuracy.item()*100))\n",
        "                    torch.save(model_vgg16.state_dict(),os.path.join(model_dir,best_model_name))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################################################\n",
        "# SAVE INTERMEDIATE FILES\n",
        "####################################################\n",
        "\n",
        "    training_pickle = os.path.join(tensorboard_dir,'tensor_training_loss.pickle')\n",
        "    validation_pickle = os.path.join(tensorboard_dir,'tensor_validation_loss.pickle')\n",
        "    accuracy_pickle = os.path.join(tensorboard_dir,'tensor_training_accuracy.pickle')\n",
        "    val_accuracy_pickle = os.path.join(tensorboard_dir,'tensor_validation_accuracy.pickle')\n",
        "\n",
        "    pickle.dump(training_loss, open( training_pickle, \"wb\" ),protocol=5 )\n",
        "    pickle.dump(validation_loss, open( validation_pickle, \"wb\" ),protocol=5 )\n",
        "    pickle.dump(training_accuracy,open( accuracy_pickle, \"wb\" ),protocol=5 )\n",
        "    pickle.dump(val_accuracy,open( val_accuracy_pickle, \"wb\" ),protocol=5 )\n",
        "\n",
        "\n",
        "    #save a midterm file\n",
        "    if (1): #epoch %2 == 0):\n",
        "        save_model_mid = 'model_final_' + str(epoch)\n",
        "        torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model_vgg16.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': loss,\n",
        "                    }, os.path.join(model_dir,save_model_mid))\n",
        "\n",
        "    if (use_tensor_board == 1):\n",
        "        writer.flush() #write out all the info to the board\n",
        "\n",
        "torch.save(model_vgg16.state_dict(),os.path.join(model_dir,'vgg16_trained_final'))\n",
        "print('Finished Training')\n",
        "if (use_tensor_board == 1):\n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50, 3])\n",
            "torch.Size([50])\n",
            "Loss =  1.0991638898849487\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]) tensor(0.9200)\n",
            "loop,epoch, loss, total running loss =  0 0 1.0991638898849487 0.010991638898849488\n",
            "Loss =  1.095999002456665\n",
            "Loss =  1.098305344581604\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0200)\n",
            "Loss =  1.1065834760665894\n",
            "Loss =  1.1003516912460327\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.0906541347503662\n",
            "Loss =  1.1042537689208984\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0200)\n",
            "Loss =  1.1092655658721924\n",
            "Loss =  1.1038873195648193\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
            "        2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.]) tensor(0.9000)\n",
            "Loss =  1.10506272315979\n",
            "Loss =  1.0962172746658325\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 2,\n",
            "        0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) tensor(0.8600)\n",
            "Loss =  1.1050118207931519\n",
            "Loss =  1.107035517692566\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
            "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) tensor(0.8800)\n",
            "Loss =  1.0924760103225708\n",
            "Loss =  1.0995312929153442\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) tensor(0.8600)\n",
            "Loss =  1.0971852540969849\n",
            "Loss =  1.097328782081604\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.]) tensor(0.9200)\n",
            "Loss =  1.0973856449127197\n",
            "Loss =  1.0995615720748901\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2,\n",
            "        0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) tensor(0.8800)\n",
            "Loss =  1.0995094776153564\n",
            "Loss =  1.1033728122711182\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.]) tensor(0.8400)\n",
            "Loss =  1.0943719148635864\n",
            "Loss =  1.0986193418502808\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.]) tensor(0.9200)\n",
            "Loss =  1.0993757247924805\n",
            "Loss =  1.0987080335617065\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) tensor(0.9000)\n",
            "Loss =  1.099800705909729\n",
            "Loss =  1.0994857549667358\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2,\n",
            "        0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) tensor(0.8600)\n",
            "Loss =  1.099025011062622\n",
            "Loss =  1.0993295907974243\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.]) tensor(0.8400)\n",
            "Loss =  1.0979063510894775\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
            "        0, 0])\n",
            "....#correct, validation accuracy =  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.]) tensor(0.9200)\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
            "        0, 0])\n",
            "....#correct, validation accuracy =  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.]) tensor(0.9000)\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0,\n",
            "        2, 0])\n",
            "....#correct, validation accuracy =  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.]) tensor(0.9200)\n",
            "torch.Size([50, 3])\n",
            "torch.Size([50])\n",
            "Loss =  1.0981392860412598\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) tensor(0.8600)\n",
            "loop,epoch, loss, total running loss =  0 1 1.0981392860412598 0.010981392860412598\n",
            "Loss =  1.099791407585144\n",
            "Loss =  1.0977743864059448\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.1003570556640625\n",
            "Loss =  1.0991556644439697\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0200)\n",
            "Loss =  1.100533127784729\n",
            "Loss =  1.103294014930725\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.]) tensor(0.)\n",
            "Loss =  1.095852255821228\n",
            "Loss =  1.0971431732177734\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0,\n",
            "        0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        1, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0600)\n",
            "Loss =  1.098087191581726\n",
            "Loss =  1.098069429397583\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
            "        0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.1001356840133667\n",
            "Loss =  1.1002024412155151\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0200)\n",
            "Loss =  1.097663164138794\n",
            "Loss =  1.1001192331314087\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0200)\n",
            "Loss =  1.1001887321472168\n",
            "Loss =  1.0999133586883545\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0200)\n",
            "Loss =  1.0989466905593872\n",
            "Loss =  1.0997167825698853\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0200)\n",
            "Loss =  1.1012547016143799\n",
            "Loss =  1.0971524715423584\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]) tensor(0.0800)\n",
            "Loss =  1.0987379550933838\n",
            "Loss =  1.098317265510559\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.0981318950653076\n",
            "Loss =  1.0989768505096436\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.0987821817398071\n",
            "Loss =  1.0996781587600708\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]) tensor(0.0200)\n",
            "Loss =  1.0978777408599854\n",
            "Loss =  1.097805380821228\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0,\n",
            "        0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
            "        2, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.]) tensor(0.0600)\n",
            "Loss =  1.099017858505249\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "....#correct, validation accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]) tensor(0.1000)\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "....#correct, validation accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0600)\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0,\n",
            "        0, 0])\n",
            "....#correct, validation accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]) tensor(0.0200)\n",
            "torch.Size([50, 3])\n",
            "torch.Size([50])\n",
            "Loss =  1.0993810892105103\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0400)\n",
            "loop,epoch, loss, total running loss =  0 2 1.0993810892105103 0.010993810892105103\n",
            "Loss =  1.0981262922286987\n",
            "Loss =  1.1018919944763184\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.]) tensor(0.)\n",
            "Loss =  1.0981801748275757\n",
            "Loss =  1.0981866121292114\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0,\n",
            "        2, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.]) tensor(0.0400)\n",
            "Loss =  1.0969353914260864\n",
            "Loss =  1.1026842594146729\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.]) tensor(0.)\n",
            "Loss =  1.0980887413024902\n",
            "Loss =  1.0980502367019653\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.0999830961227417\n",
            "Loss =  1.0980044603347778\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.096874475479126\n",
            "Loss =  1.0979580879211426\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.0978156328201294\n",
            "Loss =  1.0975804328918457\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
            "        0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.0980801582336426\n",
            "Loss =  1.1018612384796143\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.]) tensor(0.)\n",
            "Loss =  1.0978052616119385\n",
            "Loss =  1.0974913835525513\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.1013238430023193\n",
            "Loss =  1.1001856327056885\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0200)\n",
            "Loss =  1.0970555543899536\n",
            "Loss =  1.096187949180603\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 2])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0600)\n",
            "Loss =  1.0954197645187378\n",
            "Loss =  1.105613350868225\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.]) tensor(0.)\n",
            "Loss =  1.1029958724975586\n",
            "Loss =  1.0949084758758545\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 0, 1, 0, 2,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.]) tensor(0.0800)\n",
            "Loss =  1.0953060388565063\n",
            "Loss =  1.1010262966156006\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0200)\n",
            "Loss =  1.0993988513946533\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0,\n",
            "        0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "....#correct, validation accuracy =  tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0600)\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "        2, 0])\n",
            "....#correct, validation accuracy =  tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]) tensor(0.0800)\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "....#correct, validation accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.]) tensor(0.)\n",
            "torch.Size([50, 3])\n",
            "torch.Size([50])\n",
            "Loss =  1.0998178720474243\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0200)\n",
            "loop,epoch, loss, total running loss =  0 3 1.0998178720474243 0.010998178720474244\n",
            "Loss =  1.1009764671325684\n",
            "Loss =  1.0971852540969849\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.0967637300491333\n",
            "Loss =  1.0994155406951904\n",
            "tensor([2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2,\n",
            "        2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1,\n",
            "        2, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.0963369607925415\n",
            "Loss =  1.101291298866272\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) tensor(0.0600)\n",
            "Loss =  1.0987693071365356\n",
            "Loss =  1.0987061262130737\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0800)\n",
            "Loss =  1.0997052192687988\n",
            "Loss =  1.0982317924499512\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]) tensor(0.0600)\n",
            "Loss =  1.0976070165634155\n",
            "Loss =  1.0948336124420166\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
            "        0, 2, 1, 0, 0, 0, 0, 0, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.1600)\n",
            "Loss =  1.0983030796051025\n",
            "Loss =  1.0991846323013306\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.0958716869354248\n",
            "Loss =  1.1005922555923462\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0200)\n",
            "Loss =  1.097791075706482\n",
            "Loss =  1.099714756011963\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0400)\n",
            "Loss =  1.0953288078308105\n",
            "Loss =  1.0976228713989258\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0800)\n",
            "Loss =  1.0998107194900513\n",
            "Loss =  1.09819495677948\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([2, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0600)\n",
            "Loss =  1.0985212326049805\n",
            "Loss =  1.095217227935791\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0600)\n",
            "Loss =  1.090379238128662\n",
            "Loss =  1.0977205038070679\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.0600)\n",
            "Loss =  1.1129226684570312\n",
            "Loss =  1.0914047956466675\n",
            "tensor([1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.1200)\n",
            "Loss =  1.0890754461288452\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 2, 0, 1, 1, 1, 1, 0, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "....#correct, validation accuracy =  tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.1400)\n",
            "tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "....#correct, validation accuracy =  tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor(0.1200)\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1,\n",
            "        1, 1])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "....#correct, validation accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]) tensor(0.1400)\n",
            "torch.Size([50, 3])\n",
            "torch.Size([50])\n",
            "Loss =  1.0924017429351807\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
            "        0, 1])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.]) tensor(0.1200)\n",
            "loop,epoch, loss, total running loss =  0 4 1.0924017429351807 0.010924017429351807\n",
            "Loss =  1.1057095527648926\n",
            "Loss =  1.0900133848190308\n",
            "tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
            "        1, 1])\n",
            "tensor([0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 2])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.]) tensor(0.4400)\n",
            "Loss =  1.0961940288543701\n",
            "Loss =  1.0848013162612915\n",
            "tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
            "        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
            "        1, 0])\n",
            "tensor([0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
            "        2, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1.]) tensor(0.4600)\n",
            "Loss =  1.0709385871887207\n",
            "Loss =  1.0882285833358765\n",
            "tensor([1, 0, 0, 1, 1, 2, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0,\n",
            "        0, 0, 2, 1, 0, 1, 0, 1, 1, 1, 2, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
            "        1, 0])\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.]) tensor(0.5000)\n",
            "Loss =  1.0807729959487915\n",
            "Loss =  1.098097562789917\n",
            "tensor([0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
            "        0, 1, 0, 0, 1, 2, 0, 0, 2, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.]) tensor(0.6400)\n",
            "Loss =  1.0853941440582275\n",
            "Loss =  1.0713279247283936\n",
            "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.]) tensor(0.8200)\n",
            "Loss =  1.0758426189422607\n",
            "Loss =  1.0572172403335571\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.]) tensor(0.8000)\n",
            "Loss =  1.1066107749938965\n",
            "Loss =  1.0954185724258423\n",
            "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0])\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 2,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.]) tensor(0.7400)\n",
            "Loss =  1.0373655557632446\n",
            "Loss =  1.0209623575210571\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
            "        1, 0])\n",
            "tensor([2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
            "        2, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.]) tensor(0.6000)\n",
            "Loss =  1.094161868095398\n",
            "Loss =  0.9773348569869995\n",
            "tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
            "        2, 0, 0, 0, 1, 1, 1, 1, 2, 1, 2, 0, 2, 1, 1, 0, 2, 0, 0, 1, 2, 0, 1, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.]) tensor(0.5400)\n",
            "Loss =  0.9886215925216675\n",
            "Loss =  1.0749679803848267\n",
            "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 2, 2, 0, 0, 1, 2, 1, 1, 1, 2,\n",
            "        1, 1, 2, 1, 0, 0, 1, 1, 0, 2, 1, 1, 0, 0, 0, 1, 0, 1, 2, 0, 1, 1, 0, 1,\n",
            "        1, 2])\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
            "        1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.]) tensor(0.4400)\n",
            "Loss =  1.02628493309021\n",
            "Loss =  1.0164953470230103\n",
            "tensor([0, 0, 0, 2, 1, 1, 2, 2, 1, 2, 0, 1, 1, 0, 2, 0, 1, 1, 0, 0, 2, 2, 0, 2,\n",
            "        0, 0, 2, 0, 0, 2, 1, 2, 0, 0, 2, 2, 1, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2, 2,\n",
            "        2, 0])\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        2, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.]) tensor(0.4600)\n",
            "Loss =  1.1287349462509155\n",
            "Loss =  1.0043503046035767\n",
            "tensor([2, 1, 0, 0, 1, 0, 2, 2, 1, 0, 0, 1, 0, 2, 1, 0, 2, 0, 2, 0, 1, 0, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 1, 0, 0, 2, 2, 2, 2, 0, 0, 2, 0, 1, 1, 2,\n",
            "        0, 2])\n",
            "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.]) tensor(0.4400)\n",
            "Loss =  1.1202666759490967\n",
            "Loss =  0.9905841946601868\n",
            "tensor([0, 2, 0, 0, 1, 0, 2, 2, 0, 2, 0, 0, 0, 1, 1, 0, 2, 0, 2, 2, 2, 0, 0, 2,\n",
            "        0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 1, 0, 0, 1, 2, 0, 2, 2, 0, 0, 2, 2,\n",
            "        0, 2])\n",
            "tensor([0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2,\n",
            "        0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.]) tensor(0.5400)\n",
            "Loss =  1.1638737916946411\n",
            "Loss =  1.1659598350524902\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0,\n",
            "        2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
            "        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "-----#correct, training accuracy =  tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.]) tensor(0.6800)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCXdhoQJfA11"
      },
      "source": [
        "#### Test out saved pickle files\n",
        "#tdata = training_pickle\n",
        "training_pickle = os.path.join(tensorboard_dir,'tensor_training.pickle')\n",
        "validation_pickle = os.path.join(tensorboard_dir,'tensor_validation.pickle')\n",
        "accuracy_pickle = os.path.join(tensorboard_dir,'tensor_accuracy.pickle')\n",
        "\n",
        "tdata = pickle.load( open( training_pickle, \"rb\" ) )\n",
        "vdata =  pickle.load(open(validation_pickle,\"rb\"))\n",
        "adata =  pickle.load(open(accuracy_pickle,\"rb\"))\n",
        "#torch.load(accuracy_pickle,map_location=torch.device('cpu'))\n",
        "\n",
        "tdata.keys()\n",
        "\n",
        "total_losses= []\n",
        "for ii in tdata.keys():\n",
        "    for num in tdata[ii]:\n",
        "        total_losses.append(num)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(total_losses)\n",
        "\n",
        "total_validation = []\n",
        "for ii in vdata.keys():\n",
        "    for num in vdata[ii]:\n",
        "        total_validation.append(num)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(total_validation)\n",
        "\n",
        "\n",
        "total_accuracy = []\n",
        "for ii in adata.keys():\n",
        "    for num in adata[ii]:\n",
        "        total_accuracy.append(num)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(total_accuracy)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvPAsqqGUiu-"
      },
      "source": [
        "#\n",
        "#Disconnect from the VM to save our GPU time\n",
        "#\n",
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IG1u00h0biD"
      },
      "source": [
        "model_weights = [] # we will save the conv layer weights in this list\n",
        "conv_layers = [] # we will save the 49 conv layers in this list\n",
        "model_children = list(model_vgg16.children())\n",
        "print(len(model_children))\n",
        "print(model_children[1])\n",
        "\n",
        "\n",
        "# counter to keep count of the conv layers\n",
        "counter = 0 \n",
        "# append all the conv layers and their respective weights to the list\n",
        "for i in range(len(model_children[1])):\n",
        "    if type(model_children[1][i]) == nn.Conv2d:\n",
        "        counter += 1\n",
        "        model_weights.append(model_children[1][i].weight)\n",
        "        conv_layers.append(model_children[1][i])\n",
        "    elif type(model_children[1][i]) == nn.Sequential:\n",
        "        for j in range(len(model_children[1][i])):\n",
        "            for child in model_children[1][i][j].children():\n",
        "                if type(child) == nn.Conv2d:\n",
        "                    counter += 1\n",
        "                    model_weights.append(child.weight)\n",
        "                    conv_layers.append(child)\n",
        "print(f\"Total convolutional layers: {counter}\")\n",
        "print(len(model_weights))\n",
        "print(np.shape(model_weights[0]))\n",
        "\n",
        "# take a look at the conv layers and the respective weights\n",
        "for weight, conv in zip(model_weights, conv_layers):\n",
        "    # print(f\"WEIGHT: {weight} \\nSHAPE: {weight.shape}\")\n",
        "    print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# visualize the first conv layer filters\n",
        "print(np.shape(model_weights[3]))\n",
        "plt.figure(figsize=(20, 17))\n",
        "for i, filter in enumerate(model_weights[0]):\n",
        "    plt.subplot(8, 8, i+1) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n",
        "    plt.imshow(filter[0, :, :].detach(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    #plt.savefig('../outputs/filter.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyxp-yp5xiCK"
      },
      "source": [
        "check_file = os.path.join(model_dir,'model_final_15')\n",
        "checkpoint = torch.load(check_file)#, map_location=torch.device('cpu'))\n",
        "#model_vgg16.load_state_dict(checkpoint)\n",
        "#best_acc = checkpoint['acc']\n",
        "#start_epoch = checkpoint['epoch']\n",
        "model_vgg16.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S09G-Y-0yTrm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-aNItv2-UbC"
      },
      "source": [
        "outputs #.softmax(dim=1)\n",
        "loss\n",
        "y_pred_softmax = torch.log_softmax(outputs, dim = 1)\n",
        "_, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
        "print(y_pred_tags)\n",
        "print(labels)\n",
        "correct_pred = (y_pred_tags == labels).float()\n",
        "acc = correct_pred.sum() / len(correct_pred)\n",
        "\n",
        "acc = torch.round(acc * 100)\n",
        "print('correct_pred, accuracy = ',correct_pred,acc)\n",
        "#torch.flatten(outputs, start_dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXgCtKaWF9zY"
      },
      "source": [
        "#target.topk\n",
        "#target = model_vgg16(vinputs)\n",
        "target = outputs\n",
        "print(inputs.shape)\n",
        "#vout = model_vgg16(inputs)\n",
        "print(vout.shape)\n",
        "vout_save = vout\n",
        "\n",
        "#probabilities = torch.exp(vout).data\n",
        "\n",
        "# getting the topk (=5) probabilites and indexes\n",
        "# 0 -> probabilities\n",
        "# 1 -> index\n",
        "print('vout size = ', vout.shape)\n",
        "#probs = torch.log_softmax(target, dim=1)\n",
        "#probs = torch.softmax(vout.flatten, dim=1)\n",
        "probs = torch.flatten(vout,3)\n",
        "#print('prob shape = ',probs.shape)\n",
        "print(torch.softmax(probs,dim=0))\n",
        "#prob = torch.topk(probabilities, topk)[0].tolist()[0] # probabilities\n",
        "#index = torch.topk(probabilities, topk)[1].tolist()[0] # index\n",
        "print(probs[0,:].sum()/len(probs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-hbpPftgQuX"
      },
      "source": [
        "import torch.nn.functional as nnf\n",
        "\n",
        "# ...\n",
        "prob = nnf.softmax(outputs, dim=1)\n",
        "\n",
        "top_p, top_class = prob.topk(1, dim = 1)\n",
        "print(top_p,top_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzwAPvmdqx_C"
      },
      "source": [
        "counter = 0\n",
        "for ii in dataloader_validation:\n",
        "    print(ii)\n",
        "    counter= counter + 1\n",
        "    if (counter > 0):\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWmfYzQVkJeA"
      },
      "source": [
        "#outputs=torch.flatten(outputs, start_dim=1)\n",
        "#loss = criterion(outputs, labels.long())\n",
        "\n",
        "outputs = model_vgg16(inputs) #.permute(0, 1, 2, 3))\n",
        "print(outputs.size())\n",
        "print(labels.size())\n",
        "a= torch.flatten(outputs,start_dim=1)\n",
        "\n",
        "print(outputs.mean())\n",
        "print(a)\n",
        "criterion(a,labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4BDZ5ZxXupV"
      },
      "source": [
        "#model_vgg16.state_dict()\n",
        "print(outputs.shape)\n",
        "a=outputs.view(4*512*7,4)\n",
        "print(a.shape)\n",
        "print(labels)\n",
        "x=torch.flatten(outputs, start_dim=1)\n",
        "\n",
        "#proper_target = torch.argmax(labels, dim=1)  # make sure keepdim=False\n",
        "#loss = criterion(outputs, proper_target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvbWBBE_KooQ"
      },
      "source": [
        "torch.tensor([2])\n",
        "a=outputs/2\n",
        "a = torch.tensor(a)\n",
        "m = np.mean(inputs)\n",
        "b= inputs/1000\n",
        "b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVdV-EFSu9Gt"
      },
      "source": [
        "num_correct = 0\n",
        "total = 0\n",
        "model_vgg16.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader_test, 0):\n",
        "    #for data, labels in test_subset:\n",
        "        #data = data.to(device=device)\n",
        "        #labels = labels.to(device=device)\n",
        "        inputs = data['image'].type(FloatTensor)\n",
        "        labels = data['label'] #.type(FloatTensor)\n",
        "\n",
        "        predictions = model_vgg16(inputs) #for testing, only pass in images, no labels\n",
        "        predictions = predictions.permute(3,1,2,0)\n",
        "        num_correct += (predictions == labels).sum()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    print(f\"Test Accuracy of the model: {float(num_correct)/float(total)*100:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r88X8eSqIvV3"
      },
      "source": [
        "#\n",
        "# Test out every file for the correct shape\n",
        "#\n",
        "total_files = len(full_file_list)\n",
        "for counter, file_name in enumerate(full_file_list):\n",
        "\n",
        "    text_tokens = file_name.split(sep='_')\n",
        "    label_class = text_tokens[3] #get the label token in 4th position\n",
        "    category_name =  label_class.upper() \n",
        "\n",
        "    #print('testing file = ', counter, file_name)\n",
        "    cfile = os.path.join(patch_dir,category_name,file_name)\n",
        "    image = pickle.load( open( cfile, \"rb\" ) )\n",
        "    #print('testing ',cfile)\n",
        "    shape = np.shape(image)\n",
        "    if ((shape[0] != 3) or (shape[1] != 244) or (shape[2] != 244)):\n",
        "        print('Failed file ',file_name, shape)\n",
        "\n",
        "#    else:\n",
        "#        print('testing file = ', counter, file_name, shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqkjcMaMN6Nt"
      },
      "source": [
        "#patch_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES' \n",
        "!ls /content/gdrive/MyDrive/DBT_WORKSPACE/TRAINING_PATCHES/NORMAL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNv_YjYpWrRg"
      },
      "source": [
        "\n",
        "cfile = os.path.join(patch_dir,'NORMAL','DBT-P03406_DBT-S05192_lcc_Normal_s83_cx520_cy1165_244_244.pickle')\n",
        "image = pickle.load( open( cfile, \"rb\" ) )\n",
        "image.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}