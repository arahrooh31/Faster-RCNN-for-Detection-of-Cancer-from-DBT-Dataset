{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_DBT",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPstd2+NlMY4j/601zYh/8q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arahrooh31/UCLA_BE223C/blob/Keane_temp/VGG_DBT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iix74kj_aDNY",
        "outputId": "f470eed8-46ed-46a1-e719-cdee89a0ec05"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import gc  #debug memory leaks in matplotlib\n",
        "import csv #read in description files\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "#\n",
        "# Read Data from google drive\n",
        "#\n",
        "from google.colab import drive #for loading gdrive data\n",
        "from google.colab import files\n",
        "\n",
        "# install dependencies not included by Colab\n",
        "# use pip3 to ensure compatibility w/ Google Deep Learning Images \n",
        "!pip3 install -q pydicom \n",
        "!pip3 install -q tqdm \n",
        "!pip3 install -q imgaug\n",
        "!pip3 install -q pickle5\n",
        "\n",
        "import pydicom #to read dicom files\n",
        "from pydicom import dcmread\n",
        "import pickle5 as pickle; #generic storage of image arra\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "\n",
        "#\n",
        "# Load data from google drive\n",
        "#\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data_dir = '/content/gdrive/My Drive/DBT_DATA/IMG_ARRAYS'\n",
        "top_dir = '/content/gdrive/My Drive/DBT_WORKSPACE/TRAINING_PATCHES' \n",
        "#DBT_DATA/TRAINING_DATA/manifest-1605042674814/Breast-Cancer-Screening-DBT'\n",
        "#png_dir = '/content/gdrive/My Drive/BE223C_SPRING_2021/PNG_IMAGES'\n",
        "#png_annotation_dir = '/content/gdrive/My Drive/BE223C_SPRING_2021/PNG_ANNOTATION_IMAGES'\n",
        "#image_save_dir = '/content/gdrive/My Drive/DBT_DATA/IMG_ARRAYS' #matrix img data\n",
        "#'/content/gdrive/My Drive/BE223C_SPRING_2021/IMAGE_ARRAY'\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9MB 17.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 16.6MB/s \n",
            "\u001b[?25h  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G49j_VOIdgA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e3fb61-5004-438a-fa11-7e1e95856686"
      },
      "source": [
        "#\n",
        "# GET FULL LIST OF FILES IN IMAGE ARRAY DIRECTORY\n",
        "#\n",
        "raw_files = os.listdir(data_dir)\n",
        "print('found #files: ',len(raw_files))\n",
        "\n",
        "if (0):\n",
        "    #create fake patches for now\n",
        "    patch_dict = {}\n",
        "    for counter,filename in enumerate(raw_files):\n",
        "        #load full array\n",
        "        full_filename = os.path.join(data_dir,filename)\n",
        "        img_data = pickle.load( open( full_filename, \"rb\" ) )\n",
        "        patch_data = img_data[0:3,:,:]\n",
        "        patch_dict[counter]= patch_data\n",
        "        print(full_filename,np.shape(patch_data))\n",
        "\n",
        "        if (counter > 0):\n",
        "            break\n",
        "        \n",
        "    "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "found #files:  2568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSX1m2-J6Wtv",
        "outputId": "67a376c3-7cb2-4d6b-aa18-4845be7f444f"
      },
      "source": [
        "a = np.ones((1,8))\n",
        "a.astype(float)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp7kwrfSlK2r"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class CustomImageDataset(): #Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, file_list,transform=None, target_transform=None):\n",
        "        #self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list) #99 #len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        for ii in self.file_list:\n",
        "            full_filename = os.path.join(self.img_dir,ii) #self.img_dir# = os.path.join(self.img_dir) self.img_labels.iloc[idx, 0])\n",
        "            image = pickle.load( open( full_filename, \"rb\" ) )\n",
        "            patch_data = image[0:3,1200:1444,1200:1444].astype(float)\n",
        "            image = patch_data\n",
        "            label = 1\n",
        "\n",
        "\n",
        "            #read_image(img_path)\n",
        "            #label = self.img_labels.iloc[idx, 1]\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            if self.target_transform:\n",
        "                label = self.target_transform(label)\n",
        "            sample = {\"image\": image, \"label\": label}\n",
        "            return sample"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1p45uP4oi6a"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#\n",
        "# Setup the custom dataset\n",
        "#\n",
        "\n",
        "training_data = CustomImageDataset( annotations_file='', img_dir=data_dir, file_list=raw_files,transform=None, target_transform=None)\n",
        "dataloader = DataLoader(training_data, batch_size=1,shuffle=True, num_workers=2) #only 2 workers for Colab CPU\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-x-rQSkAbxQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNyc0Qzc3naC"
      },
      "source": [
        "### DEBUG\n",
        "#for ii in training_data:\n",
        "#    print(ii)\n",
        "if (0):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        print(type(data['image']))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea5d1nDDVt1t",
        "outputId": "97dcc2a4-95b8-4c35-b03a-2c20228488ef"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.vgg16_stack = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),  \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),         \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),        \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),            \n",
        "            nn.ReLU(inplace=True),         \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),         \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Linear(7, 4096), #in should match 512x512 above\n",
        "            nn.Linear(4096,4))\n",
        "        #nn.Sequential(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.flatten = nn.Flatten()\n",
        "        #x = self.flatten(x)\n",
        "        logits = self.vgg16_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "model_vgg16 = VGG16() #.to(device)\n",
        "model_vgg16 = model_vgg16.float()\n",
        "print(model_vgg16)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG16(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (vgg16_stack): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (13): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): ReLU(inplace=True)\n",
            "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (23): Linear(in_features=7, out_features=4096, bias=True)\n",
            "    (24): Linear(in_features=4096, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5SN__GwscbW",
        "outputId": "9e27dd78-f900-4905-c9f1-f80e269fa6b4"
      },
      "source": [
        "#\n",
        "# DEFINE MODEL LAYERS FOR A TEST CASE\n",
        "#\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(64*64,512), #28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 4),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        #VGG16setup matching paper\n",
        "            #'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print('type in forward is ',type(x))\n",
        "        #print(x)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "model = NeuralNetwork() #.to(device)\n",
        "model = model.float()\n",
        "print(model)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=4096, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY_ek5CWuvLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0b41d4-9a8b-4296-c2bf-ce0828908fb4"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_vgg16.parameters(), lr=0.001, momentum=0.9)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "L1loss = nn.L1Loss()\n",
        "\n",
        "model_vgg16.parameters"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of VGG16(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (vgg16_stack): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (13): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (16): ReLU(inplace=True)\n",
              "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (21): ReLU(inplace=True)\n",
              "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (23): Linear(in_features=7, out_features=4096, bias=True)\n",
              "    (24): Linear(in_features=4096, out_features=4, bias=True)\n",
              "  )\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp_zI8hfwFRL",
        "outputId": "debee2fa-08c8-4c08-ed9e-53928bd5ffc7"
      },
      "source": [
        "### TRAIN THE NETWORK\n",
        "from torch import FloatTensor\n",
        "from torch import tensor\n",
        "\n",
        "\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        #inputs, labels = data\n",
        "        inputs = data['image'].type(FloatTensor)\n",
        "        if (i %2 == 0):\n",
        "            labels = tensor([1, 2, 3,4])\n",
        "        else:\n",
        "            labels = tensor([1, 2, 3,4])\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        #outputs = model(inputs)\n",
        "        outputs = model_vgg16(inputs) #.permute(0, 1, 2, 3))\n",
        "        #outputs = model_vgg16(inputs)\n",
        "        #print('outputs type is ', outputs.size())\n",
        "        #print(outputs)\n",
        "        loss = L1loss(outputs,labels)\n",
        "        #loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 0:    # print every 10 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([1, 512, 7, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,     1] loss: 0.028\n",
            "[1,   101] loss: 0.211\n",
            "[1,   201] loss: 0.037\n",
            "[1,   301] loss: 0.042\n",
            "[1,   401] loss: 0.038\n",
            "[1,   501] loss: 0.038\n",
            "[1,   601] loss: 0.038\n",
            "[1,   701] loss: 0.038\n",
            "[1,   801] loss: 0.037\n",
            "[1,   901] loss: 0.041\n",
            "[1,  1001] loss: 0.038\n",
            "[1,  1101] loss: 0.041\n",
            "[1,  1201] loss: 0.039\n",
            "[1,  1301] loss: 0.040\n",
            "[1,  1401] loss: 0.041\n",
            "[1,  1501] loss: 0.041\n",
            "[1,  1601] loss: 0.040\n",
            "[1,  1701] loss: 0.041\n",
            "[1,  1801] loss: 0.040\n",
            "[1,  1901] loss: 0.040\n",
            "[1,  2001] loss: 0.041\n",
            "[1,  2101] loss: 0.040\n",
            "[1,  2201] loss: 0.040\n",
            "[1,  2301] loss: 0.040\n",
            "[1,  2401] loss: 0.036\n",
            "[1,  2501] loss: 0.040\n",
            "[2,     1] loss: 0.000\n",
            "[2,   101] loss: 0.037\n",
            "[2,   201] loss: 0.035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM7MJkcp9CGo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npghhl1jdUs5"
      },
      "source": [
        "import torch # PyTorch \n",
        "from torchvision import datasets # Datasets module \n",
        "import torchvision.transforms as transforms # Image Transforms \n",
        "from torch.utils.data.sampler import SubsetRandomSampler # Sampler \n",
        "!pip3 -q install torchlayers\n",
        "import torchlayers as tl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0UPPs_vhP_r",
        "outputId": "4789468f-dc87-4b31-d61d-030c19e752e2"
      },
      "source": [
        "# torch.nn and torchlayers can be mixed easily\n",
        "model = torch.nn.Sequential(\n",
        "    tl.Conv(64),  # specify ONLY out_channels\n",
        "    torch.nn.ReLU(),  # use torch.nn wherever you wish\n",
        "    tl.BatchNorm(),  # BatchNormNd inferred from input\n",
        "    tl.Conv(128),  # Default kernel_size equal to 3\n",
        "    tl.ReLU(),\n",
        "    tl.Conv(256, kernel_size=11),  # \"same\" padding as default\n",
        "    tl.GlobalMaxPool(),  # Known from Keras\n",
        "    tl.Linear(10),  # Output for 10 classes\n",
        ")\n",
        " \n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Conv(in_channels=?, out_channels=64, kernel_size=3, stride=1, padding=same, dilation=1, groups=1, bias=True, padding_mode=zeros)\n",
            "  (1): ReLU()\n",
            "  (2): BatchNorm(num_features=?, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (3): Conv(in_channels=?, out_channels=128, kernel_size=3, stride=1, padding=same, dilation=1, groups=1, bias=True, padding_mode=zeros)\n",
            "  (4): ReLU()\n",
            "  (5): Conv(in_channels=?, out_channels=256, kernel_size=11, stride=1, padding=same, dilation=1, groups=1, bias=True, padding_mode=zeros)\n",
            "  (6): GlobalMaxPool()\n",
            "  (7): Linear(in_features=?, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prA4kK_gk5tZ",
        "outputId": "6e06c6d5-3703-44a8-c093-ddfcbce26bc8"
      },
      "source": [
        "\n",
        " \n",
        "'''VGG11/13/16/19 in Pytorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = VGG('VGG16')\n",
        "    x = torch.randn(2,3,32,32)\n",
        "    y = net(x)\n",
        "    print(y.size())\n",
        "    print(net)\n",
        "\n",
        "test()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 10])\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "-MHbdkNpWHw2",
        "outputId": "24f5934a-4600-4b1d-b989-a2dd5c129e31"
      },
      "source": [
        "net()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4c863004183b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-uYUwyT5KUR"
      },
      "source": [
        "\n",
        " \n",
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "from models import *\n",
        "from utils import progress_bar\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "parser.add_argument('--resume', '-r', action='store_true',\n",
        "                    help='resume from checkpoint')\n",
        "args = parser.parse_args()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "# net = VGG('VGG19')\n",
        "# net = ResNet18()\n",
        "# net = PreActResNet18()\n",
        "# net = GoogLeNet()\n",
        "# net = DenseNet121()\n",
        "# net = ResNeXt29_2x64d()\n",
        "# net = MobileNet()\n",
        "# net = MobileNetV2()\n",
        "# net = DPN92()\n",
        "# net = ShuffleNetG2()\n",
        "# net = SENet18()\n",
        "# net = ShuffleNetV2(1)\n",
        "# net = EfficientNetB0()\n",
        "# net = RegNetX_200MF()\n",
        "net = SimpleDLA()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "if args.resume:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=args.lr,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+200):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}